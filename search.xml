<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ubuntu12.04 Hadoop+Spark 分布式部署步骤]]></title>
    <url>%2F2018%2F06%2F04%2Fubuntu12-04-Hadoop-Spark-%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[1、准备工具： 虚拟机：vmware 操作系统 ：ubuntu12.04 （桌面版或server版）(iso文件) java：jdk-7u75-linux-x64.tar.gz oracle官网可下载 hadoop：haopop2.5.2 spark: spark1.1.1 登陆终端 secure crt 文件传输：file zilla 2、具体步骤 （1）、安装vmware —过程略 （2）、新建虚拟机（3个）安装ubuntu系统 —过程略 注：下面的步骤无特殊说明则需要在三个虚拟机上同时执行 （3）、apt-get 更新： apt-get update （4）、安装vim openssh客户端，服务端 apt-get instsall vim openssh-client openssh-server 判断ssh是否正常启动 命令：ps -e |grep ssh 如果既有sshd 又有ssh-agent 则ssh启动成功 （5）、安装java环境 假定准备将java安装在/usr/lib/java中 则建立该目录：mkdir /usr/lib/java 通过file zilla 将压缩文件传送到虚拟机上 解压jdk tar -xzvf jdkxxxxxx 设置环境变量： 编辑 /etc/profile文件，在其中加入以下内容 export JAVA_HOME=/usr/lib/java/jdk1.7.0_75 export JRE_HOME={$JAVA_HOME}/jre 保存退出 令其生效 source /etc/profile 此时查看java版本 java -version 如果系统中有多个java环境则需要手动设置默认的jdk 命令如下： sudo update-alternatives –install /usr/bin/javac javac /usr/lib/java/jdk1.7.0_75/bin/javac 300 sudo update-alternatives –install /usr/bin/java java /usr/lib/java/jdk1.7.0_75/bin/java 300 sudo update-alternatives –install /usr/bin/jps jps /usr/lib/java/jdk1.7.0_75/bin/jps 300 此时在输入java -version就可以看到上图的结果了 （6）、修改hostname+配置hosts 修改hostname:编辑/etc/hostname文件分别将三台机器的hostname改成master slave1 slave2 之后重启服务器 配置hosts：编辑/etc/hosts，添加 ip 和对应的hostname。 保存退出之后验证三台机器直接ping 各自的hostname是否互通 （7）、配置ssh hadoop是通过ssh来进行节点间的通信，所以要去掉节点间ssh的密码，所以需要对ssh进行配置 a）、生成公钥：命令：ssh-keygen -t rsa -P “” b）、将master公钥保存在authorized_keys下 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 对于本机可用ssh localhost来验证是否设置成功 c）、将另外两台机器的公钥生成后通过scp命令汇聚到master下命令如下：在目录/root/.ssh中 scp id_rsa.pub root@master:/root/.ssh/id_rsa.pub.slave1(2) d）、将所有公钥全部保存在authorized_keys下命令如b e）、将设置好的authorized_keys文件发送给两个slave节点，之后验证三台机器是否可以ssh无密码登陆 （8）、安装hadoop 解压之前的压缩文件（tar -zxvf） 设置环境变量：在/etc/profile中添加相应变量 export HADOOP_COMMON_HOME=/usr/local/hadoopspark/hadoop-2.5.2 export HADOOP_CONF_DIR=/usr/local/hadoopspark/hadoop-2.5.2/etc/hadoop export YARN_CONF_DIR=/usr/local/hadoopspark/hadoop-2.5.2/etc/hadoop export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_COMMON_HOME/lib/native export HADOOP_OPTS=”-Djava.library.path=$HADOOP_COMMON_HOME/lib” export CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar export PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin:${HADOOP_COMMON_HOME}/bin:${HADOOP_COMMON_HOME}/sbin:$PATH 保存并使其生效。 设置hadoop-env.sh（文件位于hadoop归档后的目录的etc/hadoop/中）： 加入java环境变量 export JAVA_HOME=/usr/lib/java/jdk1.7.0_75 此时可验证hadoop是否安装成功：hadoop version 若能出现hadoop信息说明安装基本成功 （9）、配置hadoop信息： 主要配置一下几个文件 配置之前先建立几se个目录用来保存hadoop执行过程中的相关信息：mkdir tmp hdfs hdfs/name hdfs/data 1）、etc/hadoop/core-site.xml 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/hadoop-2.7.0/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2)、etc/hadoop/mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3）、etc/hadoop/hdfs-site.xml 1234567891011121314151617181920&lt;value&gt;Master:50090&lt;/value&gt;是网页访问地址/home/hadoop/tools/hadoop-2.7.0/&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;Master:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/hadoop-2.7.0/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/hadoop-2.7.0/hdfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4）、etc/hadoop/yarn-site.xml 12345678910111213141516171819202122232425262728&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;Master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;Master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;Master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;Master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;Master:8088&lt;/value&gt; &lt;/property&gt;&lt;!-- Site specific YARN configuration properties --&gt;&lt;/configuration&gt; （5）、etc/hadoop/slaves 注意要删除自己本身的节点，或者在配置互信时，加入自己的密钥。 配置完毕则可以准备0启动hadoop 下面的过程在master节点上执行相关操作 namenode的格式化：hadoop namenode -format 之后到hadoop所在目录启动hadoop：sbin/start-all.sh 一小段等待后hadoop启动，用jps查看 123456DataNodeJpsSecondaryNameNodeNameNodeNodeManagerResourceManager 若出现下面信息则启动成功，可通过web-browser查看相关信息url:http://masterip:50070 关闭hadoop服务：sbin/stop-all-sh 若需要重新格式化namenode则需要删除掉tmp 和/tmp/hadoop* 和hdfs/name hdfs/data中的内容 至此hadoop 分布式部署成功，下面准备在此基础上安装spark（由于spark的工作需要基于hadoop中的hdfs分布式文件系统） （10）、安装scala 解压下载好的压缩文件（tar -zxvf） 配置环境变量（/etc/profile） export SCALA_HOME=/usr/lib/scala/scala-2.11.4 验证：键入scala：出现scala命令控制行说明安装成功 （11）、安装spark 解压文件–过程略 配置环境变量：编辑conf下的spark-env.sh文件将java scala相关环境变量加入并且配置masterip以及各个worker节点的内存 export JAVA_HOME=/usr/lib/java/jdk1.7.0_75 export SCALA_HOME=/usr/lib/scala/scala-2.11.4 export SPARK_MASTER_IP=192.168.194.129 export SPARK_WORKER_MEMORY=2g export SPARK_CONF_DIR=/usr/local/hadoopspark/spark-1.1.1-bin-hadoop2.4/conf 配置同目录下的slaves 将所有节点的hostname添加进该文件（三台机器均配置） 配置完毕启动spark sbin/start-all.sh jps命令查看若新增这两个进程即spark成功启动（Master Worker） 至此spark+hadoop的分布式部署完成]]></content>
      <categories>
        <category>大数据-分布式</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 文件结构]]></title>
    <url>%2F2018%2F06%2F04%2Fhexo-%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[hexo 文件结构 接着上篇如何发布一篇博客，本篇将分析以下hexo框架中的基本文件结构。 在hexo框架中主要目录结构如下： 12345678|-- _config.yml|-- package.json|-- scaffolds|-- source |-- _posts|-- themes|-- .gitignore|-- package.json _config.yml：全局配置文件，网站的很多信息都在这里配置，诸如网站名称，副标题，描述，作者，语言，主题，部署等等参数。 package.json：hexo框架的参数和所依赖插件。 scaffolds：scaffolds是“脚手架、骨架”的意思，当你新建一篇文章（hexo new 'title'）的时候，hexo是根据这个目录下的文件进行构建的。基本不用关心。 source： 这个目录很重要，新建的文章都是在保存在这个目录下的._posts 。需要新建的博文都放在 _posts 目录下。 _posts 目录下是一个个 markdown 文件。你应该可以看到一个 hello-world.md 的文件，文章就在这个文件中编辑。 _posts 目录下的md文件，会被编译成html文件，放到 public （此文件现在应该没有，因为你还没有编译过）文件夹下。 themes：网站主题目录，hexo有非常好的主题拓展，支持的主题也很丰富。该目录下，每一个子目录就是一个主题。 _config.yml 文件的具体说明 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: Hexo #网站标题subtitle: #网站副标题description: #网站描述author: John Doe #作者language: #语言timezone: #网站时区。Hexo 默认使用您电脑的时区。时区列表。比如说：America/New_York, Japan, 和 UTC 。# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://yoursite.com #你的站点Urlroot: / #站点的根目录permalink: :year/:month/:day/:title/ #文章的 永久链接 格式 permalink_defaults: #永久链接中各部分的默认值# Directory source_dir: source #资源文件夹，这个文件夹用来存放内容public_dir: public #公共文件夹，这个文件夹用于存放生成的站点文件。tag_dir: tags # 标签文件夹 archive_dir: archives #归档文件夹category_dir: categories #分类文件夹code_dir: downloads/code #Include code 文件夹i18n_dir: :lang #国际化（i18n）文件夹skip_render: #跳过指定文件的渲染，您可使用 glob 表达式来匹配路径。 # Writingnew_post_name: :title.md # 新文章的文件名称default_layout: post #预设布局titlecase: false # 把标题转换为 title caseexternal_link: true # 在新标签中打开链接filename_case: 0 #把文件名称转换为 (1) 小写或 (2) 大写render_drafts: false #是否显示草稿post_asset_folder: false #是否启动 Asset 文件夹relative_link: false #把链接改为与根目录的相对位址 future: true #显示未来的文章highlight: #内容中代码块的设置 enable: true line_number: true auto_detect: false tab_replace:# Category &amp; Tagdefault_category: uncategorizedcategory_map: #分类别名tag_map: #标签别名# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DD #日期格式time_format: HH:mm:ss #时间格式 # Pagination## Set per_page to 0 to disable paginationper_page: 10 #分页数量pagination_dir: page # Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: landscape #主题名称# Deployment## Docs: https://hexo.io/docs/deployment.html# 部署部分的设置deploy:]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何发布一篇博客]]></title>
    <url>%2F2018%2F06%2F04%2F%E5%A6%82%E4%BD%95%E5%8F%91%E5%B8%83%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[如何发布一篇博客 自己曾经搭建过无数个博客。从开始自己使用yaf+php自己写编写，到采用GitHub+jeklly。后来也用过腾讯云服务器+WordPress的模式。最近知道了一种新的博客搭建方法，GitHub+hexo的模式。hexo框架有着丰富的主题，并且使用方便。马上要工作了，这次花费了几天搭建了一下新的博客。未来也会慢慢把自己原先的博客，慢慢迁移到这里。 hexo+GitHub搭建比较简单见大神教程 本文主要分析了一下，博客发布的过程。 1执行代码：hexo new [layout] &lt;title&gt; 这里 [layout] 是指布局格式，默认的布局格式为post。在hexo 中一共有3中不同的默认格式： 布局格式 路径 post source/_posts Page source Draft source/_drafts 默认布局格式为 post。先以布局格式为 post 的情况为说明。 首先会在在根目录的下的 source 文件夹中创建了一个 _post 文件夹，并在里面生成一个对应的文件。例如myBlog.md。这时候文件会根据模板，填入相应信息。模板存储在 scaffolds 中，可以根据需要自行修改。 根据不同的模板，决定了 Front-matter 的内容。 Front-matter 是文件最上方以 --- 分隔的区域，用于指定个别文件的变量。例如： 123456---title: 如何发布一篇博客date: 2018-06-04 01:35:26tags: hexocategories: blog--- 在 Front-matter 中有以下预定参数: 参数 描述 默认值 layout 布局 title 标题&lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space:pre&quot;&gt;&lt;/span&gt; date 建立日期 &lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space:pre&quot;&gt;&lt;/span&gt;文件建立日期 updated 更新日期 &lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space:pre&quot;&gt;&lt;/span&gt;文件更新日期 comments 开启文章的评论功能 true tags 标签（不适用于分页） categories 分类（不适用于分页） permalink 覆盖文章网址 其中 tags 都是并列的。categories 则有严格的顺序性和层次性。例如 12345categories:- blogtags:- 技术- 搭建]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
