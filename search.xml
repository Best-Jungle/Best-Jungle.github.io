<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深入理解 xgboost]]></title>
    <url>%2F2018%2F06%2F05%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-xgboost%2F</url>
    <content type="text"><![CDATA[深入理解 xgboost 前言 1）Boosted Tree的若干同义词 可能有人会问，为什么我没有听过这个名字。这是因为Boosted Tree有各种马甲，比如GBDT, GBRT (gradient boosted regression tree)，LambdaMART也是一种boosted tree的变种。网上有很多介绍Boosted tree的资料，不过大部分都是基于Friedman的最早一篇文章Greedy Function Approximation: A Gradient Boosting Machine的翻译。个人觉得这不是最好最一般地介绍boosted tree的方式。而网上除了这个角度之外的介绍并不多。 2）有监督学习算法的逻辑组成 要讲boosted tree，要先从有监督学习讲起。在有监督学习里面有几个逻辑上的重要组成部件33，初略地分可以分为：模型，参数 和 目标函数。 i. 模型和参数 模型指给定输入xixi如何去预测 输出 yiyi。我们比较常见的模型如线性模型（包括线性回归和logistic regression）采用了线性叠加的方式进行预测ŷ i=∑jwjxijy^i=∑jwjxij 。其实这里的预测yy可以有不同的解释，比如我们可以用它来作为回归目标的输出，或者进行sigmoid 变换得到概率，或者作为排序的指标等。而一个线性模型根据yy的解释不同（以及设计对应的目标函数）用到回归，分类或排序等场景。参数指我们需要学习的东西，在线性模型中，参数指我们的线性系数ww。 ii. 目标函数：损失 + 正则 模型和参数本身指定了给定输入我们如何做预测，但是没有告诉我们如何去寻找一个比较好的参数，这个时候就需要目标函数登场了。一般的目标函数包含下面两项： 1Obj(ø) = L(ø) + Ω(ø) 其中 L(ø) 用来指导模型参数调整的方向。Ω(ø) 为正则项，用来防止过拟合现象。 常见的误差函数比如平方误差、logistic误差函数等。而对于线性模型常见的正则化项有L2正则和L1正则。这样目标函数的设计来自于统计学习里面的一个重要概念叫做Bias-variance tradeoff44。比较感性的理解，Bias可以理解为假设我们有无限多数据的时候，可以训练出最好的模型所拿到的误差。而Variance是因为我们只有有限数据，其中随机性带来的误差。目标中误差函数鼓励我们的模型尽量去拟合训练数据，这样相对来说最后的模型会有比较少的 bias。而正则化项则鼓励更加简单的模型。因为当模型简单之后，有限数据拟合出来结果的随机性比较小，不容易过拟合，使得最后模型的预测更加稳定。 iii. 优化算法 讲了这么多有监督学习的基本概念，为什么要讲这些呢？ 是因为这几部分包含了机器学习的主要成分，也是机器学习工具设计中划分模块比较有效的办法。其实这几部分之外，还有一个优化算法，就是给定目标函数之后怎么学的问题。之所以我没有讲优化算法，是因为这是大家往往比较熟悉的“机器学习的部分”。而有时候我们往往只知道“优化算法”，而没有仔细考虑目标函数的设计的问题，比较常见的例子如决策树的学习，大家知道的算法是每一步去优化gini entropy，然后剪枝，但是没有考虑到后面的目标是什么。 Boosted Tree 话题回到boosted tree，我们也是从这几个方面开始讲，首先讲模型。Boosted tree 最基本的组成部分叫做回归树(regression tree)，也叫做CART5。 上面就是一个CART的例子。CART会把输入根据输入的属性分配到各个叶子节点，而每个叶子节点上面都会对应一个实数分数。上面的例子是一个预测一个人是否会喜欢电脑游戏的 CART，你可以把叶子的分数理解为有多可能这个人喜欢电脑游戏。有人可能会问它和decision tree的关系，其实我们可以简单地把它理解为decision tree的一个扩展。从简单的类标到分数之后，我们可以做很多事情，如概率预测，排序。 一个CART往往过于简单无法有效地预测，因此一个更加强力的模型叫做tree ensemble。在上面的例子中，我们用两棵树来进行预测。我们对于每个样本的预测结果就是每棵树预测分数的和。到这里，我们的模型就介绍完毕了。现在问题来了，我们常见的随机森林和boosted tree和tree ensemble有什么关系呢？如果你仔细的思考，你会发现RF和boosted tree的模型都是tree ensemble，只是构造（学习）模型参数的方法不同。第二个问题：在这个模型中的“参数”是什么。在tree ensemble中，参数对应了树的结构，以及每个叶子节点上面的预测分数。 最后一个问题当然是如何学习这些参数。在这一部分，答案可能千奇百怪，但是最标准的答案始终是一个：定义合理的目标函数，然后去尝试优化这个目标函数。在这里我要多说一句，因为决策树学习往往充满了heuristic。 如先优化吉尼系数，然后再剪枝啦，限制最大深度，等等。其实这些heuristic的背后往往隐含了一个目标函数，而理解目标函数本身也有利于我们设计学习算法，这个会在后面具体展开。 对于tree ensemble，我们可以比较严格的把我们的模型写成是： 其中每个f是一个在函数空间6(F)里面的函数，而F对应了所有regression tree的集合。我们设计的目标函数也需要遵循前面的主要原则，包含两部分 iii. 模型学习：additive training 其中第一部分是训练误差，也就是大家相对比较熟悉的如平方误差, logistic loss等。而第二部分是每棵树的复杂度的和。这个在后面会继续讲到。因为现在我们的参数可以认为是在一个函数空间里面，我们不能采用传统的如SGD之类的算法来学习我们的模型，因此我们会采用一种叫做additive training的方式（另外，在我个人的理解里面7，boosting就是指additive training的意思）。每一次保留原来的模型不变，加入一个新的函数$f$到我们的模型中。 现在还剩下一个问题，我们如何选择每一轮加入什么f呢？答案是非常直接的，选取一个f来使得我们的目标函数尽量最大地降低。 这个公式可能有些过于抽象，我们可以考虑当l是平方误差的情况。这个时候我们的目标可以被写成下面这样的二次函数： 更加一般的，对于不是平方误差的情况，我们会采用如下的泰勒展开近似来定义一个近似的目标函数，方便我们进行这一步的计算。 当我们把常数项移除之后，我们会发现如下一个比较统一的目标函数。这一个目标函数有一个非常明显的特点，它只依赖于每个数据点的在误差函数上的一阶导数和二阶导数。有人可能会问，这个材料似乎比我们之前学过的决策树学习难懂。为什么要花这么多力气来做推导呢？ 因为这样做使得我们可以很清楚地理解整个目标是什么，并且一步一步推导出如何进行树的学习。这一个抽象的形式对于实现机器学习工具也是非常有帮助的。传统的GBDT可能大家可以理解如优化平法a残差，但是这样一个形式包含可所有可以求导的目标函数。也就是说有了这个形式，我们写出来的代码可以用来求解包括回归，分类和排序的各种问题，正式的推导可以使得机器学习的工具更加一般。 iv. 树的复杂度 到目前为止我们讨论了目标函数中训练误差的部分。接下来我们讨论如何定义树的复杂度。我们先对于f的定义做一下细化，把树拆分成结构部分q和叶子权重部分w。下图是一个具体的例子。结构函数q把输入映射到叶子的索引号上面去，而w给定了每个索引号对应的叶子分数是什么。 当我们给定了如上定义之后，我们可以定义一棵树的复杂度如下。这个复杂度包含了一棵树里面节点的个数，以及每个树叶子节点上面输出分数的$L2$模平方。当然这不是唯一的一种定义方式，不过这一定义方式学习出的树效果一般都比较不错。下图还给出了复杂度计算的一个例子。 v. 关键步骤 接下来是最关键的一步11，在这种新的定义下，我们可以把目标函数进行如下改写，其中I被定义为每个叶子上面样本集合 Ij = { i|q(xi)=j} 这一个目标包含了T个相互独立的单变量二次函数。我们可以定义 那么这个目标函数可以进一步改写成如下的形式，假设我们已经知道树的结构q，我们可以通过这个目标函数来求解出最好的w，以及最好的w对应的目标函数最大的增益 这两个的结果对应如下，左边是最好的w，右边是这个w对应的目标函数的值。到这里大家可能会觉得这个推导略复杂。其实这里只涉及到了如何求一个一维二次函数的最小值的问题12。如果觉得没有理解不妨再仔细琢磨一下 vi. 打分函数计算举例 Obj代表了当我们指定一个树的结构的时候，我们在目标上面最多减少多少。我们可以把它叫做结构分数(structure score)。你可以认为这个就是类似吉尼系数一样更加一般的对于树结构进行打分的函数。下面是一个具体的打分函数计算的例子 vii. 枚举所有不同树结构的贪心法 所以我们的算法也很简单，我们不断地枚举不同树的结构，利用这个打分函数来寻找出一个最优结构的树，加入到我们的模型中，再重复这样的操作。不过枚举所有树结构这个操作不太可行，所以常用的方法是贪心法，每一次尝试去对已有的叶子加入一个分割。对于一个具体的分割方案，我们可以获得的增益可以由如下公式计算 对于每次扩展，我们还是要枚举所有可能的分割方案，如何高效地枚举所有的分割呢？我假设我们要枚举所有 x&lt;a 这样的条件，对于某个特定的分割a我们要计算a左边和右边的导数和。 我们可以发现对于所有的a，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和GL和GR。然后用上面的公式计算每个分割方案的分数就可以了。 观察这个目标函数，大家会发现第二个值得注意的事情就是引入分割不一定会使得情况变好，因为我们有一个引入新叶子的惩罚项。优化这个目标对应了树的剪枝， 当引入的分割带来的增益小于一个阀值的时候，我们可以剪掉这个分割。大家可以发现，当我们正式地推导目标的时候，像计算分数和剪枝这样的策略都会自然地出现，而不再是一种因为heuristic而进行的操作了。 讲到这里文章进入了尾声，虽然有些长，希望对大家有所帮助，这篇文章介绍了如何通过目标函数优化的方法比较严格地推导出boosted tree的学习。因为有这样一般的推导，得到的算法可以直接应用到回归，分类排序等各个应用场景中去。 5 尾声：xgboost 这篇文章讲的所有推导和技术都指导了xgboost 的设计。xgboost是大规模并行boosted tree的工具，它是目前最快最好的开源boosted tree工具包，比常见的工具包快10倍以上。在数据科学方面，有大量kaggle选手选用它进行数据挖掘比赛，其中包括两个以上kaggle比赛的夺冠方案。在工业界规模方面，xgboost的分布式版本有广泛的可移植性，支持在YARN, MPI, Sungrid Engine等各个平台上面运行，并且保留了单机并行版本的各种优化，使得它可以很好地解决于工业界规模的问题。有兴趣的同学可以尝试使用一下，也欢迎贡献代码。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解GBDT]]></title>
    <url>%2F2018%2F06%2F05%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3GBDT%2F</url>
    <content type="text"><![CDATA[深入理解GBDT GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。 GBDT主要由三个概念组成：Regression Decistion Tree（即DT)，Gradient Boosting（即GB)，Shrinkage (算法的一个重要演进分枝，目前大部分源码都按该版本实现）。搞定这三个概念后就能明白GBDT是如何工作的，要继续理解它如何用于搜索排序则需要额外理解RankNet概念，之后便功德圆满。下文将逐个碎片介绍，最终把整张图拼出来。 一、 DT：回归树 Regression Decision Tree 提起决策树（DT, Decision Tree) 绝大部分人首先想到的就是C4.5分类决策树。但如果一开始就把GBDT中的树想成分类树，那就是一条歪路走到黑，一路各种坑，最终摔得都要咯血了还是一头雾水说的就是LZ自己啊有木有。咳嗯，所以说千万不要以为GBDT是很多棵分类树。决策树分为两大类，回归树和分类树。前者用于预测实数值，如明天的温度、用户的年龄、网页的相关程度；后者用于分类标签值，如晴天/阴天/雾/雨、用户性别、网页是否是垃圾页面。这里要强调的是，前者的结果加减是有意义的，如10岁+5岁-3岁=12岁，后者则无意义，如男+男+女=到底是男是女？ GBDT的核心在于累加所有树的结果作为最终结果，就像前面对年龄的累加（-3是加负3），而分类树的结果显然是没办法累加的，所以GBDT中的树都是回归树，不是分类树，这点对理解GBDT相当重要（尽管GBDT调整后也可用于分类但不代表GBDT的树是分类树）。那么回归树是如何工作的呢？ 下面我们以对人的性别判别/年龄预测为例来说明，每个instance都是一个我们已知性别/年龄的人，而feature则包括这个人上网的时长、上网的时段、网购所花的金额等。 作为对比，先说分类树，我们知道C4.5分类树在每次分枝时，是穷举每一个feature的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的feature和阈值（熵最大的概念可理解成尽可能每个分枝的男女比例都远离1:1），按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。若还不明白可以Google “Regression Tree”，或阅读本文的第一篇论文中Regression Tree部分。 二、 GB：梯度迭代 Gradient Boosting 好吧，我起了一个很大的标题，但事实上我并不想多讲Gradient Boosting的原理，因为不明白原理并无碍于理解GBDT中的Gradient Boosting。 Boosting，迭代，即通过迭代多棵树来共同决策。这怎么实现呢？难道是每棵树独立训练一遍，比如A这个人，第一棵树认为是10岁，第二棵树认为是0岁，第三棵树认为是20岁，我们就取平均值10岁做最终结论？–当然不是！且不说这是投票方法并不是GBDT，只要训练集不变，独立训练三次的三棵树必定完全相同，这样做完全没有意义。之前说过，GBDT是把所有树的结论累加起来做最终结论的，所以可以想到每棵树的结论并不是年龄本身，而是年龄的一个累加量。GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义，简单吧。 三、 GBDT工作过程实例。 还是年龄预测，简单起见训练集只有4个人，A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下图1所示结果： 现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点做多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图2所示结果： 在第一棵树分枝和图1一样，由于A,B年龄较为相近，C,D年龄较为相近，他们被分为两拨，每拨用平均年龄作为预测值。此时计算残差（残差的意思就是： A的预测值 + A的残差 = A的实际值），所以A的残差就是16-15=1（注意，A的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是15，如果还有树则需要都累加起来作为A的预测值）。进而得到A,B,C,D的残差分别为-1,1，-1,1。然后我们拿残差替代A,B,C,D的原值，到第二棵树去学习，如果我们的预测值和它们的残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。这里的数据显然是我可以做的，第二棵树只有两个值1和-1，直接分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。 换句话说，现在A,B,C,D的预测值都和真实年龄一致了。Perfect!： A: 14岁高一学生，购物较少，经常问学长问题；预测年龄A = 15 – 1 = 14 B: 16岁高三学生；购物较少，经常被学弟问问题；预测年龄B = 15 + 1 = 16 C: 24岁应届毕业生；购物较多，经常问师兄问题；预测年龄C = 25 – 1 = 24 D: 26岁工作两年员工；购物较多，经常被师弟问问题；预测年龄D = 25 + 1 = 26 那么哪里体现了Gradient呢？其实回到第一棵树结束时想一想，无论此时的cost function是什么，是均方差还是均差，只要它以误差作为衡量标准，残差向量(-1, 1, -1, 1)都是它的全局最优方向，这就是Gradient。 讲到这里我们已经把GBDT最核心的概念、运算过程讲完了！没错就是这么简单。不过讲到这里很容易发现三个问题： 1）既然图1和图2 最终效果相同，为何还需要GBDT呢？ 答案是过拟合。过拟合是指为了让训练集精度更高，学到了很多”仅在训练集上成立的规律“，导致换一个数据集当前规律就不适用了。其实只要允许一棵树的叶子节点足够多，训练集总是能训练到100%准确率的（大不了最后一个叶子上只有一个instance)。在训练精度和实际精度（或测试精度）之间，后者才是我们想要真正得到的。 我们发现图1为了达到100%精度使用了3个feature（上网时长、时段、网购金额），其中分枝“上网时长&gt;1.1h” 很显然已经过拟合了，这个数据集上A,B也许恰好A每天上网1.09h, B上网1.05小时，但用上网时间是不是&gt;1.1小时来判断所有人的年龄很显然是有悖常识的； 相对来说图2的boosting虽然用了两棵树 ，但其实只用了2个feature就搞定了，后一个feature是问答比例，显然图2的依据更靠谱。（当然，这里是LZ故意做的数据，所以才能靠谱得如此狗血。实际中靠谱不靠谱总是相对的） Boosting的最大好处在于，每一步的残差计算其实变相地增大了分错instance的权重，而已经分对的instance则都趋向于0。这样后面的树就能越来越专注那些前面被分错的instance。就像我们做互联网，总是先解决60%用户的需求凑合着，再解决35%用户的需求，最后才关注那5%人的需求，这样就能逐渐把产品做好，因为不同类型用户需求可能完全不同，需要分别独立分析。如果反过来做，或者刚上来就一定要做到尽善尽美，往往最终会竹篮打水一场空。 2）Gradient呢？不是“G”BDT么？ 到目前为止，我们的确没有用到求导的Gradient。在当前版本GBDT描述中，的确没有用到Gradient，该版本用残差作为全局最优的绝对方向，并不需要Gradient求解. 3）这不是boosting吧？Adaboost可不是这么定义的。 这是boosting，但不是Adaboost。GBDT不是Adaboost Decistion Tree。就像提到决策树大家会想起C4.5，提到boost多数人也会想到Adaboost。Adaboost是另一种boost方法，它按分类对错，分配不同的weight，计算cost function时使用这些weight，从而让“错分的样本权重越来越大，使它们更被重视”。Bootstrap也有类似思想，它在每一步迭代时不改变模型本身，也不计算残差，而是从N个instance训练集中按一定概率重新抽取N个instance出来（单个instance可以被重复sample），对着这N个新的instance再训练一轮。由于数据集变了迭代模型训练结果也不一样，而一个instance被前面分错的越厉害，它的概率就被设的越高，这样就能同样达到逐步关注被分错的instance，逐步完善的效果。Adaboost的方法被实践证明是一种很好的防止过拟合的方法，但至于为什么则至今没从理论上被证明。GBDT也可以在使用残差的同时引入Bootstrap re-sampling，GBDT多数实现版本中也增加的这个选项，但是否一定使用则有不同看法。re-sampling一个缺点是它的随机性，即同样的数据集合训练两遍结果是不一样的，也就是模型不可稳定复现，这对评估是很大挑战，比如很难说一个模型变好是因为你选用了更好的feature，还是由于这次sample的随机因素。 四、Shrinkage Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。用方程来看更清晰，即 没用Shrinkage时：（yi表示第i棵树上y的预测值， y(1~i)表示前i棵树y的综合预测值） y(i+1) = 残差(y1~yi)， 其中： 残差(y1~yi) = y真实值 – y(1 ~ i) y(1 ~ i) = SUM(y1, …, yi) Shrinkage不改变第一个方程，只把第二个方程改为： y(1 ~ i) = y(1 ~ i-1) + step * yi 即Shrinkage仍然以残差作为学习目标，但对于残差学习出来的结果，只累加一小部分（step*残差）逐步逼近目标，step一般都比较小，如0.01~0.001（注意该step非gradient的step），导致各个树的残差是渐变的而不是陡变的。直觉上这也很好理解，不像直接用残差一步修复误差，而是只修复一点点，其实就是把大步切成了很多小步。本质上，Shrinkage为每棵树设置了一个weight，累加时要乘以这个weight，但和Gradient并没有关系。这个weight就是step。就像Adaboost一样，Shrinkage能减少过拟合发生也是经验证明的，目前还没有看到从理论的证明。 五、 GBDT的适用范围 该版本GBDT几乎可用于所有回归问题（线性/非线性），相对logistic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。 六、 搜索引擎排序应用 RankNet 搜索排序关注各个doc的顺序而不是绝对值，所以需要一个新的cost function，而RankNet基本就是在定义这个cost function，它可以兼容不同的算法（GBDT、神经网络…）。 实际的搜索排序使用的是LambdaMART算法，必须指出的是由于这里要使用排序需要的cost function，LambdaMART迭代用的并不是残差。Lambda在这里充当替代残差的计算方法，它使用了一种类似Gradient*步长模拟残差的方法。这里的MART在求解方法上和之前说的残差略有不同，其区别描述见这里。 就像所有的机器学习一样，搜索排序的学习也需要训练集，这里一般是用人工标注实现，即对每一个(query,doc) pair给定一个分值（如1,2,3,4）,分值越高表示越相关，越应该排到前面。然而这些绝对的分值本身意义不大，例如你很难说1分和2分文档的相关程度差异是1分和3分文档差距的一半。相关度本身就是一个很主观的评判，标注人员无法做到这种定量标注，这种标准也无法制定。但标注人员很容易做到的是”AB都不错，但文档A比文档B更相关，所以A是4分，B是3分“。RankNet就是基于此制定了一个学习误差衡量方法，即cost function。具体而言，RankNet对任意两个文档A,B，通过它们的人工标注分差，用sigmoid函数估计两者顺序和逆序的概率P1。然后同理用机器学习到的分差计算概率P2（sigmoid的好处在于它允许机器学习得到的分值是任意实数值，只要它们的分差和标准分的分差一致，P2就趋近于P1）。这时利用P1和P2求的两者的交叉熵，该交叉熵就是cost function。它越低说明机器学得的当前排序越趋近于标注排序。为了体现NDCG的作用（NDCG是搜索排序业界最常用的评判标准），RankNet还在cost function中乘以了NDCG。 好，现在我们有了cost function，而且它是和各个文档的当前分值yi相关的，那么虽然我们不知道它的全局最优方向，但可以求导求Gradient，Gradient即每个文档得分的一个下降方向组成的N维向量，N为文档个数（应该说是query-doc pair个数）。这里仅仅是把”求残差“的逻辑替换为”求梯度“，可以这样想：梯度方向为每一步最优方向，累加的步数多了，总能走到局部最优点，若该点恰好为全局最优点，那和用残差的效果是一样的。这时套到之前讲的逻辑，GDBT就已经可以上了。那么最终排序怎么产生呢？很简单，每个样本通过Shrinkage累加都会得到一个最终得分，直接按分数从大到小排序就可以了（因为机器学习产生的是实数域的预测分，极少会出现在人工标注中常见的两文档分数相等的情况，几乎不同考虑同分文档的排序方式） 另外，如果feature个数太多，每一棵回归树都要耗费大量时间，这时每个分支时可以随机抽一部分feature来遍历求最优（ELF源码实现方式）。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从损失函数看-xgboost、GBDT、adaboost]]></title>
    <url>%2F2018%2F06%2F05%2F%E4%BB%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9C%8B-xgboost%E3%80%81GBDT%E3%80%81adaboost%2F</url>
    <content type="text"><![CDATA[adaboost和GBDT和xgboost在损失函数的最优化方法是有很多不同的，三者的不同之处其实就在于最优化方法的不同（这样说不知道是否妥当，至少站在这个角度的我认为是正确的，多年后可能发现这个观点不太妥当）。adaboost在李航博士的《统计学习基础》里面用加法模型和向前算法解释了权值更新策略。在解释的过程中，样本权值更新和弱分类器权值的求取是直接通过偏导数等于零来计算的，如果记不清楚的可以回去仔细看一下。不管是做回归还是做分类，adaboost的目标都是找到一个值（通过使得偏导数等零的方法）直接使得损失函数降低到最小。 而GBDT使用的是梯度下降法使得损失函数有所降低。首先说一下为什么会提出这种方法，然后说一下为什么GBDT是使用梯度下降法将损失函数有所降低。这种方法的出现肯定是因为对于某些损失函数，偏导数等于零是可解的，而对于一般的损失函数偏导数等于零是无法求解的，不然梯度下降法也不会存在了。而至于为什么GBDT使用的是梯度下降法将损失函数降低的。首先将损失函数在ft-1（x）处做一阶泰勒展开（这里的ft-1（x）就是前t-1个弱分类器的整合），展开之后是一个常数C 和 梯度g和新的弱分类器f的乘积（就是L = C + gf），这时候如果要让损失函数L变得更小，就要让梯度g和弱分类器f的乘积变得更小，所以就是用负梯度来拟合新的弱分类器。 如果把弱分类器当做步长，那么损失函数就代表沿着梯度走了一步。如果新的弱分类器跟负梯度完美拟合，那么损失函数就变成了L = C + g（-f）= C-gg，所以这时候损失函数是下降了的。如果是这样的话，只要保证新的弱分类器f的向量方向和梯度g相反，而向量模长尽可能大不就行了吗？答案当然是不行的，因为我们做的是损失函数在ft-1（x）的泰勒展开啊，如果步长太长的话沿着梯度相反的方向走一步，误差就大了。所以步长也要小一点。在这不知道为什么定义步长f的模和梯度的模相等，可能Freidman有所考虑的吧,如果后续让我来改进，我就从该地方入手，选择多长的步长也就是-a*g来拟合新的弱分类器f，其中a&gt;0。 xgboost损失函数同样是在ft-1（x）处做泰勒展开，不同的是做的二阶泰勒展开，而且还附带了对树叶子节点约束、权值模长（其实可以考虑为步长）约束的正则项。当然在GBDT里面一样可以做这些正则项约束，这里面并不是xgboost特有的，但是不同的地方就是xgboost损失函数在ft-1（x）处做二阶泰勒展开。对就是从二阶泰勒展开入手去最优化损失函数的。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 博客配置]]></title>
    <url>%2F2018%2F06%2F04%2Fhexo%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[本博客选用了 hexo 中 Next主题。在 Next 主题中，采用了font awesome 字体库。 相关的对照表，如font awesome所示。 在博客中设置分类和标签是非常重要的操作。 首先新建一个 tags 页面 12$ cd your-hexo-site$ hexo new page tags 进入刚刚生成的页面，一般为 index 文件，修改 type 类型为tags 即可。如下： 1234title: 标签date: 2014-12-22 12:39:04type: "tags"--- 对于分类 categories 同理，只需要将 type 类型改为 categories 即可。但是需要注意一点，对于 tags 没有层级区分。而 categories 则有着严格的先后级关系。 可以修改 /scaffolds/post.md 模板，加入 tags、 categories 的标签。如下： 123456---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags:categories:---]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu12.04 Hadoop+Spark 分布式部署步骤]]></title>
    <url>%2F2018%2F06%2F04%2Fubuntu12-04-Hadoop-Spark-%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[1、准备工具： 虚拟机：vmware 操作系统 ：ubuntu12.04 （桌面版或server版）(iso文件) java：jdk-7u75-linux-x64.tar.gz oracle官网可下载 hadoop：haopop2.5.2 spark: spark1.1.1 登陆终端 secure crt 文件传输：file zilla 2、具体步骤 （1）、安装vmware —过程略 （2）、新建虚拟机（3个）安装ubuntu系统 —过程略 注：下面的步骤无特殊说明则需要在三个虚拟机上同时执行 （3）、apt-get 更新： apt-get update （4）、安装vim openssh客户端，服务端 apt-get instsall vim openssh-client openssh-server 判断ssh是否正常启动 命令：ps -e |grep ssh 如果既有sshd 又有ssh-agent 则ssh启动成功 （5）、安装java环境 假定准备将java安装在/usr/lib/java中 则建立该目录：mkdir /usr/lib/java 通过file zilla 将压缩文件传送到虚拟机上 解压jdk tar -xzvf jdkxxxxxx 设置环境变量： 编辑 /etc/profile文件，在其中加入以下内容 export JAVA_HOME=/usr/lib/java/jdk1.7.0_75 export JRE_HOME={$JAVA_HOME}/jre 保存退出 令其生效 source /etc/profile 此时查看java版本 java -version 如果系统中有多个java环境则需要手动设置默认的jdk 命令如下： sudo update-alternatives –install /usr/bin/javac javac /usr/lib/java/jdk1.7.0_75/bin/javac 300 sudo update-alternatives –install /usr/bin/java java /usr/lib/java/jdk1.7.0_75/bin/java 300 sudo update-alternatives –install /usr/bin/jps jps /usr/lib/java/jdk1.7.0_75/bin/jps 300 此时在输入java -version就可以看到上图的结果了 （6）、修改hostname+配置hosts 修改hostname:编辑/etc/hostname文件分别将三台机器的hostname改成master slave1 slave2 之后重启服务器 配置hosts：编辑/etc/hosts，添加 ip 和对应的hostname。 保存退出之后验证三台机器直接ping 各自的hostname是否互通 （7）、配置ssh hadoop是通过ssh来进行节点间的通信，所以要去掉节点间ssh的密码，所以需要对ssh进行配置 a）、生成公钥：命令：ssh-keygen -t rsa -P “” b）、将master公钥保存在authorized_keys下 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 对于本机可用ssh localhost来验证是否设置成功 c）、将另外两台机器的公钥生成后通过scp命令汇聚到master下命令如下：在目录/root/.ssh中 scp id_rsa.pub root@master:/root/.ssh/id_rsa.pub.slave1(2) d）、将所有公钥全部保存在authorized_keys下命令如b e）、将设置好的authorized_keys文件发送给两个slave节点，之后验证三台机器是否可以ssh无密码登陆 （8）、安装hadoop 解压之前的压缩文件（tar -zxvf） 设置环境变量：在/etc/profile中添加相应变量 export HADOOP_COMMON_HOME=/usr/local/hadoopspark/hadoop-2.5.2 export HADOOP_CONF_DIR=/usr/local/hadoopspark/hadoop-2.5.2/etc/hadoop export YARN_CONF_DIR=/usr/local/hadoopspark/hadoop-2.5.2/etc/hadoop export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_COMMON_HOME/lib/native export HADOOP_OPTS=”-Djava.library.path=$HADOOP_COMMON_HOME/lib” export CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar export PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin:${HADOOP_COMMON_HOME}/bin:${HADOOP_COMMON_HOME}/sbin:$PATH 保存并使其生效。 设置hadoop-env.sh（文件位于hadoop归档后的目录的etc/hadoop/中）： 加入java环境变量 export JAVA_HOME=/usr/lib/java/jdk1.7.0_75 此时可验证hadoop是否安装成功：hadoop version 若能出现hadoop信息说明安装基本成功 （9）、配置hadoop信息： 主要配置一下几个文件 配置之前先建立几se个目录用来保存hadoop执行过程中的相关信息：mkdir tmp hdfs hdfs/name hdfs/data 1）、etc/hadoop/core-site.xml 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/hadoop-2.7.0/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2)、etc/hadoop/mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3）、etc/hadoop/hdfs-site.xml 1234567891011121314151617181920&lt;value&gt;Master:50090&lt;/value&gt;是网页访问地址/home/hadoop/tools/hadoop-2.7.0/&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;Master:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/hadoop-2.7.0/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/hadoop-2.7.0/hdfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4）、etc/hadoop/yarn-site.xml 12345678910111213141516171819202122232425262728&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;Master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;Master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;Master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;Master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;Master:8088&lt;/value&gt; &lt;/property&gt;&lt;!-- Site specific YARN configuration properties --&gt;&lt;/configuration&gt; （5）、etc/hadoop/slaves 注意要删除自己本身的节点，或者在配置互信时，加入自己的密钥。 配置完毕则可以准备0启动hadoop 下面的过程在master节点上执行相关操作 namenode的格式化：hadoop namenode -format 之后到hadoop所在目录启动hadoop：sbin/start-all.sh 一小段等待后hadoop启动，用jps查看 123456DataNodeJpsSecondaryNameNodeNameNodeNodeManagerResourceManager 若出现下面信息则启动成功，可通过web-browser查看相关信息url:http://masterip:50070 关闭hadoop服务：sbin/stop-all-sh 若需要重新格式化namenode则需要删除掉tmp 和/tmp/hadoop* 和hdfs/name hdfs/data中的内容 至此hadoop 分布式部署成功，下面准备在此基础上安装spark（由于spark的工作需要基于hadoop中的hdfs分布式文件系统） （10）、安装scala 解压下载好的压缩文件（tar -zxvf） 配置环境变量（/etc/profile） export SCALA_HOME=/usr/lib/scala/scala-2.11.4 验证：键入scala：出现scala命令控制行说明安装成功 （11）、安装spark 解压文件–过程略 配置环境变量：编辑conf下的spark-env.sh文件将java scala相关环境变量加入并且配置masterip以及各个worker节点的内存 export JAVA_HOME=/usr/lib/java/jdk1.7.0_75 export SCALA_HOME=/usr/lib/scala/scala-2.11.4 export SPARK_MASTER_IP=192.168.194.129 export SPARK_WORKER_MEMORY=2g export SPARK_CONF_DIR=/usr/local/hadoopspark/spark-1.1.1-bin-hadoop2.4/conf 配置同目录下的slaves 将所有节点的hostname添加进该文件（三台机器均配置） 配置完毕启动spark sbin/start-all.sh jps命令查看若新增这两个进程即spark成功启动（Master Worker） 至此spark+hadoop的分布式部署完成]]></content>
      <categories>
        <category>大数据-分布式</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 文件结构]]></title>
    <url>%2F2018%2F06%2F04%2Fhexo-%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[hexo 文件结构 接着上篇如何发布一篇博客，本篇将分析以下hexo框架中的基本文件结构。 在hexo框架中主要目录结构如下： 12345678|-- _config.yml|-- package.json|-- scaffolds|-- source |-- _posts|-- themes|-- .gitignore|-- package.json _config.yml：全局配置文件，网站的很多信息都在这里配置，诸如网站名称，副标题，描述，作者，语言，主题，部署等等参数。 package.json：hexo框架的参数和所依赖插件。 scaffolds：scaffolds是“脚手架、骨架”的意思，当你新建一篇文章（hexo new 'title'）的时候，hexo是根据这个目录下的文件进行构建的。基本不用关心。 source： 这个目录很重要，新建的文章都是在保存在这个目录下的._posts 。需要新建的博文都放在 _posts 目录下。 _posts 目录下是一个个 markdown 文件。你应该可以看到一个 hello-world.md 的文件，文章就在这个文件中编辑。 _posts 目录下的md文件，会被编译成html文件，放到 public （此文件现在应该没有，因为你还没有编译过）文件夹下。 themes：网站主题目录，hexo有非常好的主题拓展，支持的主题也很丰富。该目录下，每一个子目录就是一个主题。 _config.yml 文件的具体说明 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: Hexo #网站标题subtitle: #网站副标题description: #网站描述author: John Doe #作者language: #语言timezone: #网站时区。Hexo 默认使用您电脑的时区。时区列表。比如说：America/New_York, Japan, 和 UTC 。# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://yoursite.com #你的站点Urlroot: / #站点的根目录permalink: :year/:month/:day/:title/ #文章的 永久链接 格式 permalink_defaults: #永久链接中各部分的默认值# Directory source_dir: source #资源文件夹，这个文件夹用来存放内容public_dir: public #公共文件夹，这个文件夹用于存放生成的站点文件。tag_dir: tags # 标签文件夹 archive_dir: archives #归档文件夹category_dir: categories #分类文件夹code_dir: downloads/code #Include code 文件夹i18n_dir: :lang #国际化（i18n）文件夹skip_render: #跳过指定文件的渲染，您可使用 glob 表达式来匹配路径。 # Writingnew_post_name: :title.md # 新文章的文件名称default_layout: post #预设布局titlecase: false # 把标题转换为 title caseexternal_link: true # 在新标签中打开链接filename_case: 0 #把文件名称转换为 (1) 小写或 (2) 大写render_drafts: false #是否显示草稿post_asset_folder: false #是否启动 Asset 文件夹relative_link: false #把链接改为与根目录的相对位址 future: true #显示未来的文章highlight: #内容中代码块的设置 enable: true line_number: true auto_detect: false tab_replace:# Category &amp; Tagdefault_category: uncategorizedcategory_map: #分类别名tag_map: #标签别名# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DD #日期格式time_format: HH:mm:ss #时间格式 # Pagination## Set per_page to 0 to disable paginationper_page: 10 #分页数量pagination_dir: page # Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: landscape #主题名称# Deployment## Docs: https://hexo.io/docs/deployment.html# 部署部分的设置deploy:]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何发布一篇博客]]></title>
    <url>%2F2018%2F06%2F04%2F%E5%A6%82%E4%BD%95%E5%8F%91%E5%B8%83%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[如何发布一篇博客 自己曾经搭建过无数个博客。从开始自己使用yaf+php自己写编写，到采用GitHub+jeklly。后来也用过腾讯云服务器+WordPress的模式。最近知道了一种新的博客搭建方法，GitHub+hexo的模式。hexo框架有着丰富的主题，并且使用方便。马上要工作了，这次花费了几天搭建了一下新的博客。未来也会慢慢把自己原先的博客，慢慢迁移到这里。 hexo+GitHub搭建比较简单见大神教程 本文主要分析了一下，博客发布的过程。 1执行代码：hexo new [layout] &lt;title&gt; 这里 [layout] 是指布局格式，默认的布局格式为post。在hexo 中一共有3中不同的默认格式： 布局格式 路径 post source/_posts Page source Draft source/_drafts 默认布局格式为 post。先以布局格式为 post 的情况为说明。 首先会在在根目录的下的 source 文件夹中创建了一个 _post 文件夹，并在里面生成一个对应的文件。例如myBlog.md。这时候文件会根据模板，填入相应信息。模板存储在 scaffolds 中，可以根据需要自行修改。 根据不同的模板，决定了 Front-matter 的内容。 Front-matter 是文件最上方以 --- 分隔的区域，用于指定个别文件的变量。例如： 123456---title: 如何发布一篇博客date: 2018-06-04 01:35:26tags: hexocategories: blog--- 在 Front-matter 中有以下预定参数: 参数 描述 默认值 layout 布局 title 标题&lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space:pre&quot;&gt;&lt;/span&gt; date 建立日期 &lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space:pre&quot;&gt;&lt;/span&gt;文件建立日期 updated 更新日期 &lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space:pre&quot;&gt;&lt;/span&gt;文件更新日期 comments 开启文章的评论功能 true tags 标签（不适用于分页） categories 分类（不适用于分页） permalink 覆盖文章网址 其中 tags 都是并列的。categories 则有严格的顺序性和层次性。例如 12345categories:- blogtags:- 技术- 搭建]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
