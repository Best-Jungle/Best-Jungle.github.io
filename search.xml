<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Mac & Linux ÈÖçÁΩÆÊñá‰ª∂ bash_profile]]></title>
    <url>%2F2018%2F06%2F06%2FMac-Linux-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-bash-profile%2F</url>
    <content type="text"><![CDATA[ÊúÄËøëÂºÄÂßãÊâæÂÆû‰π†‰∫ÜÔºåÂæàÂ§ö‰∫ãÊÉÖÂæàÂøôÔºåÊâÄ‰ª•ÂçöÂÆ¢Âæà‰πÖÊ≤°Êõ¥‰∫Ü„ÄÇÂú®Á¨îËØïÁöÑËøáÁ®ã‰∏≠ÂèëÁé∞Ë¶ÅËÄÉ PythonÔºåÂæà‰πÖ‰ª•ÂâçÂ≠¶ËøáÔºå‰ΩÜÊòØÊÑüËßâËá™Â∑±Âü∫Êú¨ÂøòÂÖâ„ÄÇÊâÄ‰ª•‰ªéÂ§¥Êç°Ëµ∑Êù•Âêß„ÄÇÁªôÂ§ßÂÆ∂‰∏Ä‰∏™Âª∫ËÆÆÔºåÂú®Â≠¶ÁîüÈò∂ÊÆµÊúÄÂ•ΩË¶ÅÊúâ‰∏ÄÊù°‰∏ªÁ∫ø‰ªªÂä°„ÄÇÂÉèÊàëÂ∞±ÊòØÊîØÁ∫øÂâßÊÉÖÊâìÂ§™Â§öÔºåÊäÄËÉΩüå≤ÈÉΩÁÇπ‰∫ÜÔºåÊ≤°ÊúâÂ§™Ê∑±ÂÖ•ÁöÑÔºåÊâÄ‰ª•ÊÑüËßâÁé∞Âú®ÊâæÂÆû‰π†ÁöÑÊó∂ÂÄôÂ∞±ÊØîËæÉÈªë‰∫Ü„ÄÇÂ•ΩÂï¶Âï∞Âó¶ÁöÑËØùËØ¥ÂÆå‰∫ÜÔºå‰∏ãÈù¢‰∏ªË¶Å‰ªãÁªç‰∏Ä‰∏ã Mac ÁöÑÈÖçÁΩÆÊñá‰ª∂Âä†ËΩΩÔºåÁéØÂ¢ÉÂèòÈáèÂïäÔºå‰ªÄ‰πàÁöÑ„ÄÇLinux Âíå Mac Á±ª‰ººÔºåÂ§ßÂÆ∂ÂèØ‰ª•Êâ©Â±ïÈòÖËØª‰∏Ä‰∏ãÔºåËøôÈáåÂ∞±‰∏çÂ±ïÂºÄÁªÜËØ¥‰∫Ü„ÄÇ ÊàëËÆ§‰∏∫Á®ãÂ∫èÂëòÁî® Mac ÂÅöÂºÄÂèëË¶ÅÊØîÁî® Windows Êú¨Êõ¥Â•Ω‰∏ÄÁÇπÔºåÂõ†‰∏∫Ëá™Â∏¶‰∫ÜÂæàÂ§öÊèí‰ª∂ÔºåÊØîÂ¶Ç Mac Â∞±ÂÆâË£Ö‰∫Ü PythonÔºåÁâàÊú¨ÊòØÁ®≥ÂÆöÁöÑ2.7„ÄÇ‰ΩÜÊòØÂõ†‰∏∫ÊòØÁ≥ªÁªüËá™Â∏¶ÁöÑÔºåÊâÄ‰ª•‰πü‰∏çÂÖÅËÆ∏Áî®Êà∑Ëá™Â∑±‰øÆÊîπ„ÄÇÊàëËøô‰πàÁà±ÊäòËÖæÁöÑ‰∫∫ÔºåÁÆ°‰ªñÊúâÁî®Ê≤°Áî®ÔºåÊàëÈÉΩÊÉ≥ÊîπÊîπÔºåÊúâÁÇπËá™Â∑±ÁöÑÂΩ±Â≠êÊâçÂ•Ω„ÄÇÊâÄ‰ª•ÂÆâË£Ö‰∫ÜÊñ∞ÁöÑ3.5ÁöÑÁâàÊú¨Ôºå‰ΩÜÊòØÊÇ≤ÂâßÁöÑÊòØ‰∏çÁÆ°ÊàëÊÄé‰πàË∞ÉÔºå‰πüÊ≤°ÊúâÂäûÊ≥ïÊîπÂèòÂΩìÂâç Python ÁöÑÁºñËØëÁéØÂ¢É„ÄÇÊúÄÂêéÂú®Áü•‰πé‰∏äÊúâÂ§ßÁ•ûÁªôÊÑèËßÅÔºåÂú®ÂÖ®Â±ÄÂèòÈáèÈáåÈù¢‰øÆÊîπ„ÄÇÂõ†Ê≠§‰∏ãÈù¢‰ªãÁªç‰∏Ä‰∏ã Mac ÁöÑÈÖçÁΩÆÊñá‰ª∂Âä†ËΩΩÁöÑÈóÆÈ¢ò„ÄÇ MacÁ≥ªÁªüÁöÑÁéØÂ¢ÉÂèòÈáèÔºåÂä†ËΩΩÈ°∫Â∫è‰∏∫Ôºö 123456/etc/profile /etc/paths ~/.bash_profile ~/.bash_login ~/.profile ~/.bashrc ÂΩìÁÑ∂/etc/profileÂíå/etc/pathsÊòØÁ≥ªÁªüÁ∫ßÂà´ÁöÑÔºåÁ≥ªÁªüÂêØÂä®Â∞±‰ºöÂä†ËΩΩÔºåÂêéÈù¢Âá†‰∏™ÊòØÂΩìÂâçÁî®Êà∑Á∫ßÁöÑÁéØÂ¢ÉÂèòÈáè„ÄÇÂêéÈù¢3‰∏™ÊåâÁÖß‰ªéÂâçÂæÄÂêéÁöÑÈ°∫Â∫èËØªÂèñÔºåÂ¶ÇÊûú~/.bash_profileÊñá‰ª∂Â≠òÂú®ÔºåÂàôÂêéÈù¢ÁöÑÂá†‰∏™Êñá‰ª∂Â∞±‰ºöË¢´ÂøΩÁï•‰∏çËØª‰∫ÜÔºåÂ¶ÇÊûú~/.bash_profileÊñá‰ª∂‰∏çÂ≠òÂú®ÔºåÊâç‰ºö‰ª•Ê≠§Á±ªÊé®ËØªÂèñÂêéÈù¢ÁöÑÊñá‰ª∂„ÄÇ~/.bashrcÊ≤°Êúâ‰∏äËø∞ËßÑÂàôÔºåÂÆÉÊòØbash shellÊâìÂºÄÁöÑÊó∂ÂÄôËΩΩÂÖ•ÁöÑ„ÄÇ ÂÖ®Â±ÄËÆæÁΩÆ ‰∏ãÈù¢ÁöÑÂá†‰∏™Êñá‰ª∂ËÆæÁΩÆÊòØÂÖ®Â±ÄÁöÑÔºå‰øÆÊîπÊó∂ÈúÄË¶ÅrootÊùÉÈôê 1Ôºâ/etc/paths ÔºàÂÖ®Â±ÄÂª∫ËÆÆ‰øÆÊîπËøô‰∏™Êñá‰ª∂ Ôºâ ÁºñËæë pathsÔºåÂ∞ÜÁéØÂ¢ÉÂèòÈáèÊ∑ªÂä†Âà∞ pathsÊñá‰ª∂‰∏≠ Ôºå‰∏ÄË°å‰∏Ä‰∏™Ë∑ØÂæÑ HintÔºöËæìÂÖ•ÁéØÂ¢ÉÂèòÈáèÊó∂Ôºå‰∏çÁî®‰∏Ä‰∏™‰∏Ä‰∏™Âú∞ËæìÂÖ•ÔºåÂè™Ë¶ÅÊãñÂä®Êñá‰ª∂Â§πÂà∞ Terminal ÈáåÂ∞±ÂèØ‰ª•‰∫Ü„ÄÇ 2Ôºâ/etc/profile ÔºàÂª∫ËÆÆ‰∏ç‰øÆÊîπËøô‰∏™Êñá‰ª∂ Ôºâ ÂÖ®Â±ÄÔºàÂÖ¨ÊúâÔºâÈÖçÁΩÆÔºå‰∏çÁÆ°ÊòØÂì™‰∏™Áî®Êà∑ÔºåÁôªÂΩïÊó∂ÈÉΩ‰ºöËØªÂèñËØ•Êñá‰ª∂„ÄÇ 3Ôºâ/etc/bashrc Ôºà‰∏ÄËà¨Âú®Ëøô‰∏™Êñá‰ª∂‰∏≠Ê∑ªÂä†Á≥ªÁªüÁ∫ßÁéØÂ¢ÉÂèòÈáèÔºâ ÂÖ®Â±ÄÔºàÂÖ¨ÊúâÔºâÈÖçÁΩÆÔºåbash shellÊâßË°åÊó∂Ôºå‰∏çÁÆ°ÊòØ‰ΩïÁßçÊñπÂºèÔºåÈÉΩ‰ºöËØªÂèñÊ≠§Êñá‰ª∂„ÄÇ]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux Áî®Êà∑ÊùÉÈôêÂàíÂàÜ]]></title>
    <url>%2F2018%2F06%2F06%2Flinux-%E7%94%A8%E6%88%B7%E6%9D%83%E9%99%90%E5%88%92%E5%88%86%2F</url>
    <content type="text"><![CDATA[linux Áî®Êà∑ÊùÉÈôêÂàíÂàÜ Ê∑ªÂä†ÁªÑÁöÑÂëΩ‰ª§Ôºö groupadd ÁªÑÂêç „ÄÇ ÔºàÂú®rootÁÆ°ÁêÜÊùÉÈôêÔºâ Êü•Áúãlinux‰∏≠ÊâÄÊúâÁªÑÁöÑ‰ø°ÊÅØÔºö cat /etc/group „ÄÇ ÂàõÂª∫Áî®Êà∑ÔºåÂπ∂ÂêåÊó∂ÊåáÂÆöÂ∞ÜËØ•Áî®Êà∑ÂàÜÈÖçÂà∞Âì™‰∏™ÁªÑÈáåÔºö useradd -g ÁªÑÂêç Áî®Êà∑Âêç„ÄÇ ÔºàÂú®rootÁÆ°ÁêÜÊùÉÈôêÔºâ Êü•Áúãlinux‰∏≠ÊâÄÊúâÁî®Êà∑ÁöÑ‰ø°ÊÅØÔºö cat /etc/passwd Êõ¥ÊîπÊüê‰∏™Áî®Êà∑ÊâÄÂú®ÁöÑÁªÑÔºö usermod -g ÁªÑÂêç Áî®Êà∑Âêç„ÄÇ ÔºàÂú®rootÁÆ°ÁêÜÊùÉÈôêÔºâ ÊùÉÈôêÂê´‰πâËØ¶Ëß£Ôºö ÊùÉÈôêÂàÜ‰∏∫‰∏âÁßçÔºö r ‰ª£Ë°®ÂèØËØªÔºåÁî®4Ë°®Á§∫„ÄÇ w ‰ª£Ë°®ÂèØÂÜôÔºåÁî®2Ë°®Á§∫„ÄÇ x ‰ª£Ë°®ÂèØÊâßË°åÔºåÁî®1Ë°®Á§∫„ÄÇ -rw-r‚Äìr‚Äì ÁöÑËß£ÈáäÂ¶Ç‰∏ã, Ôºö Á¨¨‰∏Ä‰ΩçÁöÑ‚Äò-‚Äô ‰ª£Ë°®Êñá‰ª∂Á±ªÂûã„ÄÇ „Äê‚Äôd‚Äô ‰ª£Ë°®ÁõÆÂΩïÔºå‚Äô|‚Äô ‰ª£Ë°®ÈìæÊé•„Äë„ÄÇ Á¨¨‰∫å‰ΩçÂà∞Á¨¨Âõõ‰ΩçÁöÑ‚Äòrw-‚ÄôÔºå‰ª£Ë°®ËØ•Êñá‰ª∂ÁöÑÊâÄÊúâËÄÖÂØπËØ•Êñá‰ª∂ÁöÑÊùÉÈôê„ÄÇ Á¨¨‰∫î‰ΩçÂà∞Á¨¨‰∏É‰ΩçÁöÑ‚Äòr‚Äì‚ÄôÔºå‰ª£Ë°®ËØ•Êñá‰ª∂ÊâÄÂú®ÁöÑÁªÑÁöÑÂÖ∂‰ªñÁî®Êà∑ÂØπËØ•Êñá‰ª∂ÁöÑÊùÉÈôê„ÄÇ Á¨¨ÂÖ´‰ΩçÂà∞Á¨¨ÂçÅ‰ΩçÁöÑ‚Äòr‚Äì‚ÄôÔºå‰ª£Ë°®ÂÖ∂‰ªñÁªÑÁöÑÁî®Êà∑ÂØπËØ•Êñá‰ª∂ÁöÑÊùÉÈôê„ÄÇ chmodÂëΩ‰ª§Ôºö‰ΩøÁî®‰æãÔºö chmod 760 ÁõÆÂΩï/Êñá‰ª∂Ôºå ÊÑèÊÄùÊòØ ÁªôËØ•ÁõÆÂΩï/Êñá‰ª∂ÁöÑÊâÄÊúâËÄÖÂØπËØ•ÁõÆÂΩï/Êñá‰ª∂Ëµã‰∫àrwxÊùÉÈôêÔºåÁªôËØ•ÁõÆÂΩï/Êñá‰ª∂ÊâÄÂú®ÁöÑÁªÑÁöÑÂÖ∂‰ªñÁî®Êà∑ÂØπËØ•ÁõÆÂΩï/Êñá‰ª∂Ëµã‰∫àrw-ÊùÉÈôêÔºåÁªôÂÖ∂‰ªñÁªÑÁöÑÁî®Êà∑ÂØπËØ•ÁõÆÂΩï/Êñá‰ª∂Ëµã‰∫à‚ÄîÊùÉÈôê„ÄÇ chmod 755 abc ÔºöËµã‰∫àabcÊùÉÈôêrwxr-xr-x„ÄÇ chmod u=rwx,g=rx,o=rx abc ÔºöÂêå‰∏ä u=Áî®Êà∑ÊùÉÈôê g=ÁªÑÊùÉÈôê o=‰∏çÂêåÁªÑÂÖ∂‰ªñÁî®Êà∑ÊùÉÈôê„ÄÇ chmod u-x,g+w abc ÔºöÁªôabcÂéªÈô§Áî®Êà∑ÊâßË°åÁöÑÊùÉÈôêÔºåÂ¢ûÂä†ÁªÑÂÜôÁöÑÊùÉÈôê„ÄÇ chmod a+r abc ÔºöÁªôÊâÄÊúâÁî®Êà∑Ê∑ªÂä†ËØªÁöÑÊùÉÈôê„ÄÇ ÊîπÂèòÊã•ÊúâËÄÖÔºàchownÔºâÂíåÁî®Êà∑ÁªÑÔºàchgrpÔºâÂëΩ‰ª§Ôºö chown xiaoming abc ÔºöÊîπÂèòabcÁöÑÊã•ÊúâËÄÖ‰∏∫xiaoming„ÄÇ chgrp root abc ÔºöÊîπÂèòabcÊâÄÂ±ûÁöÑÁªÑ‰∏∫root„ÄÇ chown root ./abc Ôºö ÊîπÂèòabcÁõÆÂΩïÁöÑÊâÄÊúâËÄÖÊòØroot„ÄÇ chown -R root ./abc ÔºöÊîπÂèòabcÁõÆÂΩïÂèäÂÖ∂‰∏ãÈù¢ÁöÑÊâÄÊúâÁõÆÂΩïÂíåÊñá‰ª∂ÁöÑÊâÄÊúâËÄÖÊòØroot„ÄÇÂèÇÊï∞-R ÊòØÈÄíÂΩíÁöÑÊÑèÊÄù„ÄÇ CHMOD UÊñá‰ª∂ÊâÄÊúâËÄÖ GÊñá‰ª∂ÊâÄÂú®ÁªÑÁî®Êà∑ OÂÖ∂ÂÆÉÁî®Êà∑ RWX=421 ‰∏â‰∏™‰ΩçÂèØ‰ª•Áõ∏Âä†,ÊùÉÈôêÂè†Âä† ËØªÊùÉÈôêÊòØ4,ÂÜôÊòØ2,ËØªÂÜôÊùÉÈôêÊòØ6 chmod U+R install.log Áªôinstall.logÊéàËØªÁöÑÊùÉÈôê chmod u=rw install.logÁªôinstall.logÊéàËØªÂÜôÁöÑÊùÉÈôê ‰πüÂèØ‰ª•Áî®‰∏ãÈù¢ÁöÑÊñπÂºè chmod 666 install.logÁªôinstall.logÁªôÊâÄÊúâËÄÖÁî®Êà∑,ÁªÑÁî®Êà∑,ÂÖ∂ÂÆÉÁî®Êà∑ÂÖ®ÈÉ®ÊéàËØªÂÜôÊùÉÈôê chmod -R 666 test ÁªôtestÊñá‰ª∂Â§πÊâÄÊúâËÄÖ,ÁªÑÁî®Êà∑,ÂÖ∂ÂÆÉÁî®Êà∑ÂÖ®ÈÉ®ÊéàËØªÂÜôÊùÉÈôê,testÊñá‰ª∂Â§πÂíåÂ≠êÊñá‰ª∂Â§πÊã•ÊúâÂêåÊ†∑ÁöÑÊéàÊùÉ,Ê≥®ÊÑè-RÂøÖÈ°ªÂ§ßÂÜô]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[500Ëã±ÈáåÁöÑ BUG]]></title>
    <url>%2F2018%2F06%2F05%2F500%E8%8B%B1%E9%87%8C%E7%9A%84-BUG%2F</url>
    <content type="text"><![CDATA[500Ëã±ÈáåÁöÑBUG BUGÊòØ‰∏Ä‰∏™Á®ãÂ∫èÂëòÂøÖÁÑ∂ÈÅáÂà∞ÁöÑÈóÆÈ¢òÔºåÊàëÊÉ≥ËØ¥‰ªª‰ΩïBUGÈÉΩÊúâÁùÄËÉåÂêéÂ•áÊÄ™ÁöÑÈÄªËæëÔºÅÊêûÊ∏ÖÊ•öËøô‰∫õÂØπËá™Â∑±ÁúüÁöÑÊòØ‰∏ÄÁßç„ÄÇ„ÄÇ‰∫´Âèó --ÁéãÊ†éÊ±â2018Âπ¥2Êúà2Êó•‰∫éÂçóÂ±± ÂèëÁîü‰∫éÈ∫ªÁúÅÁêÜÂ∑•ÁöÑ‰∏Ä‰∏™ÊúâÊÑèÊÄùÁöÑbugÔºöÂè™ËÉΩÂèë500Ëã±ÈáåÁöÑÈÇÆ‰ª∂„ÄÇÁõ∏ÂΩìNerdÁöÑbugÔºåÊúâÂÖ¥Ë∂£ÁöÑ‰∫∫ÁúãÁúãÂêß„ÄÇ Â§ßÊÑèÊòØÔºåÂΩìÂπ¥È∫ªÁúÅÁöÑ‰∏ÄÂêçÁ≥ªÁªüÁÆ°ÁêÜÂëòÔºåÂøΩÁÑ∂Êî∂Âà∞ÁªüËÆ°Á≥ª‰∏ª‰ªªÊâìÊù•ÁöÑÊ±ÇÂä©ÁîµËØù‚ÄúÂí±‰ª¨ÁöÑÈÇÆ‰ª∂Âèë‰∏ç‰∫Ü500Ëã±Èáå‰ª•Â§ñÁöÑÂú∞ÊñπÔºåÂÖ∂ÂÆûÔºåÊòØ520Ëã±ÈáåÊõ¥ÂáÜÁ°ÆÁÇπ‚Äù„ÄÇ Á≥ªÁªüÁÆ°ÁêÜÂëòÂøÉÈáåÔø•ÔºÅ&amp;‚Ä¶‚Ä¶*&amp;„ÄÇ ‰∏çËøáÂú®‰ªñÂºÄÂßãÁî®Ëá™Â∑±ÁöÑÈÇÆ‰ª∂ÊµãËØïÂêéÔºåÂèëÁé∞ÈÇÆ‰ª∂ÁöÑÁ°ÆÂè™ËÉΩÂèëÂæÄ520Ëã±Èáå‰ª•ÂÜÖÔºåÂÖ∂‰ΩôÁöÑÊî∂‰ª∂Âú∞ÁÇπ‰∏ÄÂæãÂ§±Ë¥•„ÄÇ ‰∫éÊòØÂú®‰ªñ‰∏ÄÁâáÁ∫†Áªì‰∏≠‰ªñÊ∏êÊ∏êÂºÄÂßãÂèëÁé∞ÈóÆÈ¢òÔºåÈÇÆ‰ª∂ÊúçÂä°Âô®Ë¢´‰∫∫Êõ¥Êñ∞ËøáÊìç‰ΩúÁ≥ªÁªüÔºàÂΩìÂπ¥ËøòÊòØSunOSÔºâÔºå‰ΩÜÊòØÁî±‰∫éÊìç‰ΩúÁ≥ªÁªüÁöÑÂèëË°åÁâàÂæÄÂæÄÈÖçÂ§á‰∫ÜÊóßÁâàËΩØ‰ª∂Ôºå‰∫éÊòØÂú®Êõ¥Êñ∞Êìç‰ΩúÁ≥ªÁªüÁöÑÊó∂ÂÄôÈÇÆ‰ª∂ËΩØ‰ª∂ÂèçËÄåË¢´ÈôçÁ∫ß‰∫ÜÔºàSendmail 8 -&gt; Sendmail 5Ôºâ„ÄÇ ‰∫éÊòØËøõ‰∏ÄÊ≠•Ë∞ÉÊü•ÂèëÁé∞ÔºåÂú®Êõ¥Êñ∞Êìç‰ΩúÁ≥ªÁªüÊó∂ÔºåÁÆ°ÁêÜÂëòËá™Â∑±ÁºñÂÜôÁöÑSendmailÈÖçÁΩÆÊñá‰ª∂Ôºàsendmail.cfÔºâË¢´‰øùÁïô‰∫Ü‰∏ãÊù•„ÄÇËøôÊ†∑Â∞±Âá∫Áé∞‰∫ÜËøôÁßçÁä∂ÂÜµÔºöSendmail 5Â∞ùËØïËß£ÊûêSendmail 8ÁöÑÈÖçÁΩÆÊñá‰ª∂„ÄÇ‰ΩÜÊòØ‰∏∫‰ªÄ‰πà‰ºöÊòØ500milesÂë¢Ôºü‰∏∫‰ªÄ‰πàÊòØ500milesÂíßÔºü ÂéüÂõ†ÊòØËøôÊ†∑ÁöÑÔºåSendmail 5Èù¢ÂØπÈôåÁîüÁöÑÈÖçÁΩÆÊñá‰ª∂ÔºåÂá°ÊòØ‰∏çÁêÜËß£ÁöÑÈÉ®ÂàÜÈÉΩ‰ºöÂøΩÁï•ÔºåÂá°ÊòØÊ≤°ËÆæÁΩÆËøáÁöÑÈÖçÁΩÆÈ°πËá™Âä®ËÆæÁΩÆÊàê0„ÄÇËøôÊ†∑ÂÖ∂‰∏≠Êúâ‰∏Ä‰∏™Ë¢´ËÆæÁΩÆÊàê0ÔºåËøô‰∏ÄÈ°πÂ∞±ÊòØ ÔºàËøûÊé•ËøúÁ´ØSMTPÊúçÂä°Âô®ÁöÑË∂ÖÊó∂Êó∂Èó¥Ôºâtimeout to connect to the remote SMTP server„ÄÇÂêéÊù•ÁªèËøáÂÆûÈ™åÔºåÂèëÁé∞0ÁßíÁöÑtimeout‰ºöÂØºËá¥SendmailÂú®3ÊØ´ÁßíÂêé‰∏≠Êñ≠ËøûÊé•„ÄÇ ÊâÄ‰ª•Ôºå‰∏∫Âï•ÊòØ500milesÔºü Âú®ÂΩìÂπ¥ÔºåMITÁöÑÊ†°Âõ≠ÁΩëÊòØÊ≤°ÊúâÈÇ£‰πàÂ§örouterÁöÑÔºå‰πüÂ∞±Ê≤°ÈÇ£‰πàÂ§öÁΩëÁªúÂª∂ËøüÔºåÊâÄ‰ª•ËøûÊé•‰∏Ä‰∏™ËøúÁ´Ø‰∏ªÊú∫ÁöÑÊó∂Èó¥Â§ßÊ¶ÇÂ∞±ÊòØÂÖâÊâÄÈúÄÁöÑÊó∂Èó¥„ÄÇ‰∫éÊòØ3ÊØ´Áßí, Â∞±ÊÑèÂë≥ÁùÄÔºö 0.003 * 3 * 10 ^ 8 * 0.001 * 0.621 = 558.9000000000001 558Ëã±Èáå„ÄÇ‰πüÂ∞±ÊòØ558Ëã±Èáå‰ª•Â§ñÁöÑÊúçÂä°Âô®ÔºåÈÉΩÊó†Ê≥ïËøûÊé•Âà∞ÔºåËÄå558Ëã±Èáå‰ª•ÂÜÖÁöÑÊúçÂä°Âô®ÔºåÈÉΩÂèØ‰ª•Ê≠£Â∏∏ÈÄö‰ø°„ÄÇÂΩìÂΩìÂΩìÔºåËøôÂ∞±ÊòØ500Ëã±ÈáåÁöÑbugÂï¶„ÄÇ]]></content>
      <categories>
        <category>ÊùÇËÆ∞</category>
      </categories>
      <tags>
        <tag>ÊùÇËÆ∞</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Novel Anti-Obfuscation Model for Detecting Malicious Code]]></title>
    <url>%2F2018%2F06%2F05%2FA-Novel-Anti-Obfuscation-Model-for-Detecting-Malicious-Code%2F</url>
    <content type="text"><![CDATA[A Novel Anti-Obfuscation Model for Detecting Malicious Code Yuehan Wang Tong Li YongQuan Cai Zhenghu Ning Fei Xue Abstract In this article, the authors present a new malicious code detection model. The detection model improves typical n-gram feature extraction algorithms that are easy to be obfuscated. Specifically, the proposed model can dynamically determine obfuscation features and then adjust the selection of meaningful features to improve corresponding machine learning analysis. The experimental results show that the feature database, which is built based on the proposed feature selection and cleaning method, contains a stable number of features and can automatically get rid of obfuscation features. Overall, the proposed detection model has features of long timeliness, high applicability and high accuracy of identification KeywordÔºö Malicious„ÄÅMachine Learning„ÄÅFeature Extraction„ÄÅObfuscation 1 Introduction The malicious code is a kind of software that is intended to damage or disable computers and computer systems, including computer Trojans, blackmail software, spyware, and so on . According to Symantec (2015), more than 44.5 million new pieces of malware created in May 2015. One of the main reasons for this high volume of malware samples is the extensive use of obfuscation and metamorphic techniques by malware developers . So the most of new malicious code can be divided into several families by the original code . The malicious code detection technologies are usually based on features, which represent the original software code. Thus, same malware familys should have the same features (e.g., Wo≈Çkowicz &amp; KesÃåelj (2013) and Preda &amp; Giacobazzi (2005)). By extracting the family features in each malware family, the defense systems can constructs a feature database for detecting variants. However, the obfuscation techniques can help variants to escape the detection by interfering the feature extraction. For example, in the malicious defense system (Lu, Wang, Zhao, Wang, &amp; Su, 2013) which extracting the key string as a feature. Variants escape the detection by equivalently replacing the key string or adding the invalid string. Many scholars (Shafiq, Tabish, Mirza, &amp; Farooq, 2009; Sung, Xu, Chavez, &amp; Mukkamala, 2004; Gaudesi, Marcelli, Sanchez, Squillero, &amp; Tonda, 2016; Tabish, Shafiq, &amp; Farooq,2009) have proposed various feature extraction methods to defend against this kind of obfuscation technology. But such extraction methods can also be broken by emerging obfuscation technology. On the other hand, more effectively extraction methods will also lead to excessive computing resources, systems real-time poor and so on. Machine learning model (Tahan, Rokach, &amp; Shahar, 2012; Narouei, Ahmadi, Giacinto, Takabi, &amp; Sami, 2015; O.W.D.C., 1992) are used to deal with detection malicious code, which have achieved good results. Through the feature database and labels, the model will train a set of classifiers to identify the variants. However, the accuracy of machine learning model depend on the quality of feature database, so that the feature extraction method will determine the accuracy of model . When the extraction method is broken, the obfuscation technologys (Nataraj, Karthikeyan, Jacob, &amp; Manjunath, 2011; Fredrikson, Jha, Christodorescu, Sailer, &amp; Yan, 2010; Svetnik et al., 2003) will make feature database contains a lot of obfuscation features and the accuracy will be be seriously influenced .For the machine learning model used in detection malicious code, ensuring the effectiveness of feature database is an essential research task . In particular, due to the rapid growth of malicious code, the timeliness of feature extraction method becomes more and more short. In addition, it becomes increasingly difficult to maintain the security of the system by using the replacement feature extraction method. In this article, we propose a method to ensure the effectiveness of feature database which cleans the feature database rather than changing the extraction method.The method was guided by the obfuscation features cleaning and feature selection. The final database will be used in the random forests algorithm.The main contributions of this paper are summarized as follows: 1. An algorithm based on multi-sample analysis is proposed to identity obfuscation features dynamically. This method get through analyzing some numbers of sample data in detail and builds a linear regression algorithm. This linear algorithm is used to compute the thresholds of the obfuscation features dynamically for each sample. 2. A feature selection algorithm is proposed to select family feature. The method first normalizes the eigenvector and identify the family feature according to the number of input data set. 3. Achieving the malicious code detection model. The model use random forest algorithm to reduce the effect of obfuscation technologys furtherly and improves the data utilization.The detection of result by the classifier voted. 2. METHODOLOGY In this paper, the main research content is to clean the feature database based on the machine learning malicious code detection system. The n-gram algorithm is one of the earliest feature extraction algorithms for malware code and the feature has a stronger readable and interpretable that extract by the extraction method. It is impossible to guarantee the feature extraction algorithm never be broken. So this paper choose n-gram feature extraction method to build the feature database. The modern obfuscation technologys make n-gram algorithm is invalid and the feature database is full of obfuscation and noisy feature. So the final feature database was guide by the obfuscation features cleaning method and feature selection method. The two clean method make the feature database has a good anti-interference, and replace the bad features automatically with the training samples increased. As shown in Figure 1. Firstly, this paper construct the linear regression algorithm to identity the obfuscation features and clean dynamically.Secondly, the normalizing method make the eigenvalues range be uniform so that the feature selection method will not be affected by the range of eigenvalues. The feature selection method will guide the train database to select the family features. Finally, this paper choose random forest algorithm to construct the classifier cluster. For the test set samples, the final result is voted on by the cluster. The overall flow chart of the model is shown in Figure 1: 2.1. Initial Feature Database Construction The n-gram algorithm is one of the most primitive malicious code feature extraction methods, which requires less malicious code acquisition and lower computational resources. According to this kind of extraction method, the features can be described from the perspective of the real semantics. This model actually characterizes the importance of each feature by counting the number of occurrences of each feature in a single sample.The obfuscation technology confuse the sequence of operations code and produced a lot of obfuscation features, greatly changing the distribution of the features of the sample. The value of obfuscation features are very large, resulting in the problem that these features have an important impact on the model. For a ‚Äú.asm‚Äù malicious code disassembler, mainly by the paragraph start identifier, memory address, bytecode, opcode and parameters formed. Disassembly program fragment as shown in Figure 2, segment that corresponds to the current instruction belongs to the paragraph, address means memory address, bytes corresponding to the hexadecimal code, opcode means opcode and operands are passed parameters. For a disassembled file, we can extract the corresponding opcode by locating ‚Äú.text‚Äù and obtain the malicious code n-gram features. 2.2. Obfuscation Features Cleaning Method The feature database is full of obfuscation and noise features.If the features database is used for training, the model would be likely to appear a serious over-fitting phenomenon, that the mode cannot detect new variants in the future. Therefore, it is necessary to clean the obfuscation features in the database. Due to the complexity of the samples in the training set, the obfuscation technologs used in each sample is different and the feature distribution is also different. Therefore, for each sample, it is necessary to calculation the value of obfuscation features dynamically. It is more reasonable to find the minimum value and clear others obfuscation features which larger than the value. The thresholds of the obfuscation value (Œæ) are dynamically changed in each sample. In order to measure and characterize the size of the value effectively, we define the following two indicators: the expected value (Feature_averages) and the feature standard (Feature_median). These two indicators are obtained by solving a single sample dynamically, which are used to describe the distribution of features in the sample. Œ± and Œ≤ represent the proportion of the characteristic expected value and the characteristic standard value in the feature obfuscation value respectively. The feature obfuscation threshold is calculated as shown in Equation (1), which reflects the relationship between the threshold and the expected standard values: 1Œæ = Œ± * Feature_averages + Œ≤ * Feature_median For a malicious code sample, the features expected value ( Featureaverages represents the ideal value of the feature in the most primitive case of the sample. However, the n-gram algorithm will extract a lot of noisy features naturally which only appeared small amounts. If all feature are directly manipulated, the expected value will be seriously underestimated. The noisy features will be removed through the feature selection method. In this section, the method is used to remove the larger feature value. Therefore, when calculating of the expected value ( Feature_averages , the first step is to delete the same feature value, and then do the average operation. The expected value of the feature is calculated as shown in Equation (2), ‚Äúm‚Äù is the number of residual features after removing the repetition, and the featurei represents the value of the each features. 1Feature_averages = ‚àëfeature(i)/m The feature expected value can only be used to describe the current sample feature distribution. In order to describe the actual semantics of the sample this paper points out the characteristic standard value ( Featuremedian , which is obtained by calculating the median of all the features in the sample. It can reflect the ideal value of the feature when the sample is undisturbed. The distribution of features in a malicious code sample tends to be a Gaussian distribution, the obscure feature has a very small proportion in its feature distribution. Therefore, the range of the ideal feature can be obtained by solving the median value in the distribution.The characteristic standard value is calculated by the following formula 3, m is the number of residual features after removing the repetition, and the featurei represents the feature of the feature(i). The mid function is the median of the solution sequence. 1Feature(median) = mid(feature(1),feature(2),...,feature(m)) By defining two indicators above, we can describe the obfuscation threshold for each malicious sample dynamically. However, the sample set is always very large. It is a very difficult task to analyze all the malicious code samples manually and find the appropriate weight parameter (Œ±, Œ≤) to describe the obfuscation threshold. Linear regression is a statistical analysis method that uses the regression analysis in mathematical statistics to determine the quantitative relationship between two or more variables. Therefore, this paper chooses linear regression to learn the value of the parameter (Œ±, Œ≤) and calculate the obfuscation threshold in each sample. However, each sample in the set has only the family label and it is useless to identify the obfuscation features. Thus,we should study a small number of samples and make the label of obfuscation features firstly. Then train the linear regression equation based on the labels. The finial equation can be used to predict the obfuscation threshold in unknown samples. The main steps are as follows: Randomly select multiple malicious code sample files. Artificial analysis of sample files to extract the obfuscation threshold. Extracting the feature values and characteristic values of each sample. Selecting 70% of the samples as the training samples of the model, and 30% of the samples as the test samples of the model. Selecting the characteristic prediction value and the characteristic standard value as the model input data and the obfuscation threshold as the prediction data of the model. Inputting Linear Regression Prediction Model Training. The final linear regression equation is obtained to dynamically describe the confounding threshold of the sample. 2.3. Feature Selection Method The n-gram algorithm will extract many noisy features. they will result in some problems, including unclear data features, overheaded model calculation and other issues.Although most of smaller feature values are made up of the noisy data, some of them are important family features in the malicious code. If all the smaller features are cleared, the accuracy of the model will be interfered inevitably. This paper points out a way to quantify the importance of characterization based on the size of the input training data set. If a small feature feature belongs to the noisy data in the overall training set, the feature will only appear in a few samples. And, if it belongs to family features, it will be repeated in the same samples. Therefore, we can select the family features by summing all the sample features. Before constructing the feature selection scheme, this paper refers an operation of feature normalization. Due to the diversity of malicious code samples, the range of eigenvectors in each sample is also different. For the same value of the feature, the importance is different. In order to eliminate the impact of the final measurement of the features which caused by different ranges, this paper presents a standardized operation based on occupancy. For a single sample, the importance of each feature in the sample is measured by calculating the ratio and sum. The characteristic criterion algorithm is as follows: Equation (4), m is the number of all the features in the current sample and feature' represents the new value of the feature after the standardization. 1feature(i) = feature(i)/‚àëfeature(i) (4) From the formula we can see that for the normalized training feature database, the sum of all features of a single sample is 1. Therefore, for the total number of samples S, the sum of all features is S. This paper proposes a feature selection method based on the number of input sample S and the number of malicious code family n in the training set. Since the family features will be repeated in the same family, they will increase the size of the final feature after they have been accumulated. As shown in Equation (5), the value of the Featurei or a feature is the sum of the values of the feature in all sample files. Featurei is the value of the final i-th feature, S is the number of training set samples and Featurei represents the value of the current feature in each sample. 1Feature(i) = ‚àëfeature(i) (5) By studying a small number of samples, we find that for a single sample, the number of features that can usually be extracted is much larger than 100 and floats around 1000. Thus, for a valid feature, the proportion in the sample feature sequence is higher than 0.001. Taking into account that each feature is finally summed by the corresponding features in each sample. So the final selection for the training of the features should meet the following formula 6. Featureselect is the final selection feature for the training signature, S is the total number of training samples and n is the number of malicious code family categories in the sample. 1Feature(select) &gt; S*0.001/n (6) 2.4. Classifier Cluster Construction Due to the complexity of malicious code variants, the final feature database is also very difficult to remove all invalid features completely. Therefore, it is necessary to sample the set and put back. it will improve the generalization and diversity of data sets. The final detection system will be better in anti-interference, robustness. The stochastic forest tree model is an improved model based on the decision tree. The model constructs multiple decision trees by selecting the sample combination feature vector randomly. The malicious database are taken by random sampling with replacement.The different sample set maximizes the utilization rate of the database, which improves the ability of the model for detection future variants of malicious code. The random forest model is shown in Figure 3: When the random forest in training, it will choose different features to build database randomly. In the training feature database that used by the model, some bad features cannot be cleaned up which means there are still some obfuscation features. For the single random node in the model, since the input feature may contain obfuscation features, the final output classifier maybe have some poor bad classifier. However, after the previous obfuscation process, the vast majority of the obfuscation features in the training database has been cleared. What is more, the remaining bad features have only a small proportion in the feature database. By extracting the feature randomly, the effect of the bad features on the final classification can be further relieved. 3. eXPeRIMeNTAL DATA AND RESULTS ANALYSIS 3.1. experimental environment and Test Data The malicious code anti-obfuscation model based on large data is used by the database provided by Microsoft. The set contains a set of known malware files representing a mix of 9 different families. Each malware file has an Id, a 20 character hash value uniquely identifying the file, and a Class, an integer representing one of 9 family names to which the malware may belong:1. Ramnit, 2. Lollipop, 3. Kelihos_ver3, 4. Vundo, 5. Simda, 6. Tracur, 7. Kelihos_ver1, 8. Obfuscator.ACY, 9. Gatak. For each file, the raw data contains the hexadecimal representation of the file‚Äôs binary content, without the PE header (to ensure sterility). The total of number of malicious code samples is 10868. The malicious code sample test set is described below in Figure 4: 3.2. experimental Design This experiment mainly verifies the validity of the two feature processing methods mentioned above. According to the previous papers (Svetnik et al, 2003; Latinne, Debeir, &amp; Decaestecker, 2001), The value of the sliding window n in the n-gram algorithm will be taken as 3 and the number of trees in the random forest algorithm is chosen as 100. Control variables in this experiment include the size of input set, the method of cleaning the obfuscated features, the method of feature selection and the method of standardized processing. The method of cleaning the obfuscated features will effect the selection method.Therefore, the first part of experiment will be carried out without feature obfuscation value cleaning process, testing the different feature selection method on the impact of the model. In this experiment, the sample set size 1000, 2000 is to test the effect of set size and schemes on accuracy when the number of samples is lacking. The 5000 is to verify the upper limit of accuracy on schemes when the number of samples is abundance. This experiment also verifies the validity of the normalizing by the A and B series schemes. The selection criteria are adjusted manually according to the analysis of data set. The specific feature selection scheme, and the corresponding parameter selection are shown in Table 1: The A1 and A2 schemes are used to verify the effectiveness of manual analysis and to compare with the other schemes. The B1 and B2 schemes are used to verify the effectiveness of normalizing. And the C1 schemes is the feature selection method of the paper. In order to evaluate the effect of obfuscation feature cleaning method effectively. This paper select the C1 feature selection method, and tests the effect of different cleaning schemes on the accuracy. Considering the A1 and A2 scheme, select the features that the sum of values larger than 300 or 500. Therefore, in the E1 and E2 scheme, feature values which larger than 300 or 500 are selected for cleaning. The specific obfuscation features cleaning scheme is shown in Table 2: E1 and E2 schemes are mainly used to compare the impact of different cleaning conditions on the final program. F1 and F2 schemes are the same in condition, the value of Œæ is the threshold of obfuscation features. In addition to compare the effects of different condition with E1, E2 programs, the effects of different cleaning methods on the final results were also tested. 3.3. experimental Results and Analysis Firstly, this paper tests the accuracy of each scheme in different data sets to verify the effectiveness of the random forest algorithm. Considering the random forest algorithm constructs the training subset by extracting the method randomly, the accuracy of each model is different. As shown in Figure 5, the relationships between times of tests and accuracy. The model is based on the A1 test scheme in different input data sets and its tested accuracy. The abscissa indicates the number of model constructions and the ordinate indicates the accuracy of the model. From Figure 5 we can figure out that with the growth of the input data set, the accuracy will increase at the same time and the degree of volatility will reduce gradually. When the input data set reaches a certain threshold, it would be hard to improve the accuracy by increasing the data set continually. The reason is that when the data set is lacking, the feature database is also not comprehensive enough. The classifiers could be affected by the bad features, it will lead to lower and unstable on accuracy. When the data set is adequate, the accuracy of a single classifier will be improved and the predictions between the classifiers tend to be consistent. 3.3.1. Comparison of Feature Selection Schemes In the contrast experiment of the feature selection method, the A1, A2, B1, B2 and C1 schemes verify the validity of the feature selection method. The effect of different schemes on the accuracy is described below in Figure 6: As we can see from Figure 6. For all scheme, the accuracy will increase as the input data set increases. And the A2 scheme is able to achieve the best accuracy of 0.976. A1 scheme is the only test scheme in which the model accuracy is reduced as the input sample set grows. The reason for the decrease in accuracy will be analyzed later. For A2,B1,B2C1 scheme, it is difficult to measure the differences because their accuracy is closer. this paper compare the relationship between the accuracy and the number of features in these schemes. As shown in Figure 7 feature selection test program accuracy and characteristic relationship: From the Figure 7 of A1, we can see that when the input sample reaches 5000, the number of features in the model training feature database is more than 8,000. Excessive quantity of features will lead to obscure model features, increased interference of obfuscation features and so on. The parameters used in the B1 and B2 test schemes are different in different input data sets and the number of features is also change greatly. For example, when the number of input samples reaches 1000, the numbers of features are much lower than the other test programs and the accuracy is also weak. While, when the number of input samples reaches 2000, you can get the best detection results. Compared with the results of the B1, B2 test methods, C1 test method has a stronger adaptability, the number of features reach to 1000 stably. In order to verify the feature selection method proposed in this paper and ensure the stability of the final extracted feature number, this paper tests the relationship between the accuracy and the number of features in all test schemes (E1, E2, F1, F2, C1))) by using the feature selection method. At the same time, in order to compare the unused feature selection methods, this article also joined the A1, A2 test program as a comparison. As shown in Figure 8 feature selection method validation: Figure 8 shows that the use of C1 feature selection method of the test program, the final selection of the number of features is more stable. The final accuracy of the model will increase steadily with the number of the input sample set when the variation of the characteristic number is small. Due to the dynamic selection of features, will be based on the number of the input sample set to adjust the selected features. When the input sample set is increased, the feature selection method will also improve the selection of the features of the features to achieve the features of the elimination of poor features. 3.3.2. Obfuscation Value Cleaning Scheme Comparison Although we can guarantee the stability of the number of features used in the model by adjusting the selection method dynamically. However, the method of cleaning the obfuscated features will influence the effect of the accuracy greatly. From Figure 6 can be learned, the accuracy in C1 scheme is lowest, the final detection accuracy has a greater space for improvement. In order to evaluate the impact of the clean obfuscation features method on the model and the differences between different methods. This paper compares the C1, E1, E2, F1, F2, 5 different schemes.In particularly, the C1 scheme has no clean the obfuscation features and the others schemes has different clean method. The model accuracy of each scheme and the sample set relations as shown in Figure 9. Accuracy and input sample set relationship: As shown in Figure 9, the E1, E2, F1, and F2 schemes higher in the final model accuracy than the C1 scheme. The obfuscation feature cleaning method improved accuracy. It is worth mentioning that, for F1, F2 scheme is the same way to find out the obfuscation threshold. But the F1 scheme reduce the obfuscation features value to the obfuscation threshold and the F2 scheme choice reduce to zero. In the F1 scheme the accuracy reducing with the increase in the number of samples. The reason is that F1 scheme changes the feature distribution of the sample by changing the value of the obfuscated features in the sample. Since the value of obfuscated features is still greater than zero after the change and this kind of the feature distribution can be learned by the model. When the input sample size is small, this way will improve the accuracy of the model. However, when the input sample continues to increase, this changing have a certain conflict with the actual feature distribution and the accuracy will be reduced . The F2 scheme clean the obfuscation features from the sample distribution. And this change has not been studied by the model, so there is no human impact the accuracy will be more real. Since the E1, E2 and F2 schemes has adopted the C1 feature selection method, the model detection accuracy and the number of features are closer to each other . The random forest features will select features randomly.Thus,the more obfuscation features in the database, the more unstable the final result. When repeated experiments, the detection accuracy of large fluctuations means the obfuscation features clean schemes is worse. If the number of experimented is small, the results are difficult to observe.In this paper, we choose experimented 100 times for the every test schemes E1, E2 and F2. It is enough to observe the result. The cleaning effect of each test scheme is measured by comparing the degree of volatility of the final model accuracy of each test scheme. The accuracy and frequency of each obfuscation cleaning scheme are shown in Figures 10, 11, 12: In order to measure the detection accuracy of the fluctuations of the test scenarios in Figure 10- 12 in different data sets, the standard deviation is used to measure the volatility of the final model accuracy of each test scheme in this paper. The standard deviation is calculated from the square root of the arithmetic mean of the squared difference squared, reflecting the degree of discretization of a data set. The average number of the same two sets of data, the standard deviation may not be the same. This paper counts the standard deviations of the cleaning schemes in different input samples, as shown in Table 3. 4. RELATED WORK At present, the research of malicious detection technology is mainly focus on the feature extraction. In terms of the features extraction in malicious code (Rieck, Trinius, &amp; Willems, 2011; Li, Santorelli, Laforest, &amp; Coates, 2015; Yong &amp; Zuo, 2007), they have been studied by previous researchers fully. In order to defent against obfuscation technologys, a various of methods of malicious code feature extraction have been proposed by researchers. These methods from the security attributes, dependencies, real semantics and so on. The modern extraction method focuses on the the actual semantics of malicious code. This kind of semantic-based method can express the actual behavior of malicious code. It has strong interpretability, easy to maintain the actual analysis of personnel and the development of the detection strategy. The related work in this paper will be introduced from the malicious code detection technology and feature extraction methods these two aspects. 4.1. Detection Technology In general, malicious code detection techniques can be divided into two types of technology (Shahzad &amp; Lavesson, 2013), including the detection method based on heuristic and the detection method based on signature. Early detection methods are mostly based on heuristic, the requirements of this detection method about the researchers‚Äô experience and judgment are very high. For example, the Rootkit Revealer (Willems, Holz, &amp; Freiling, 2007) detection system identifies hidden processes, files and associated registry information by comparing system upper-level information and file system status from the kernel. The detection effect of this type of detection system depends on the degree of research on the system, it does not have generalization and versatility. Therefore, most of the detection systems is based on feature database. Machine learning model can predict variants well based on database, the modern detection systems used machine learning to train classifiers .The detection technique could be divided into three steps (Yin, Song, Egele, Kruegel, &amp; Kirda, 2007; Narudin, Feizollah, Anuar, &amp; Gani, 2014), the first step is the extraction of the features of malicious code. Second, removing obfuscation features, fusion and building low-dimensional features database. Finally, the machine learning algorithms use features database to train the classifier and identity the classification of malicious code. The malicious code detection system which based on signature (Abawajy, Kelarev, &amp; Chowdhury, 2014) constructs the feature database of a kind of malicious code by extracting the morphological features of executable binary file. After the same feature has been extracted from the malicious code to be detected, it is detected by pattern matching with the feature database. However, this method could only match a single malicious code. With the increase number of the sample, the data volume of the feature set is often very large, which is not conducive to the automatic detection of malicious code in the future. With the progress of dis-assembly technology, hidden sensitive vocabulary, special methods, resource calls could be found through the dis-assembly technology on the malicious code dynamic analysis in the malicious code samples. Therefore, researchers have proposed the detection method that based on these behavioral features. Behavior-based detection methods pay more attention to the actual behavior of malicious code and have a better ability to defence obfuscation operation. 4.2. Malicious Code Feature extraction The use of a single feature extraction method does not have good anti-jamming and stability. When the extraction method is compromised, the monitoring system will break generally. In this case, some scholars (Kirda, Kruegel, Banks, Vigna, &amp; Kemmerer, 2006) proposed for the malicious code for multi-feature extraction and the extraction of the features of the fusion. So that it could get more robust anti-interference malicious code family core features. This multi-feature extraction and fusion method enhances the anti-jamming of the features. The malicious code can‚Äôt fully affect the fusion features. So it is difficult to influence the final result. Kirda and others (Rui, Feng, Yi, &amp; Pu-Rui, 2012) use spy code to obtain user-sensitive data and then leak the behavior of the data features of the test. But this method is limited to the detection of spy malicious code, other data does not cause the disclosure of malicious code could not be detected. From the perspective of the actual semantic code of malicious code, Rui et al. (2012) and others use feature map to calculate malicious code by constructing a behavioral feature map based on malicious code semantics. It has achieved very good detection results. However, this method is based on the detection method of the program itself merely. It cannot recognize some special variants of malicious code without taking into account the program for the resource call problem. Mao et al. (2017) and others (Zhang, Ren, Jiang, &amp; Zhang, 2015) proposed an active learning method that solves the problem of malicious code detection when the labeled samples are less marked. The feature extraction method mainly constructs the system data flow dependency graph. The graph according to the resource scheduling relation of the sample from the angle of the malicious code resource call. The method is to use a large number of normal software behavior data and a small amount of tagged malicious code. The normal software resources would be distinguished between differentiated malicious software through the active learning way to update the malicious code classifier. In a small amount of samples of the case, the timely identification of new malicious code. This kind of method mainly describes the features from the aspects of security attribute and statistical feature. However, the expression form of this feature is not comprehensive for the future analysis of malicious code. Nataraj and Karthikeyan (2011) first proposed the concept of malicious images. Malicious images from the malicious code executable binary file and the binary file mapping to generate grayscale malicious images. The method uses the same family of malicious code programs that will use some family of historical resource files, resulting in malicious images that are similar. The method requires a small computational cost and a strong anti - jamming capability. However, due to the increasing complexity of malicious code, for malicious code image extraction detection methods, you can also call the obfuscation of resources, etc. to bypass or interfere with the detection of features. Han, Qu, Yao, Guo, and Zhou (2014) and Han, Yao, Wu, and Guo (2014) combined with image analysis technology to extract the fingerprint feature of the image, and use the gray-level co-occurrence matrix algorithm to extract the malicious code texture fingerprint. 5. SUMMARY Due to the feature extraction method was broken, the feature database full of obfuscation features and the malware variants escape detection. An new type of malicious code detection model is proposed in this paper. Our feature database is built on n-gram feature extraction method. At the same time, obfuscation features cleaning and feature selection method will eliminate the effects of obfuscation techniques on database. In particular, our method can dynamically determine obfuscation features and adjust the selection of family features. In such a way, the method can ensuring the effectiveness of feature database and improve the accuracy of detect variants. When the value of obfuscation features is close to the value of family features, the cleaning method can not remove this kind of obfuscation features effectively. Thus, most of obfuscation features have been cleared by this cleaning method. Moreover, the random forest algorithm will further reduce the influence of remaining obfuscation features. In order to select the features more rationally, this paper proposes a dynamic selection method based on sample set. This method can stabilize the number of features in the database, the features will also change when the number of input samples increases. Although the new malicious code detection model proposed in this paper can eliminate most of the obfuscation features, there are still some shortcomings. For example, the speed of operation is too low, the dependency of a large number of mark data is quite strong and so on. On the other hand, the database is very large, it is difficult for model to learn and detect new malware families that do not have a lot of tagged detal. Therefore, how to deal with fewer malicious code families will be launched in the follow-up work. The spark platform has the ability to parallelize the process of data. It is necessary to combine the test model and spark platform together. Considering that our method takes more time for a single sample process, it is need to separate the model detection and the sample feature extraction separately. At the same time, the high real-time requirements of online detection and the need to simultaneously handle multiple sample files.Therefore, the future model combined with the spark platform for the sample set to calculate and feature extraction, and return the eigenvector. Then, the feature vector is calculated by the classifier to improve the response speed of the model. We will also test the performance of the model in more malware sets.]]></content>
      <categories>
        <category>‰∏™‰∫∫ÊàêÊûú</category>
      </categories>
      <tags>
        <tag>ËÆ∫Êñá</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ê∑±ÂÖ•ÁêÜËß£ xgboost]]></title>
    <url>%2F2018%2F06%2F05%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-xgboost%2F</url>
    <content type="text"><![CDATA[Ê∑±ÂÖ•ÁêÜËß£ xgboost ÂâçË®Ä 1ÔºâBoosted TreeÁöÑËã•Âπ≤Âêå‰πâËØç ÂèØËÉΩÊúâ‰∫∫‰ºöÈóÆÔºå‰∏∫‰ªÄ‰πàÊàëÊ≤°ÊúâÂê¨ËøáËøô‰∏™ÂêçÂ≠ó„ÄÇËøôÊòØÂõ†‰∏∫Boosted TreeÊúâÂêÑÁßçÈ©¨Áî≤ÔºåÊØîÂ¶ÇGBDT, GBRT (gradient boosted regression tree)ÔºåLambdaMART‰πüÊòØ‰∏ÄÁßçboosted treeÁöÑÂèòÁßç„ÄÇÁΩë‰∏äÊúâÂæàÂ§ö‰ªãÁªçBoosted treeÁöÑËµÑÊñôÔºå‰∏çËøáÂ§ßÈÉ®ÂàÜÈÉΩÊòØÂü∫‰∫éFriedmanÁöÑÊúÄÊó©‰∏ÄÁØáÊñáÁ´†Greedy Function Approximation: A Gradient Boosting MachineÁöÑÁøªËØë„ÄÇ‰∏™‰∫∫ËßâÂæóËøô‰∏çÊòØÊúÄÂ•ΩÊúÄ‰∏ÄËà¨Âú∞‰ªãÁªçboosted treeÁöÑÊñπÂºè„ÄÇËÄåÁΩë‰∏äÈô§‰∫ÜËøô‰∏™ËßíÂ∫¶‰πãÂ§ñÁöÑ‰ªãÁªçÂπ∂‰∏çÂ§ö„ÄÇ 2ÔºâÊúâÁõëÁù£Â≠¶‰π†ÁÆóÊ≥ïÁöÑÈÄªËæëÁªÑÊàê Ë¶ÅËÆ≤boosted treeÔºåË¶ÅÂÖà‰ªéÊúâÁõëÁù£Â≠¶‰π†ËÆ≤Ëµ∑„ÄÇÂú®ÊúâÁõëÁù£Â≠¶‰π†ÈáåÈù¢ÊúâÂá†‰∏™ÈÄªËæë‰∏äÁöÑÈáçË¶ÅÁªÑÊàêÈÉ®‰ª∂33ÔºåÂàùÁï•Âú∞ÂàÜÂèØ‰ª•ÂàÜ‰∏∫ÔºöÊ®°ÂûãÔºåÂèÇÊï∞ Âíå ÁõÆÊ†áÂáΩÊï∞„ÄÇ i. Ê®°ÂûãÂíåÂèÇÊï∞ Ê®°ÂûãÊåáÁªôÂÆöËæìÂÖ•xixiÂ¶Ç‰ΩïÂéªÈ¢ÑÊµã ËæìÂá∫ yiyi„ÄÇÊàë‰ª¨ÊØîËæÉÂ∏∏ËßÅÁöÑÊ®°ÂûãÂ¶ÇÁ∫øÊÄßÊ®°ÂûãÔºàÂåÖÊã¨Á∫øÊÄßÂõûÂΩíÂíålogistic regressionÔºâÈááÁî®‰∫ÜÁ∫øÊÄßÂè†Âä†ÁöÑÊñπÂºèËøõË°åÈ¢ÑÊµãyÃÇ i=‚àëjwjxijy^i=‚àëjwjxij „ÄÇÂÖ∂ÂÆûËøôÈáåÁöÑÈ¢ÑÊµãyyÂèØ‰ª•Êúâ‰∏çÂêåÁöÑËß£ÈáäÔºåÊØîÂ¶ÇÊàë‰ª¨ÂèØ‰ª•Áî®ÂÆÉÊù•‰Ωú‰∏∫ÂõûÂΩíÁõÆÊ†áÁöÑËæìÂá∫ÔºåÊàñËÄÖËøõË°åsigmoid ÂèòÊç¢ÂæóÂà∞Ê¶ÇÁéáÔºåÊàñËÄÖ‰Ωú‰∏∫ÊéíÂ∫èÁöÑÊåáÊ†áÁ≠â„ÄÇËÄå‰∏Ä‰∏™Á∫øÊÄßÊ®°ÂûãÊ†πÊçÆyyÁöÑËß£Èáä‰∏çÂêåÔºà‰ª•ÂèäËÆæËÆ°ÂØπÂ∫îÁöÑÁõÆÊ†áÂáΩÊï∞ÔºâÁî®Âà∞ÂõûÂΩíÔºåÂàÜÁ±ªÊàñÊéíÂ∫èÁ≠âÂú∫ÊôØ„ÄÇÂèÇÊï∞ÊåáÊàë‰ª¨ÈúÄË¶ÅÂ≠¶‰π†ÁöÑ‰∏úË•øÔºåÂú®Á∫øÊÄßÊ®°Âûã‰∏≠ÔºåÂèÇÊï∞ÊåáÊàë‰ª¨ÁöÑÁ∫øÊÄßÁ≥ªÊï∞ww„ÄÇ ii. ÁõÆÊ†áÂáΩÊï∞ÔºöÊçüÂ§± + Ê≠£Âàô Ê®°ÂûãÂíåÂèÇÊï∞Êú¨Ë∫´ÊåáÂÆö‰∫ÜÁªôÂÆöËæìÂÖ•Êàë‰ª¨Â¶Ç‰ΩïÂÅöÈ¢ÑÊµãÔºå‰ΩÜÊòØÊ≤°ÊúâÂëäËØâÊàë‰ª¨Â¶Ç‰ΩïÂéªÂØªÊâæ‰∏Ä‰∏™ÊØîËæÉÂ•ΩÁöÑÂèÇÊï∞ÔºåËøô‰∏™Êó∂ÂÄôÂ∞±ÈúÄË¶ÅÁõÆÊ†áÂáΩÊï∞ÁôªÂú∫‰∫Ü„ÄÇ‰∏ÄËà¨ÁöÑÁõÆÊ†áÂáΩÊï∞ÂåÖÂê´‰∏ãÈù¢‰∏§È°πÔºö 1Obj(√∏) = L(√∏) + Œ©(√∏) ÂÖ∂‰∏≠ L(√∏) Áî®Êù•ÊåáÂØºÊ®°ÂûãÂèÇÊï∞Ë∞ÉÊï¥ÁöÑÊñπÂêë„ÄÇŒ©(√∏) ‰∏∫Ê≠£ÂàôÈ°πÔºåÁî®Êù•Èò≤Ê≠¢ËøáÊãüÂêàÁé∞Ë±°„ÄÇ Â∏∏ËßÅÁöÑËØØÂ∑ÆÂáΩÊï∞ÊØîÂ¶ÇÂπ≥ÊñπËØØÂ∑Æ„ÄÅlogisticËØØÂ∑ÆÂáΩÊï∞Á≠â„ÄÇËÄåÂØπ‰∫éÁ∫øÊÄßÊ®°ÂûãÂ∏∏ËßÅÁöÑÊ≠£ÂàôÂåñÈ°πÊúâL2Ê≠£ÂàôÂíåL1Ê≠£Âàô„ÄÇËøôÊ†∑ÁõÆÊ†áÂáΩÊï∞ÁöÑËÆæËÆ°Êù•Ëá™‰∫éÁªüËÆ°Â≠¶‰π†ÈáåÈù¢ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÊ¶ÇÂøµÂè´ÂÅöBias-variance tradeoff44„ÄÇÊØîËæÉÊÑüÊÄßÁöÑÁêÜËß£ÔºåBiasÂèØ‰ª•ÁêÜËß£‰∏∫ÂÅáËÆæÊàë‰ª¨ÊúâÊó†ÈôêÂ§öÊï∞ÊçÆÁöÑÊó∂ÂÄôÔºåÂèØ‰ª•ËÆ≠ÁªÉÂá∫ÊúÄÂ•ΩÁöÑÊ®°ÂûãÊâÄÊãøÂà∞ÁöÑËØØÂ∑Æ„ÄÇËÄåVarianceÊòØÂõ†‰∏∫Êàë‰ª¨Âè™ÊúâÊúâÈôêÊï∞ÊçÆÔºåÂÖ∂‰∏≠ÈöèÊú∫ÊÄßÂ∏¶Êù•ÁöÑËØØÂ∑Æ„ÄÇÁõÆÊ†á‰∏≠ËØØÂ∑ÆÂáΩÊï∞ÈºìÂä±Êàë‰ª¨ÁöÑÊ®°ÂûãÂ∞ΩÈáèÂéªÊãüÂêàËÆ≠ÁªÉÊï∞ÊçÆÔºåËøôÊ†∑Áõ∏ÂØπÊù•ËØ¥ÊúÄÂêéÁöÑÊ®°Âûã‰ºöÊúâÊØîËæÉÂ∞ëÁöÑ bias„ÄÇËÄåÊ≠£ÂàôÂåñÈ°πÂàôÈºìÂä±Êõ¥Âä†ÁÆÄÂçïÁöÑÊ®°Âûã„ÄÇÂõ†‰∏∫ÂΩìÊ®°ÂûãÁÆÄÂçï‰πãÂêéÔºåÊúâÈôêÊï∞ÊçÆÊãüÂêàÂá∫Êù•ÁªìÊûúÁöÑÈöèÊú∫ÊÄßÊØîËæÉÂ∞èÔºå‰∏çÂÆπÊòìËøáÊãüÂêàÔºå‰ΩøÂæóÊúÄÂêéÊ®°ÂûãÁöÑÈ¢ÑÊµãÊõ¥Âä†Á®≥ÂÆö„ÄÇ iii. ‰ºòÂåñÁÆóÊ≥ï ËÆ≤‰∫ÜËøô‰πàÂ§öÊúâÁõëÁù£Â≠¶‰π†ÁöÑÂü∫Êú¨Ê¶ÇÂøµÔºå‰∏∫‰ªÄ‰πàË¶ÅËÆ≤Ëøô‰∫õÂë¢Ôºü ÊòØÂõ†‰∏∫ËøôÂá†ÈÉ®ÂàÜÂåÖÂê´‰∫ÜÊú∫Âô®Â≠¶‰π†ÁöÑ‰∏ªË¶ÅÊàêÂàÜÔºå‰πüÊòØÊú∫Âô®Â≠¶‰π†Â∑•ÂÖ∑ËÆæËÆ°‰∏≠ÂàíÂàÜÊ®°ÂùóÊØîËæÉÊúâÊïàÁöÑÂäûÊ≥ï„ÄÇÂÖ∂ÂÆûËøôÂá†ÈÉ®ÂàÜ‰πãÂ§ñÔºåËøòÊúâ‰∏Ä‰∏™‰ºòÂåñÁÆóÊ≥ïÔºåÂ∞±ÊòØÁªôÂÆöÁõÆÊ†áÂáΩÊï∞‰πãÂêéÊÄé‰πàÂ≠¶ÁöÑÈóÆÈ¢ò„ÄÇ‰πãÊâÄ‰ª•ÊàëÊ≤°ÊúâËÆ≤‰ºòÂåñÁÆóÊ≥ïÔºåÊòØÂõ†‰∏∫ËøôÊòØÂ§ßÂÆ∂ÂæÄÂæÄÊØîËæÉÁÜüÊÇâÁöÑ‚ÄúÊú∫Âô®Â≠¶‰π†ÁöÑÈÉ®ÂàÜ‚Äù„ÄÇËÄåÊúâÊó∂ÂÄôÊàë‰ª¨ÂæÄÂæÄÂè™Áü•ÈÅì‚Äú‰ºòÂåñÁÆóÊ≥ï‚ÄùÔºåËÄåÊ≤°Êúâ‰ªîÁªÜËÄÉËôëÁõÆÊ†áÂáΩÊï∞ÁöÑËÆæËÆ°ÁöÑÈóÆÈ¢òÔºåÊØîËæÉÂ∏∏ËßÅÁöÑ‰æãÂ≠êÂ¶ÇÂÜ≥Á≠ñÊ†ëÁöÑÂ≠¶‰π†ÔºåÂ§ßÂÆ∂Áü•ÈÅìÁöÑÁÆóÊ≥ïÊòØÊØè‰∏ÄÊ≠•Âéª‰ºòÂåñgini entropyÔºåÁÑ∂ÂêéÂâ™ÊûùÔºå‰ΩÜÊòØÊ≤°ÊúâËÄÉËôëÂà∞ÂêéÈù¢ÁöÑÁõÆÊ†áÊòØ‰ªÄ‰πà„ÄÇ Boosted Tree ËØùÈ¢òÂõûÂà∞boosted treeÔºåÊàë‰ª¨‰πüÊòØ‰ªéËøôÂá†‰∏™ÊñπÈù¢ÂºÄÂßãËÆ≤ÔºåÈ¶ñÂÖàËÆ≤Ê®°Âûã„ÄÇBoosted tree ÊúÄÂü∫Êú¨ÁöÑÁªÑÊàêÈÉ®ÂàÜÂè´ÂÅöÂõûÂΩíÊ†ë(regression tree)Ôºå‰πüÂè´ÂÅöCART5„ÄÇ ‰∏äÈù¢Â∞±ÊòØ‰∏Ä‰∏™CARTÁöÑ‰æãÂ≠ê„ÄÇCART‰ºöÊääËæìÂÖ•Ê†πÊçÆËæìÂÖ•ÁöÑÂ±ûÊÄßÂàÜÈÖçÂà∞ÂêÑ‰∏™Âè∂Â≠êËäÇÁÇπÔºåËÄåÊØè‰∏™Âè∂Â≠êËäÇÁÇπ‰∏äÈù¢ÈÉΩ‰ºöÂØπÂ∫î‰∏Ä‰∏™ÂÆûÊï∞ÂàÜÊï∞„ÄÇ‰∏äÈù¢ÁöÑ‰æãÂ≠êÊòØ‰∏Ä‰∏™È¢ÑÊµã‰∏Ä‰∏™‰∫∫ÊòØÂê¶‰ºöÂñúÊ¨¢ÁîµËÑëÊ∏∏ÊàèÁöÑ CARTÔºå‰Ω†ÂèØ‰ª•ÊääÂè∂Â≠êÁöÑÂàÜÊï∞ÁêÜËß£‰∏∫ÊúâÂ§öÂèØËÉΩËøô‰∏™‰∫∫ÂñúÊ¨¢ÁîµËÑëÊ∏∏Êàè„ÄÇÊúâ‰∫∫ÂèØËÉΩ‰ºöÈóÆÂÆÉÂíådecision treeÁöÑÂÖ≥Á≥ªÔºåÂÖ∂ÂÆûÊàë‰ª¨ÂèØ‰ª•ÁÆÄÂçïÂú∞ÊääÂÆÉÁêÜËß£‰∏∫decision treeÁöÑ‰∏Ä‰∏™Êâ©Â±ï„ÄÇ‰ªéÁÆÄÂçïÁöÑÁ±ªÊ†áÂà∞ÂàÜÊï∞‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ÂÅöÂæàÂ§ö‰∫ãÊÉÖÔºåÂ¶ÇÊ¶ÇÁéáÈ¢ÑÊµãÔºåÊéíÂ∫è„ÄÇ ‰∏Ä‰∏™CARTÂæÄÂæÄËøá‰∫éÁÆÄÂçïÊó†Ê≥ïÊúâÊïàÂú∞È¢ÑÊµãÔºåÂõ†Ê≠§‰∏Ä‰∏™Êõ¥Âä†Âº∫ÂäõÁöÑÊ®°ÂûãÂè´ÂÅötree ensemble„ÄÇÂú®‰∏äÈù¢ÁöÑ‰æãÂ≠ê‰∏≠ÔºåÊàë‰ª¨Áî®‰∏§Ê£µÊ†ëÊù•ËøõË°åÈ¢ÑÊµã„ÄÇÊàë‰ª¨ÂØπ‰∫éÊØè‰∏™Ê†∑Êú¨ÁöÑÈ¢ÑÊµãÁªìÊûúÂ∞±ÊòØÊØèÊ£µÊ†ëÈ¢ÑÊµãÂàÜÊï∞ÁöÑÂíå„ÄÇÂà∞ËøôÈáåÔºåÊàë‰ª¨ÁöÑÊ®°ÂûãÂ∞±‰ªãÁªçÂÆåÊØï‰∫Ü„ÄÇÁé∞Âú®ÈóÆÈ¢òÊù•‰∫ÜÔºåÊàë‰ª¨Â∏∏ËßÅÁöÑÈöèÊú∫Ê£ÆÊûóÂíåboosted treeÂíåtree ensembleÊúâ‰ªÄ‰πàÂÖ≥Á≥ªÂë¢ÔºüÂ¶ÇÊûú‰Ω†‰ªîÁªÜÁöÑÊÄùËÄÉÔºå‰Ω†‰ºöÂèëÁé∞RFÂíåboosted treeÁöÑÊ®°ÂûãÈÉΩÊòØtree ensembleÔºåÂè™ÊòØÊûÑÈÄ†ÔºàÂ≠¶‰π†ÔºâÊ®°ÂûãÂèÇÊï∞ÁöÑÊñπÊ≥ï‰∏çÂêå„ÄÇÁ¨¨‰∫å‰∏™ÈóÆÈ¢òÔºöÂú®Ëøô‰∏™Ê®°Âûã‰∏≠ÁöÑ‚ÄúÂèÇÊï∞‚ÄùÊòØ‰ªÄ‰πà„ÄÇÂú®tree ensemble‰∏≠ÔºåÂèÇÊï∞ÂØπÂ∫î‰∫ÜÊ†ëÁöÑÁªìÊûÑÔºå‰ª•ÂèäÊØè‰∏™Âè∂Â≠êËäÇÁÇπ‰∏äÈù¢ÁöÑÈ¢ÑÊµãÂàÜÊï∞„ÄÇ ÊúÄÂêé‰∏Ä‰∏™ÈóÆÈ¢òÂΩìÁÑ∂ÊòØÂ¶Ç‰ΩïÂ≠¶‰π†Ëøô‰∫õÂèÇÊï∞„ÄÇÂú®Ëøô‰∏ÄÈÉ®ÂàÜÔºåÁ≠îÊ°àÂèØËÉΩÂçÉÂ•áÁôæÊÄ™Ôºå‰ΩÜÊòØÊúÄÊ†áÂáÜÁöÑÁ≠îÊ°àÂßãÁªàÊòØ‰∏Ä‰∏™ÔºöÂÆö‰πâÂêàÁêÜÁöÑÁõÆÊ†áÂáΩÊï∞ÔºåÁÑ∂ÂêéÂéªÂ∞ùËØï‰ºòÂåñËøô‰∏™ÁõÆÊ†áÂáΩÊï∞„ÄÇÂú®ËøôÈáåÊàëË¶ÅÂ§öËØ¥‰∏ÄÂè•ÔºåÂõ†‰∏∫ÂÜ≥Á≠ñÊ†ëÂ≠¶‰π†ÂæÄÂæÄÂÖÖÊª°‰∫Üheuristic„ÄÇ Â¶ÇÂÖà‰ºòÂåñÂêâÂ∞ºÁ≥ªÊï∞ÔºåÁÑ∂ÂêéÂÜçÂâ™ÊûùÂï¶ÔºåÈôêÂà∂ÊúÄÂ§ßÊ∑±Â∫¶ÔºåÁ≠âÁ≠â„ÄÇÂÖ∂ÂÆûËøô‰∫õheuristicÁöÑËÉåÂêéÂæÄÂæÄÈöêÂê´‰∫Ü‰∏Ä‰∏™ÁõÆÊ†áÂáΩÊï∞ÔºåËÄåÁêÜËß£ÁõÆÊ†áÂáΩÊï∞Êú¨Ë∫´‰πüÊúâÂà©‰∫éÊàë‰ª¨ËÆæËÆ°Â≠¶‰π†ÁÆóÊ≥ïÔºåËøô‰∏™‰ºöÂú®ÂêéÈù¢ÂÖ∑‰ΩìÂ±ïÂºÄ„ÄÇ ÂØπ‰∫étree ensembleÔºåÊàë‰ª¨ÂèØ‰ª•ÊØîËæÉ‰∏•Ê†ºÁöÑÊääÊàë‰ª¨ÁöÑÊ®°ÂûãÂÜôÊàêÊòØÔºö ÂÖ∂‰∏≠ÊØè‰∏™fÊòØ‰∏Ä‰∏™Âú®ÂáΩÊï∞Á©∫Èó¥6(F)ÈáåÈù¢ÁöÑÂáΩÊï∞ÔºåËÄåFÂØπÂ∫î‰∫ÜÊâÄÊúâregression treeÁöÑÈõÜÂêà„ÄÇÊàë‰ª¨ËÆæËÆ°ÁöÑÁõÆÊ†áÂáΩÊï∞‰πüÈúÄË¶ÅÈÅµÂæ™ÂâçÈù¢ÁöÑ‰∏ªË¶ÅÂéüÂàôÔºåÂåÖÂê´‰∏§ÈÉ®ÂàÜ iii. Ê®°ÂûãÂ≠¶‰π†Ôºöadditive training ÂÖ∂‰∏≠Á¨¨‰∏ÄÈÉ®ÂàÜÊòØËÆ≠ÁªÉËØØÂ∑ÆÔºå‰πüÂ∞±ÊòØÂ§ßÂÆ∂Áõ∏ÂØπÊØîËæÉÁÜüÊÇâÁöÑÂ¶ÇÂπ≥ÊñπËØØÂ∑Æ, logistic lossÁ≠â„ÄÇËÄåÁ¨¨‰∫åÈÉ®ÂàÜÊòØÊØèÊ£µÊ†ëÁöÑÂ§çÊùÇÂ∫¶ÁöÑÂíå„ÄÇËøô‰∏™Âú®ÂêéÈù¢‰ºöÁªßÁª≠ËÆ≤Âà∞„ÄÇÂõ†‰∏∫Áé∞Âú®Êàë‰ª¨ÁöÑÂèÇÊï∞ÂèØ‰ª•ËÆ§‰∏∫ÊòØÂú®‰∏Ä‰∏™ÂáΩÊï∞Á©∫Èó¥ÈáåÈù¢ÔºåÊàë‰ª¨‰∏çËÉΩÈááÁî®‰º†ÁªüÁöÑÂ¶ÇSGD‰πãÁ±ªÁöÑÁÆóÊ≥ïÊù•Â≠¶‰π†Êàë‰ª¨ÁöÑÊ®°ÂûãÔºåÂõ†Ê≠§Êàë‰ª¨‰ºöÈááÁî®‰∏ÄÁßçÂè´ÂÅöadditive trainingÁöÑÊñπÂºèÔºàÂè¶Â§ñÔºåÂú®Êàë‰∏™‰∫∫ÁöÑÁêÜËß£ÈáåÈù¢7ÔºåboostingÂ∞±ÊòØÊåáadditive trainingÁöÑÊÑèÊÄùÔºâ„ÄÇÊØè‰∏ÄÊ¨°‰øùÁïôÂéüÊù•ÁöÑÊ®°Âûã‰∏çÂèòÔºåÂä†ÂÖ•‰∏Ä‰∏™Êñ∞ÁöÑÂáΩÊï∞$f$Âà∞Êàë‰ª¨ÁöÑÊ®°Âûã‰∏≠„ÄÇ Áé∞Âú®ËøòÂâ©‰∏ã‰∏Ä‰∏™ÈóÆÈ¢òÔºåÊàë‰ª¨Â¶Ç‰ΩïÈÄâÊã©ÊØè‰∏ÄËΩÆÂä†ÂÖ•‰ªÄ‰πàfÂë¢ÔºüÁ≠îÊ°àÊòØÈùûÂ∏∏Áõ¥Êé•ÁöÑÔºåÈÄâÂèñ‰∏Ä‰∏™fÊù•‰ΩøÂæóÊàë‰ª¨ÁöÑÁõÆÊ†áÂáΩÊï∞Â∞ΩÈáèÊúÄÂ§ßÂú∞Èôç‰Ωé„ÄÇ Ëøô‰∏™ÂÖ¨ÂºèÂèØËÉΩÊúâ‰∫õËøá‰∫éÊäΩË±°ÔºåÊàë‰ª¨ÂèØ‰ª•ËÄÉËôëÂΩìlÊòØÂπ≥ÊñπËØØÂ∑ÆÁöÑÊÉÖÂÜµ„ÄÇËøô‰∏™Êó∂ÂÄôÊàë‰ª¨ÁöÑÁõÆÊ†áÂèØ‰ª•Ë¢´ÂÜôÊàê‰∏ãÈù¢ËøôÊ†∑ÁöÑ‰∫åÊ¨°ÂáΩÊï∞Ôºö Êõ¥Âä†‰∏ÄËà¨ÁöÑÔºåÂØπ‰∫é‰∏çÊòØÂπ≥ÊñπËØØÂ∑ÆÁöÑÊÉÖÂÜµÔºåÊàë‰ª¨‰ºöÈááÁî®Â¶Ç‰∏ãÁöÑÊ≥∞ÂãíÂ±ïÂºÄËøë‰ººÊù•ÂÆö‰πâ‰∏Ä‰∏™Ëøë‰ººÁöÑÁõÆÊ†áÂáΩÊï∞ÔºåÊñπ‰æøÊàë‰ª¨ËøõË°åËøô‰∏ÄÊ≠•ÁöÑËÆ°ÁÆó„ÄÇ ÂΩìÊàë‰ª¨ÊääÂ∏∏Êï∞È°πÁßªÈô§‰πãÂêéÔºåÊàë‰ª¨‰ºöÂèëÁé∞Â¶Ç‰∏ã‰∏Ä‰∏™ÊØîËæÉÁªü‰∏ÄÁöÑÁõÆÊ†áÂáΩÊï∞„ÄÇËøô‰∏Ä‰∏™ÁõÆÊ†áÂáΩÊï∞Êúâ‰∏Ä‰∏™ÈùûÂ∏∏ÊòéÊòæÁöÑÁâπÁÇπÔºåÂÆÉÂè™‰æùËµñ‰∫éÊØè‰∏™Êï∞ÊçÆÁÇπÁöÑÂú®ËØØÂ∑ÆÂáΩÊï∞‰∏äÁöÑ‰∏ÄÈò∂ÂØºÊï∞Âíå‰∫åÈò∂ÂØºÊï∞„ÄÇÊúâ‰∫∫ÂèØËÉΩ‰ºöÈóÆÔºåËøô‰∏™ÊùêÊñô‰ºº‰πéÊØîÊàë‰ª¨‰πãÂâçÂ≠¶ËøáÁöÑÂÜ≥Á≠ñÊ†ëÂ≠¶‰π†ÈöæÊáÇ„ÄÇ‰∏∫‰ªÄ‰πàË¶ÅËä±Ëøô‰πàÂ§öÂäõÊ∞îÊù•ÂÅöÊé®ÂØºÂë¢Ôºü Âõ†‰∏∫ËøôÊ†∑ÂÅö‰ΩøÂæóÊàë‰ª¨ÂèØ‰ª•ÂæàÊ∏ÖÊ•öÂú∞ÁêÜËß£Êï¥‰∏™ÁõÆÊ†áÊòØ‰ªÄ‰πàÔºåÂπ∂‰∏î‰∏ÄÊ≠•‰∏ÄÊ≠•Êé®ÂØºÂá∫Â¶Ç‰ΩïËøõË°åÊ†ëÁöÑÂ≠¶‰π†„ÄÇËøô‰∏Ä‰∏™ÊäΩË±°ÁöÑÂΩ¢ÂºèÂØπ‰∫éÂÆûÁé∞Êú∫Âô®Â≠¶‰π†Â∑•ÂÖ∑‰πüÊòØÈùûÂ∏∏ÊúâÂ∏ÆÂä©ÁöÑ„ÄÇ‰º†ÁªüÁöÑGBDTÂèØËÉΩÂ§ßÂÆ∂ÂèØ‰ª•ÁêÜËß£Â¶Ç‰ºòÂåñÂπ≥Ê≥ïaÊÆãÂ∑ÆÔºå‰ΩÜÊòØËøôÊ†∑‰∏Ä‰∏™ÂΩ¢ÂºèÂåÖÂê´ÂèØÊâÄÊúâÂèØ‰ª•Ê±ÇÂØºÁöÑÁõÆÊ†áÂáΩÊï∞„ÄÇ‰πüÂ∞±ÊòØËØ¥Êúâ‰∫ÜËøô‰∏™ÂΩ¢ÂºèÔºåÊàë‰ª¨ÂÜôÂá∫Êù•ÁöÑ‰ª£Á†ÅÂèØ‰ª•Áî®Êù•Ê±ÇËß£ÂåÖÊã¨ÂõûÂΩíÔºåÂàÜÁ±ªÂíåÊéíÂ∫èÁöÑÂêÑÁßçÈóÆÈ¢òÔºåÊ≠£ÂºèÁöÑÊé®ÂØºÂèØ‰ª•‰ΩøÂæóÊú∫Âô®Â≠¶‰π†ÁöÑÂ∑•ÂÖ∑Êõ¥Âä†‰∏ÄËà¨„ÄÇ iv. Ê†ëÁöÑÂ§çÊùÇÂ∫¶ Âà∞ÁõÆÂâç‰∏∫Ê≠¢Êàë‰ª¨ËÆ®ËÆ∫‰∫ÜÁõÆÊ†áÂáΩÊï∞‰∏≠ËÆ≠ÁªÉËØØÂ∑ÆÁöÑÈÉ®ÂàÜ„ÄÇÊé•‰∏ãÊù•Êàë‰ª¨ËÆ®ËÆ∫Â¶Ç‰ΩïÂÆö‰πâÊ†ëÁöÑÂ§çÊùÇÂ∫¶„ÄÇÊàë‰ª¨ÂÖàÂØπ‰∫éfÁöÑÂÆö‰πâÂÅö‰∏Ä‰∏ãÁªÜÂåñÔºåÊääÊ†ëÊãÜÂàÜÊàêÁªìÊûÑÈÉ®ÂàÜqÂíåÂè∂Â≠êÊùÉÈáçÈÉ®ÂàÜw„ÄÇ‰∏ãÂõæÊòØ‰∏Ä‰∏™ÂÖ∑‰ΩìÁöÑ‰æãÂ≠ê„ÄÇÁªìÊûÑÂáΩÊï∞qÊääËæìÂÖ•Êò†Â∞ÑÂà∞Âè∂Â≠êÁöÑÁ¥¢ÂºïÂè∑‰∏äÈù¢ÂéªÔºåËÄåwÁªôÂÆö‰∫ÜÊØè‰∏™Á¥¢ÂºïÂè∑ÂØπÂ∫îÁöÑÂè∂Â≠êÂàÜÊï∞ÊòØ‰ªÄ‰πà„ÄÇ ÂΩìÊàë‰ª¨ÁªôÂÆö‰∫ÜÂ¶Ç‰∏äÂÆö‰πâ‰πãÂêéÔºåÊàë‰ª¨ÂèØ‰ª•ÂÆö‰πâ‰∏ÄÊ£µÊ†ëÁöÑÂ§çÊùÇÂ∫¶Â¶Ç‰∏ã„ÄÇËøô‰∏™Â§çÊùÇÂ∫¶ÂåÖÂê´‰∫Ü‰∏ÄÊ£µÊ†ëÈáåÈù¢ËäÇÁÇπÁöÑ‰∏™Êï∞Ôºå‰ª•ÂèäÊØè‰∏™Ê†ëÂè∂Â≠êËäÇÁÇπ‰∏äÈù¢ËæìÂá∫ÂàÜÊï∞ÁöÑ$L2$Ê®°Âπ≥Êñπ„ÄÇÂΩìÁÑ∂Ëøô‰∏çÊòØÂîØ‰∏ÄÁöÑ‰∏ÄÁßçÂÆö‰πâÊñπÂºèÔºå‰∏çËøáËøô‰∏ÄÂÆö‰πâÊñπÂºèÂ≠¶‰π†Âá∫ÁöÑÊ†ëÊïàÊûú‰∏ÄËà¨ÈÉΩÊØîËæÉ‰∏çÈîô„ÄÇ‰∏ãÂõæËøòÁªôÂá∫‰∫ÜÂ§çÊùÇÂ∫¶ËÆ°ÁÆóÁöÑ‰∏Ä‰∏™‰æãÂ≠ê„ÄÇ v. ÂÖ≥ÈîÆÊ≠•È™§ Êé•‰∏ãÊù•ÊòØÊúÄÂÖ≥ÈîÆÁöÑ‰∏ÄÊ≠•11ÔºåÂú®ËøôÁßçÊñ∞ÁöÑÂÆö‰πâ‰∏ãÔºåÊàë‰ª¨ÂèØ‰ª•ÊääÁõÆÊ†áÂáΩÊï∞ËøõË°åÂ¶Ç‰∏ãÊîπÂÜôÔºåÂÖ∂‰∏≠IË¢´ÂÆö‰πâ‰∏∫ÊØè‰∏™Âè∂Â≠ê‰∏äÈù¢Ê†∑Êú¨ÈõÜÂêà Ij = { i|q(xi)=j} Ëøô‰∏Ä‰∏™ÁõÆÊ†áÂåÖÂê´‰∫ÜT‰∏™Áõ∏‰∫íÁã¨Á´ãÁöÑÂçïÂèòÈáè‰∫åÊ¨°ÂáΩÊï∞„ÄÇÊàë‰ª¨ÂèØ‰ª•ÂÆö‰πâ ÈÇ£‰πàËøô‰∏™ÁõÆÊ†áÂáΩÊï∞ÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÊîπÂÜôÊàêÂ¶Ç‰∏ãÁöÑÂΩ¢ÂºèÔºåÂÅáËÆæÊàë‰ª¨Â∑≤ÁªèÁü•ÈÅìÊ†ëÁöÑÁªìÊûÑqÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄöËøáËøô‰∏™ÁõÆÊ†áÂáΩÊï∞Êù•Ê±ÇËß£Âá∫ÊúÄÂ•ΩÁöÑwÔºå‰ª•ÂèäÊúÄÂ•ΩÁöÑwÂØπÂ∫îÁöÑÁõÆÊ†áÂáΩÊï∞ÊúÄÂ§ßÁöÑÂ¢ûÁõä Ëøô‰∏§‰∏™ÁöÑÁªìÊûúÂØπÂ∫îÂ¶Ç‰∏ãÔºåÂ∑¶ËæπÊòØÊúÄÂ•ΩÁöÑwÔºåÂè≥ËæπÊòØËøô‰∏™wÂØπÂ∫îÁöÑÁõÆÊ†áÂáΩÊï∞ÁöÑÂÄº„ÄÇÂà∞ËøôÈáåÂ§ßÂÆ∂ÂèØËÉΩ‰ºöËßâÂæóËøô‰∏™Êé®ÂØºÁï•Â§çÊùÇ„ÄÇÂÖ∂ÂÆûËøôÈáåÂè™Ê∂âÂèäÂà∞‰∫ÜÂ¶Ç‰ΩïÊ±Ç‰∏Ä‰∏™‰∏ÄÁª¥‰∫åÊ¨°ÂáΩÊï∞ÁöÑÊúÄÂ∞èÂÄºÁöÑÈóÆÈ¢ò12„ÄÇÂ¶ÇÊûúËßâÂæóÊ≤°ÊúâÁêÜËß£‰∏çÂ¶®ÂÜç‰ªîÁªÜÁê¢Á£®‰∏Ä‰∏ã vi. ÊâìÂàÜÂáΩÊï∞ËÆ°ÁÆó‰∏æ‰æã Obj‰ª£Ë°®‰∫ÜÂΩìÊàë‰ª¨ÊåáÂÆö‰∏Ä‰∏™Ê†ëÁöÑÁªìÊûÑÁöÑÊó∂ÂÄôÔºåÊàë‰ª¨Âú®ÁõÆÊ†á‰∏äÈù¢ÊúÄÂ§öÂáèÂ∞ëÂ§öÂ∞ë„ÄÇÊàë‰ª¨ÂèØ‰ª•ÊääÂÆÉÂè´ÂÅöÁªìÊûÑÂàÜÊï∞(structure score)„ÄÇ‰Ω†ÂèØ‰ª•ËÆ§‰∏∫Ëøô‰∏™Â∞±ÊòØÁ±ª‰ººÂêâÂ∞ºÁ≥ªÊï∞‰∏ÄÊ†∑Êõ¥Âä†‰∏ÄËà¨ÁöÑÂØπ‰∫éÊ†ëÁªìÊûÑËøõË°åÊâìÂàÜÁöÑÂáΩÊï∞„ÄÇ‰∏ãÈù¢ÊòØ‰∏Ä‰∏™ÂÖ∑‰ΩìÁöÑÊâìÂàÜÂáΩÊï∞ËÆ°ÁÆóÁöÑ‰æãÂ≠ê vii. Êûö‰∏æÊâÄÊúâ‰∏çÂêåÊ†ëÁªìÊûÑÁöÑË¥™ÂøÉÊ≥ï ÊâÄ‰ª•Êàë‰ª¨ÁöÑÁÆóÊ≥ï‰πüÂæàÁÆÄÂçïÔºåÊàë‰ª¨‰∏çÊñ≠Âú∞Êûö‰∏æ‰∏çÂêåÊ†ëÁöÑÁªìÊûÑÔºåÂà©Áî®Ëøô‰∏™ÊâìÂàÜÂáΩÊï∞Êù•ÂØªÊâæÂá∫‰∏Ä‰∏™ÊúÄ‰ºòÁªìÊûÑÁöÑÊ†ëÔºåÂä†ÂÖ•Âà∞Êàë‰ª¨ÁöÑÊ®°Âûã‰∏≠ÔºåÂÜçÈáçÂ§çËøôÊ†∑ÁöÑÊìç‰Ωú„ÄÇ‰∏çËøáÊûö‰∏æÊâÄÊúâÊ†ëÁªìÊûÑËøô‰∏™Êìç‰Ωú‰∏çÂ§™ÂèØË°åÔºåÊâÄ‰ª•Â∏∏Áî®ÁöÑÊñπÊ≥ïÊòØË¥™ÂøÉÊ≥ïÔºåÊØè‰∏ÄÊ¨°Â∞ùËØïÂéªÂØπÂ∑≤ÊúâÁöÑÂè∂Â≠êÂä†ÂÖ•‰∏Ä‰∏™ÂàÜÂâ≤„ÄÇÂØπ‰∫é‰∏Ä‰∏™ÂÖ∑‰ΩìÁöÑÂàÜÂâ≤ÊñπÊ°àÔºåÊàë‰ª¨ÂèØ‰ª•Ëé∑ÂæóÁöÑÂ¢ûÁõäÂèØ‰ª•Áî±Â¶Ç‰∏ãÂÖ¨ÂºèËÆ°ÁÆó ÂØπ‰∫éÊØèÊ¨°Êâ©Â±ïÔºåÊàë‰ª¨ËøòÊòØË¶ÅÊûö‰∏æÊâÄÊúâÂèØËÉΩÁöÑÂàÜÂâ≤ÊñπÊ°àÔºåÂ¶Ç‰ΩïÈ´òÊïàÂú∞Êûö‰∏æÊâÄÊúâÁöÑÂàÜÂâ≤Âë¢ÔºüÊàëÂÅáËÆæÊàë‰ª¨Ë¶ÅÊûö‰∏æÊâÄÊúâ x&lt;a ËøôÊ†∑ÁöÑÊù°‰ª∂ÔºåÂØπ‰∫éÊüê‰∏™ÁâπÂÆöÁöÑÂàÜÂâ≤aÊàë‰ª¨Ë¶ÅËÆ°ÁÆóaÂ∑¶ËæπÂíåÂè≥ËæπÁöÑÂØºÊï∞Âíå„ÄÇ Êàë‰ª¨ÂèØ‰ª•ÂèëÁé∞ÂØπ‰∫éÊâÄÊúâÁöÑaÔºåÊàë‰ª¨Âè™Ë¶ÅÂÅö‰∏ÄÈÅç‰ªéÂ∑¶Âà∞Âè≥ÁöÑÊâ´ÊèèÂ∞±ÂèØ‰ª•Êûö‰∏æÂá∫ÊâÄÊúâÂàÜÂâ≤ÁöÑÊ¢ØÂ∫¶ÂíåGLÂíåGR„ÄÇÁÑ∂ÂêéÁî®‰∏äÈù¢ÁöÑÂÖ¨ÂºèËÆ°ÁÆóÊØè‰∏™ÂàÜÂâ≤ÊñπÊ°àÁöÑÂàÜÊï∞Â∞±ÂèØ‰ª•‰∫Ü„ÄÇ ËßÇÂØüËøô‰∏™ÁõÆÊ†áÂáΩÊï∞ÔºåÂ§ßÂÆ∂‰ºöÂèëÁé∞Á¨¨‰∫å‰∏™ÂÄºÂæóÊ≥®ÊÑèÁöÑ‰∫ãÊÉÖÂ∞±ÊòØÂºïÂÖ•ÂàÜÂâ≤‰∏ç‰∏ÄÂÆö‰ºö‰ΩøÂæóÊÉÖÂÜµÂèòÂ•ΩÔºåÂõ†‰∏∫Êàë‰ª¨Êúâ‰∏Ä‰∏™ÂºïÂÖ•Êñ∞Âè∂Â≠êÁöÑÊÉ©ÁΩöÈ°π„ÄÇ‰ºòÂåñËøô‰∏™ÁõÆÊ†áÂØπÂ∫î‰∫ÜÊ†ëÁöÑÂâ™ÊûùÔºå ÂΩìÂºïÂÖ•ÁöÑÂàÜÂâ≤Â∏¶Êù•ÁöÑÂ¢ûÁõäÂ∞è‰∫é‰∏Ä‰∏™ÈòÄÂÄºÁöÑÊó∂ÂÄôÔºåÊàë‰ª¨ÂèØ‰ª•Ââ™ÊéâËøô‰∏™ÂàÜÂâ≤„ÄÇÂ§ßÂÆ∂ÂèØ‰ª•ÂèëÁé∞ÔºåÂΩìÊàë‰ª¨Ê≠£ÂºèÂú∞Êé®ÂØºÁõÆÊ†áÁöÑÊó∂ÂÄôÔºåÂÉèËÆ°ÁÆóÂàÜÊï∞ÂíåÂâ™ÊûùËøôÊ†∑ÁöÑÁ≠ñÁï•ÈÉΩ‰ºöËá™ÁÑ∂Âú∞Âá∫Áé∞ÔºåËÄå‰∏çÂÜçÊòØ‰∏ÄÁßçÂõ†‰∏∫heuristicËÄåËøõË°åÁöÑÊìç‰Ωú‰∫Ü„ÄÇ ËÆ≤Âà∞ËøôÈáåÊñáÁ´†ËøõÂÖ•‰∫ÜÂ∞æÂ£∞ÔºåËôΩÁÑ∂Êúâ‰∫õÈïøÔºåÂ∏åÊúõÂØπÂ§ßÂÆ∂ÊúâÊâÄÂ∏ÆÂä©ÔºåËøôÁØáÊñáÁ´†‰ªãÁªç‰∫ÜÂ¶Ç‰ΩïÈÄöËøáÁõÆÊ†áÂáΩÊï∞‰ºòÂåñÁöÑÊñπÊ≥ïÊØîËæÉ‰∏•Ê†ºÂú∞Êé®ÂØºÂá∫boosted treeÁöÑÂ≠¶‰π†„ÄÇÂõ†‰∏∫ÊúâËøôÊ†∑‰∏ÄËà¨ÁöÑÊé®ÂØºÔºåÂæóÂà∞ÁöÑÁÆóÊ≥ïÂèØ‰ª•Áõ¥Êé•Â∫îÁî®Âà∞ÂõûÂΩíÔºåÂàÜÁ±ªÊéíÂ∫èÁ≠âÂêÑ‰∏™Â∫îÁî®Âú∫ÊôØ‰∏≠Âéª„ÄÇ 5 Â∞æÂ£∞Ôºöxgboost ËøôÁØáÊñáÁ´†ËÆ≤ÁöÑÊâÄÊúâÊé®ÂØºÂíåÊäÄÊúØÈÉΩÊåáÂØº‰∫Üxgboost ÁöÑËÆæËÆ°„ÄÇxgboostÊòØÂ§ßËßÑÊ®°Âπ∂Ë°åboosted treeÁöÑÂ∑•ÂÖ∑ÔºåÂÆÉÊòØÁõÆÂâçÊúÄÂø´ÊúÄÂ•ΩÁöÑÂºÄÊ∫êboosted treeÂ∑•ÂÖ∑ÂåÖÔºåÊØîÂ∏∏ËßÅÁöÑÂ∑•ÂÖ∑ÂåÖÂø´10ÂÄç‰ª•‰∏ä„ÄÇÂú®Êï∞ÊçÆÁßëÂ≠¶ÊñπÈù¢ÔºåÊúâÂ§ßÈáèkaggleÈÄâÊâãÈÄâÁî®ÂÆÉËøõË°åÊï∞ÊçÆÊåñÊéòÊØîËµõÔºåÂÖ∂‰∏≠ÂåÖÊã¨‰∏§‰∏™‰ª•‰∏äkaggleÊØîËµõÁöÑÂ§∫ÂÜ†ÊñπÊ°à„ÄÇÂú®Â∑•‰∏öÁïåËßÑÊ®°ÊñπÈù¢ÔºåxgboostÁöÑÂàÜÂ∏ÉÂºèÁâàÊú¨ÊúâÂπøÊ≥õÁöÑÂèØÁßªÊ§çÊÄßÔºåÊîØÊåÅÂú®YARN, MPI, Sungrid EngineÁ≠âÂêÑ‰∏™Âπ≥Âè∞‰∏äÈù¢ËøêË°åÔºåÂπ∂‰∏î‰øùÁïô‰∫ÜÂçïÊú∫Âπ∂Ë°åÁâàÊú¨ÁöÑÂêÑÁßç‰ºòÂåñÔºå‰ΩøÂæóÂÆÉÂèØ‰ª•ÂæàÂ•ΩÂú∞Ëß£ÂÜ≥‰∫éÂ∑•‰∏öÁïåËßÑÊ®°ÁöÑÈóÆÈ¢ò„ÄÇÊúâÂÖ¥Ë∂£ÁöÑÂêåÂ≠¶ÂèØ‰ª•Â∞ùËØï‰ΩøÁî®‰∏Ä‰∏ãÔºå‰πüÊ¨¢ËøéË¥°ÁåÆ‰ª£Á†Å„ÄÇ]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ê∑±ÂÖ•ÁêÜËß£GBDT]]></title>
    <url>%2F2018%2F06%2F05%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3GBDT%2F</url>
    <content type="text"><![CDATA[Ê∑±ÂÖ•ÁêÜËß£GBDT GBDT(Gradient Boosting Decision Tree) ÂèàÂè´ MARTÔºàMultiple Additive Regression Tree)ÔºåÊòØ‰∏ÄÁßçËø≠‰ª£ÁöÑÂÜ≥Á≠ñÊ†ëÁÆóÊ≥ïÔºåËØ•ÁÆóÊ≥ïÁî±Â§öÊ£µÂÜ≥Á≠ñÊ†ëÁªÑÊàêÔºåÊâÄÊúâÊ†ëÁöÑÁªìËÆ∫Á¥ØÂä†Ëµ∑Êù•ÂÅöÊúÄÁªàÁ≠îÊ°à„ÄÇÂÆÉÂú®Ë¢´ÊèêÂá∫‰πãÂàùÂ∞±ÂíåSVM‰∏ÄËµ∑Ë¢´ËÆ§‰∏∫ÊòØÊ≥õÂåñËÉΩÂäõÔºàgeneralization)ËæÉÂº∫ÁöÑÁÆóÊ≥ï„ÄÇËøë‰∫õÂπ¥Êõ¥Âõ†‰∏∫Ë¢´Áî®‰∫éÊêúÁ¥¢ÊéíÂ∫èÁöÑÊú∫Âô®Â≠¶‰π†Ê®°ÂûãËÄåÂºïËµ∑Â§ßÂÆ∂ÂÖ≥Ê≥®„ÄÇ GBDT‰∏ªË¶ÅÁî±‰∏â‰∏™Ê¶ÇÂøµÁªÑÊàêÔºöRegression Decistion TreeÔºàÂç≥DT)ÔºåGradient BoostingÔºàÂç≥GB)ÔºåShrinkage (ÁÆóÊ≥ïÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÊºîËøõÂàÜÊûùÔºåÁõÆÂâçÂ§ßÈÉ®ÂàÜÊ∫êÁ†ÅÈÉΩÊåâËØ•ÁâàÊú¨ÂÆûÁé∞Ôºâ„ÄÇÊêûÂÆöËøô‰∏â‰∏™Ê¶ÇÂøµÂêéÂ∞±ËÉΩÊòéÁôΩGBDTÊòØÂ¶Ç‰ΩïÂ∑•‰ΩúÁöÑÔºåË¶ÅÁªßÁª≠ÁêÜËß£ÂÆÉÂ¶Ç‰ΩïÁî®‰∫éÊêúÁ¥¢ÊéíÂ∫èÂàôÈúÄË¶ÅÈ¢ùÂ§ñÁêÜËß£RankNetÊ¶ÇÂøµÔºå‰πãÂêé‰æøÂäüÂæ∑ÂúÜÊª°„ÄÇ‰∏ãÊñáÂ∞ÜÈÄê‰∏™Á¢éÁâá‰ªãÁªçÔºåÊúÄÁªàÊääÊï¥Âº†ÂõæÊãºÂá∫Êù•„ÄÇ ‰∏Ä„ÄÅ DTÔºöÂõûÂΩíÊ†ë Regression Decision Tree ÊèêËµ∑ÂÜ≥Á≠ñÊ†ëÔºàDT, Decision Tree) ÁªùÂ§ßÈÉ®ÂàÜ‰∫∫È¶ñÂÖàÊÉ≥Âà∞ÁöÑÂ∞±ÊòØC4.5ÂàÜÁ±ªÂÜ≥Á≠ñÊ†ë„ÄÇ‰ΩÜÂ¶ÇÊûú‰∏ÄÂºÄÂßãÂ∞±ÊääGBDT‰∏≠ÁöÑÊ†ëÊÉ≥ÊàêÂàÜÁ±ªÊ†ëÔºåÈÇ£Â∞±ÊòØ‰∏ÄÊù°Ê≠™Ë∑ØËµ∞Âà∞ÈªëÔºå‰∏ÄË∑ØÂêÑÁßçÂùëÔºåÊúÄÁªàÊëîÂæóÈÉΩË¶ÅÂíØË°Ä‰∫ÜËøòÊòØ‰∏ÄÂ§¥ÈõæÊ∞¥ËØ¥ÁöÑÂ∞±ÊòØLZËá™Â∑±ÂïäÊúâÊú®Êúâ„ÄÇÂí≥ÂóØÔºåÊâÄ‰ª•ËØ¥ÂçÉ‰∏á‰∏çË¶Å‰ª•‰∏∫GBDTÊòØÂæàÂ§öÊ£µÂàÜÁ±ªÊ†ë„ÄÇÂÜ≥Á≠ñÊ†ëÂàÜ‰∏∫‰∏§Â§ßÁ±ªÔºåÂõûÂΩíÊ†ëÂíåÂàÜÁ±ªÊ†ë„ÄÇÂâçËÄÖÁî®‰∫éÈ¢ÑÊµãÂÆûÊï∞ÂÄºÔºåÂ¶ÇÊòéÂ§©ÁöÑÊ∏©Â∫¶„ÄÅÁî®Êà∑ÁöÑÂπ¥ÈæÑ„ÄÅÁΩëÈ°µÁöÑÁõ∏ÂÖ≥Á®ãÂ∫¶ÔºõÂêéËÄÖÁî®‰∫éÂàÜÁ±ªÊ†áÁ≠æÂÄºÔºåÂ¶ÇÊô¥Â§©/Èò¥Â§©/Èõæ/Èõ®„ÄÅÁî®Êà∑ÊÄßÂà´„ÄÅÁΩëÈ°µÊòØÂê¶ÊòØÂûÉÂúæÈ°µÈù¢„ÄÇËøôÈáåË¶ÅÂº∫Ë∞ÉÁöÑÊòØÔºåÂâçËÄÖÁöÑÁªìÊûúÂä†ÂáèÊòØÊúâÊÑè‰πâÁöÑÔºåÂ¶Ç10Â≤Å+5Â≤Å-3Â≤Å=12Â≤ÅÔºåÂêéËÄÖÂàôÊó†ÊÑè‰πâÔºåÂ¶ÇÁî∑+Áî∑+Â•≥=Âà∞Â∫ïÊòØÁî∑ÊòØÂ•≥Ôºü GBDTÁöÑÊ†∏ÂøÉÂú®‰∫éÁ¥ØÂä†ÊâÄÊúâÊ†ëÁöÑÁªìÊûú‰Ωú‰∏∫ÊúÄÁªàÁªìÊûúÔºåÂ∞±ÂÉèÂâçÈù¢ÂØπÂπ¥ÈæÑÁöÑÁ¥ØÂä†Ôºà-3ÊòØÂä†Ë¥ü3ÔºâÔºåËÄåÂàÜÁ±ªÊ†ëÁöÑÁªìÊûúÊòæÁÑ∂ÊòØÊ≤°ÂäûÊ≥ïÁ¥ØÂä†ÁöÑÔºåÊâÄ‰ª•GBDT‰∏≠ÁöÑÊ†ëÈÉΩÊòØÂõûÂΩíÊ†ëÔºå‰∏çÊòØÂàÜÁ±ªÊ†ëÔºåËøôÁÇπÂØπÁêÜËß£GBDTÁõ∏ÂΩìÈáçË¶ÅÔºàÂ∞ΩÁÆ°GBDTË∞ÉÊï¥Âêé‰πüÂèØÁî®‰∫éÂàÜÁ±ª‰ΩÜ‰∏ç‰ª£Ë°®GBDTÁöÑÊ†ëÊòØÂàÜÁ±ªÊ†ëÔºâ„ÄÇÈÇ£‰πàÂõûÂΩíÊ†ëÊòØÂ¶Ç‰ΩïÂ∑•‰ΩúÁöÑÂë¢Ôºü ‰∏ãÈù¢Êàë‰ª¨‰ª•ÂØπ‰∫∫ÁöÑÊÄßÂà´Âà§Âà´/Âπ¥ÈæÑÈ¢ÑÊµã‰∏∫‰æãÊù•ËØ¥ÊòéÔºåÊØè‰∏™instanceÈÉΩÊòØ‰∏Ä‰∏™Êàë‰ª¨Â∑≤Áü•ÊÄßÂà´/Âπ¥ÈæÑÁöÑ‰∫∫ÔºåËÄåfeatureÂàôÂåÖÊã¨Ëøô‰∏™‰∫∫‰∏äÁΩëÁöÑÊó∂Èïø„ÄÅ‰∏äÁΩëÁöÑÊó∂ÊÆµ„ÄÅÁΩëË¥≠ÊâÄËä±ÁöÑÈáëÈ¢ùÁ≠â„ÄÇ ‰Ωú‰∏∫ÂØπÊØîÔºåÂÖàËØ¥ÂàÜÁ±ªÊ†ëÔºåÊàë‰ª¨Áü•ÈÅìC4.5ÂàÜÁ±ªÊ†ëÂú®ÊØèÊ¨°ÂàÜÊûùÊó∂ÔºåÊòØÁ©∑‰∏æÊØè‰∏Ä‰∏™featureÁöÑÊØè‰∏Ä‰∏™ÈòàÂÄºÔºåÊâæÂà∞‰ΩøÂæóÊåâÁÖßfeature&lt;=ÈòàÂÄºÔºåÂíåfeature&gt;ÈòàÂÄºÂàÜÊàêÁöÑ‰∏§‰∏™ÂàÜÊûùÁöÑÁÜµÊúÄÂ§ßÁöÑfeatureÂíåÈòàÂÄºÔºàÁÜµÊúÄÂ§ßÁöÑÊ¶ÇÂøµÂèØÁêÜËß£ÊàêÂ∞ΩÂèØËÉΩÊØè‰∏™ÂàÜÊûùÁöÑÁî∑Â•≥ÊØî‰æãÈÉΩËøúÁ¶ª1:1ÔºâÔºåÊåâÁÖßËØ•Ê†áÂáÜÂàÜÊûùÂæóÂà∞‰∏§‰∏™Êñ∞ËäÇÁÇπÔºåÁî®ÂêåÊ†∑ÊñπÊ≥ïÁªßÁª≠ÂàÜÊûùÁõ¥Âà∞ÊâÄÊúâ‰∫∫ÈÉΩË¢´ÂàÜÂÖ•ÊÄßÂà´ÂîØ‰∏ÄÁöÑÂè∂Â≠êËäÇÁÇπÔºåÊàñËææÂà∞È¢ÑËÆæÁöÑÁªàÊ≠¢Êù°‰ª∂ÔºåËã•ÊúÄÁªàÂè∂Â≠êËäÇÁÇπ‰∏≠ÁöÑÊÄßÂà´‰∏çÂîØ‰∏ÄÔºåÂàô‰ª•Â§öÊï∞‰∫∫ÁöÑÊÄßÂà´‰Ωú‰∏∫ËØ•Âè∂Â≠êËäÇÁÇπÁöÑÊÄßÂà´„ÄÇ ÂõûÂΩíÊ†ëÊÄª‰ΩìÊµÅÁ®ã‰πüÊòØÁ±ª‰ººÔºå‰∏çËøáÂú®ÊØè‰∏™ËäÇÁÇπÔºà‰∏ç‰∏ÄÂÆöÊòØÂè∂Â≠êËäÇÁÇπÔºâÈÉΩ‰ºöÂæó‰∏Ä‰∏™È¢ÑÊµãÂÄºÔºå‰ª•Âπ¥ÈæÑ‰∏∫‰æãÔºåËØ•È¢ÑÊµãÂÄºÁ≠â‰∫éÂ±û‰∫éËøô‰∏™ËäÇÁÇπÁöÑÊâÄÊúâ‰∫∫Âπ¥ÈæÑÁöÑÂπ≥ÂùáÂÄº„ÄÇÂàÜÊûùÊó∂Á©∑‰∏æÊØè‰∏Ä‰∏™featureÁöÑÊØè‰∏™ÈòàÂÄºÊâæÊúÄÂ•ΩÁöÑÂàÜÂâ≤ÁÇπÔºå‰ΩÜË°°ÈáèÊúÄÂ•ΩÁöÑÊ†áÂáÜ‰∏çÂÜçÊòØÊúÄÂ§ßÁÜµÔºåËÄåÊòØÊúÄÂ∞èÂåñÂùáÊñπÂ∑Æ‚ÄìÂç≥ÔºàÊØè‰∏™‰∫∫ÁöÑÂπ¥ÈæÑ-È¢ÑÊµãÂπ¥ÈæÑÔºâ^2 ÁöÑÊÄªÂíå / NÔºåÊàñËÄÖËØ¥ÊòØÊØè‰∏™‰∫∫ÁöÑÈ¢ÑÊµãËØØÂ∑ÆÂπ≥ÊñπÂíå Èô§‰ª• N„ÄÇËøôÂæàÂ•ΩÁêÜËß£ÔºåË¢´È¢ÑÊµãÂá∫ÈîôÁöÑ‰∫∫Êï∞Ë∂äÂ§öÔºåÈîôÁöÑË∂äÁ¶ªË∞±ÔºåÂùáÊñπÂ∑ÆÂ∞±Ë∂äÂ§ßÔºåÈÄöËøáÊúÄÂ∞èÂåñÂùáÊñπÂ∑ÆËÉΩÂ§üÊâæÂà∞ÊúÄÈù†Ë∞±ÁöÑÂàÜÊûù‰æùÊçÆ„ÄÇÂàÜÊûùÁõ¥Âà∞ÊØè‰∏™Âè∂Â≠êËäÇÁÇπ‰∏ä‰∫∫ÁöÑÂπ¥ÈæÑÈÉΩÂîØ‰∏ÄÔºàËøôÂ§™Èöæ‰∫ÜÔºâÊàñËÄÖËææÂà∞È¢ÑËÆæÁöÑÁªàÊ≠¢Êù°‰ª∂ÔºàÂ¶ÇÂè∂Â≠ê‰∏™Êï∞‰∏äÈôêÔºâÔºåËã•ÊúÄÁªàÂè∂Â≠êËäÇÁÇπ‰∏ä‰∫∫ÁöÑÂπ¥ÈæÑ‰∏çÂîØ‰∏ÄÔºåÂàô‰ª•ËØ•ËäÇÁÇπ‰∏äÊâÄÊúâ‰∫∫ÁöÑÂπ≥ÂùáÂπ¥ÈæÑÂÅö‰∏∫ËØ•Âè∂Â≠êËäÇÁÇπÁöÑÈ¢ÑÊµãÂπ¥ÈæÑ„ÄÇËã•Ëøò‰∏çÊòéÁôΩÂèØ‰ª•Google ‚ÄúRegression Tree‚ÄùÔºåÊàñÈòÖËØªÊú¨ÊñáÁöÑÁ¨¨‰∏ÄÁØáËÆ∫Êñá‰∏≠Regression TreeÈÉ®ÂàÜ„ÄÇ ‰∫å„ÄÅ GBÔºöÊ¢ØÂ∫¶Ëø≠‰ª£ Gradient Boosting Â•ΩÂêßÔºåÊàëËµ∑‰∫Ü‰∏Ä‰∏™ÂæàÂ§ßÁöÑÊ†áÈ¢òÔºå‰ΩÜ‰∫ãÂÆû‰∏äÊàëÂπ∂‰∏çÊÉ≥Â§öËÆ≤Gradient BoostingÁöÑÂéüÁêÜÔºåÂõ†‰∏∫‰∏çÊòéÁôΩÂéüÁêÜÂπ∂Êó†Á¢ç‰∫éÁêÜËß£GBDT‰∏≠ÁöÑGradient Boosting„ÄÇ BoostingÔºåËø≠‰ª£ÔºåÂç≥ÈÄöËøáËø≠‰ª£Â§öÊ£µÊ†ëÊù•ÂÖ±ÂêåÂÜ≥Á≠ñ„ÄÇËøôÊÄé‰πàÂÆûÁé∞Âë¢ÔºüÈöæÈÅìÊòØÊØèÊ£µÊ†ëÁã¨Á´ãËÆ≠ÁªÉ‰∏ÄÈÅçÔºåÊØîÂ¶ÇAËøô‰∏™‰∫∫ÔºåÁ¨¨‰∏ÄÊ£µÊ†ëËÆ§‰∏∫ÊòØ10Â≤ÅÔºåÁ¨¨‰∫åÊ£µÊ†ëËÆ§‰∏∫ÊòØ0Â≤ÅÔºåÁ¨¨‰∏âÊ£µÊ†ëËÆ§‰∏∫ÊòØ20Â≤ÅÔºåÊàë‰ª¨Â∞±ÂèñÂπ≥ÂùáÂÄº10Â≤ÅÂÅöÊúÄÁªàÁªìËÆ∫Ôºü‚ÄìÂΩìÁÑ∂‰∏çÊòØÔºÅ‰∏î‰∏çËØ¥ËøôÊòØÊäïÁ•®ÊñπÊ≥ïÂπ∂‰∏çÊòØGBDTÔºåÂè™Ë¶ÅËÆ≠ÁªÉÈõÜ‰∏çÂèòÔºåÁã¨Á´ãËÆ≠ÁªÉ‰∏âÊ¨°ÁöÑ‰∏âÊ£µÊ†ëÂøÖÂÆöÂÆåÂÖ®Áõ∏ÂêåÔºåËøôÊ†∑ÂÅöÂÆåÂÖ®Ê≤°ÊúâÊÑè‰πâ„ÄÇ‰πãÂâçËØ¥ËøáÔºåGBDTÊòØÊääÊâÄÊúâÊ†ëÁöÑÁªìËÆ∫Á¥ØÂä†Ëµ∑Êù•ÂÅöÊúÄÁªàÁªìËÆ∫ÁöÑÔºåÊâÄ‰ª•ÂèØ‰ª•ÊÉ≥Âà∞ÊØèÊ£µÊ†ëÁöÑÁªìËÆ∫Âπ∂‰∏çÊòØÂπ¥ÈæÑÊú¨Ë∫´ÔºåËÄåÊòØÂπ¥ÈæÑÁöÑ‰∏Ä‰∏™Á¥ØÂä†Èáè„ÄÇGBDTÁöÑÊ†∏ÂøÉÂ∞±Âú®‰∫éÔºåÊØè‰∏ÄÊ£µÊ†ëÂ≠¶ÁöÑÊòØ‰πãÂâçÊâÄÊúâÊ†ëÁªìËÆ∫ÂíåÁöÑÊÆãÂ∑ÆÔºåËøô‰∏™ÊÆãÂ∑ÆÂ∞±ÊòØ‰∏Ä‰∏™Âä†È¢ÑÊµãÂÄºÂêéËÉΩÂæóÁúüÂÆûÂÄºÁöÑÁ¥ØÂä†Èáè„ÄÇÊØîÂ¶ÇAÁöÑÁúüÂÆûÂπ¥ÈæÑÊòØ18Â≤ÅÔºå‰ΩÜÁ¨¨‰∏ÄÊ£µÊ†ëÁöÑÈ¢ÑÊµãÂπ¥ÈæÑÊòØ12Â≤ÅÔºåÂ∑Æ‰∫Ü6Â≤ÅÔºåÂç≥ÊÆãÂ∑Æ‰∏∫6Â≤Å„ÄÇÈÇ£‰πàÂú®Á¨¨‰∫åÊ£µÊ†ëÈáåÊàë‰ª¨ÊääAÁöÑÂπ¥ÈæÑËÆæ‰∏∫6Â≤ÅÂéªÂ≠¶‰π†ÔºåÂ¶ÇÊûúÁ¨¨‰∫åÊ£µÊ†ëÁúüÁöÑËÉΩÊääAÂàÜÂà∞6Â≤ÅÁöÑÂè∂Â≠êËäÇÁÇπÔºåÈÇ£Á¥ØÂä†‰∏§Ê£µÊ†ëÁöÑÁªìËÆ∫Â∞±ÊòØAÁöÑÁúüÂÆûÂπ¥ÈæÑÔºõÂ¶ÇÊûúÁ¨¨‰∫åÊ£µÊ†ëÁöÑÁªìËÆ∫ÊòØ5Â≤ÅÔºåÂàôA‰ªçÁÑ∂Â≠òÂú®1Â≤ÅÁöÑÊÆãÂ∑ÆÔºåÁ¨¨‰∏âÊ£µÊ†ëÈáåAÁöÑÂπ¥ÈæÑÂ∞±ÂèòÊàê1Â≤ÅÔºåÁªßÁª≠Â≠¶„ÄÇËøôÂ∞±ÊòØGradient BoostingÂú®GBDT‰∏≠ÁöÑÊÑè‰πâÔºåÁÆÄÂçïÂêß„ÄÇ ‰∏â„ÄÅ GBDTÂ∑•‰ΩúËøáÁ®ãÂÆû‰æã„ÄÇ ËøòÊòØÂπ¥ÈæÑÈ¢ÑÊµãÔºåÁÆÄÂçïËµ∑ËßÅËÆ≠ÁªÉÈõÜÂè™Êúâ4‰∏™‰∫∫ÔºåA,B,C,DÔºå‰ªñ‰ª¨ÁöÑÂπ¥ÈæÑÂàÜÂà´ÊòØ14,16,24,26„ÄÇÂÖ∂‰∏≠A„ÄÅBÂàÜÂà´ÊòØÈ´ò‰∏ÄÂíåÈ´ò‰∏âÂ≠¶ÁîüÔºõC,DÂàÜÂà´ÊòØÂ∫îÂ±äÊØï‰∏öÁîüÂíåÂ∑•‰Ωú‰∏§Âπ¥ÁöÑÂëòÂ∑•„ÄÇÂ¶ÇÊûúÊòØÁî®‰∏ÄÊ£µ‰º†ÁªüÁöÑÂõûÂΩíÂÜ≥Á≠ñÊ†ëÊù•ËÆ≠ÁªÉÔºå‰ºöÂæóÂà∞Â¶Ç‰∏ãÂõæ1ÊâÄÁ§∫ÁªìÊûúÔºö Áé∞Âú®Êàë‰ª¨‰ΩøÁî®GBDTÊù•ÂÅöËøô‰ª∂‰∫ãÔºåÁî±‰∫éÊï∞ÊçÆÂ§™Â∞ëÔºåÊàë‰ª¨ÈôêÂÆöÂè∂Â≠êËäÇÁÇπÂÅöÂ§öÊúâ‰∏§‰∏™ÔºåÂç≥ÊØèÊ£µÊ†ëÈÉΩÂè™Êúâ‰∏Ä‰∏™ÂàÜÊûùÔºåÂπ∂‰∏îÈôêÂÆöÂè™Â≠¶‰∏§Ê£µÊ†ë„ÄÇÊàë‰ª¨‰ºöÂæóÂà∞Â¶Ç‰∏ãÂõæ2ÊâÄÁ§∫ÁªìÊûúÔºö Âú®Á¨¨‰∏ÄÊ£µÊ†ëÂàÜÊûùÂíåÂõæ1‰∏ÄÊ†∑ÔºåÁî±‰∫éA,BÂπ¥ÈæÑËæÉ‰∏∫Áõ∏ËøëÔºåC,DÂπ¥ÈæÑËæÉ‰∏∫Áõ∏ËøëÔºå‰ªñ‰ª¨Ë¢´ÂàÜ‰∏∫‰∏§Êã®ÔºåÊØèÊã®Áî®Âπ≥ÂùáÂπ¥ÈæÑ‰Ωú‰∏∫È¢ÑÊµãÂÄº„ÄÇÊ≠§Êó∂ËÆ°ÁÆóÊÆãÂ∑ÆÔºàÊÆãÂ∑ÆÁöÑÊÑèÊÄùÂ∞±ÊòØÔºö AÁöÑÈ¢ÑÊµãÂÄº + AÁöÑÊÆãÂ∑Æ = AÁöÑÂÆûÈôÖÂÄºÔºâÔºåÊâÄ‰ª•AÁöÑÊÆãÂ∑ÆÂ∞±ÊòØ16-15=1ÔºàÊ≥®ÊÑèÔºåAÁöÑÈ¢ÑÊµãÂÄºÊòØÊåáÂâçÈù¢ÊâÄÊúâÊ†ëÁ¥ØÂä†ÁöÑÂíåÔºåËøôÈáåÂâçÈù¢Âè™Êúâ‰∏ÄÊ£µÊ†ëÊâÄ‰ª•Áõ¥Êé•ÊòØ15ÔºåÂ¶ÇÊûúËøòÊúâÊ†ëÂàôÈúÄË¶ÅÈÉΩÁ¥ØÂä†Ëµ∑Êù•‰Ωú‰∏∫AÁöÑÈ¢ÑÊµãÂÄºÔºâ„ÄÇËøõËÄåÂæóÂà∞A,B,C,DÁöÑÊÆãÂ∑ÆÂàÜÂà´‰∏∫-1,1Ôºå-1,1„ÄÇÁÑ∂ÂêéÊàë‰ª¨ÊãøÊÆãÂ∑ÆÊõø‰ª£A,B,C,DÁöÑÂéüÂÄºÔºåÂà∞Á¨¨‰∫åÊ£µÊ†ëÂéªÂ≠¶‰π†ÔºåÂ¶ÇÊûúÊàë‰ª¨ÁöÑÈ¢ÑÊµãÂÄºÂíåÂÆÉ‰ª¨ÁöÑÊÆãÂ∑ÆÁõ∏Á≠âÔºåÂàôÂè™ÈúÄÊääÁ¨¨‰∫åÊ£µÊ†ëÁöÑÁªìËÆ∫Á¥ØÂä†Âà∞Á¨¨‰∏ÄÊ£µÊ†ë‰∏äÂ∞±ËÉΩÂæóÂà∞ÁúüÂÆûÂπ¥ÈæÑ‰∫Ü„ÄÇËøôÈáåÁöÑÊï∞ÊçÆÊòæÁÑ∂ÊòØÊàëÂèØ‰ª•ÂÅöÁöÑÔºåÁ¨¨‰∫åÊ£µÊ†ëÂè™Êúâ‰∏§‰∏™ÂÄº1Âíå-1ÔºåÁõ¥Êé•ÂàÜÊàê‰∏§‰∏™ËäÇÁÇπ„ÄÇÊ≠§Êó∂ÊâÄÊúâ‰∫∫ÁöÑÊÆãÂ∑ÆÈÉΩÊòØ0ÔºåÂç≥ÊØè‰∏™‰∫∫ÈÉΩÂæóÂà∞‰∫ÜÁúüÂÆûÁöÑÈ¢ÑÊµãÂÄº„ÄÇ Êç¢Âè•ËØùËØ¥ÔºåÁé∞Âú®A,B,C,DÁöÑÈ¢ÑÊµãÂÄºÈÉΩÂíåÁúüÂÆûÂπ¥ÈæÑ‰∏ÄËá¥‰∫Ü„ÄÇPerfect!Ôºö A: 14Â≤ÅÈ´ò‰∏ÄÂ≠¶ÁîüÔºåË¥≠Áâ©ËæÉÂ∞ëÔºåÁªèÂ∏∏ÈóÆÂ≠¶ÈïøÈóÆÈ¢òÔºõÈ¢ÑÊµãÂπ¥ÈæÑA = 15 ‚Äì 1 = 14 B: 16Â≤ÅÈ´ò‰∏âÂ≠¶ÁîüÔºõË¥≠Áâ©ËæÉÂ∞ëÔºåÁªèÂ∏∏Ë¢´Â≠¶ÂºüÈóÆÈóÆÈ¢òÔºõÈ¢ÑÊµãÂπ¥ÈæÑB = 15 + 1 = 16 C: 24Â≤ÅÂ∫îÂ±äÊØï‰∏öÁîüÔºõË¥≠Áâ©ËæÉÂ§öÔºåÁªèÂ∏∏ÈóÆÂ∏àÂÖÑÈóÆÈ¢òÔºõÈ¢ÑÊµãÂπ¥ÈæÑC = 25 ‚Äì 1 = 24 D: 26Â≤ÅÂ∑•‰Ωú‰∏§Âπ¥ÂëòÂ∑•ÔºõË¥≠Áâ©ËæÉÂ§öÔºåÁªèÂ∏∏Ë¢´Â∏àÂºüÈóÆÈóÆÈ¢òÔºõÈ¢ÑÊµãÂπ¥ÈæÑD = 25 + 1 = 26 ÈÇ£‰πàÂì™Èáå‰ΩìÁé∞‰∫ÜGradientÂë¢ÔºüÂÖ∂ÂÆûÂõûÂà∞Á¨¨‰∏ÄÊ£µÊ†ëÁªìÊùüÊó∂ÊÉ≥‰∏ÄÊÉ≥ÔºåÊó†ËÆ∫Ê≠§Êó∂ÁöÑcost functionÊòØ‰ªÄ‰πàÔºåÊòØÂùáÊñπÂ∑ÆËøòÊòØÂùáÂ∑ÆÔºåÂè™Ë¶ÅÂÆÉ‰ª•ËØØÂ∑Æ‰Ωú‰∏∫Ë°°ÈáèÊ†áÂáÜÔºåÊÆãÂ∑ÆÂêëÈáè(-1, 1, -1, 1)ÈÉΩÊòØÂÆÉÁöÑÂÖ®Â±ÄÊúÄ‰ºòÊñπÂêëÔºåËøôÂ∞±ÊòØGradient„ÄÇ ËÆ≤Âà∞ËøôÈáåÊàë‰ª¨Â∑≤ÁªèÊääGBDTÊúÄÊ†∏ÂøÉÁöÑÊ¶ÇÂøµ„ÄÅËøêÁÆóËøáÁ®ãËÆ≤ÂÆå‰∫ÜÔºÅÊ≤°ÈîôÂ∞±ÊòØËøô‰πàÁÆÄÂçï„ÄÇ‰∏çËøáËÆ≤Âà∞ËøôÈáåÂæàÂÆπÊòìÂèëÁé∞‰∏â‰∏™ÈóÆÈ¢òÔºö 1ÔºâÊó¢ÁÑ∂Âõæ1ÂíåÂõæ2 ÊúÄÁªàÊïàÊûúÁõ∏ÂêåÔºå‰∏∫‰ΩïËøòÈúÄË¶ÅGBDTÂë¢Ôºü Á≠îÊ°àÊòØËøáÊãüÂêà„ÄÇËøáÊãüÂêàÊòØÊåá‰∏∫‰∫ÜËÆ©ËÆ≠ÁªÉÈõÜÁ≤æÂ∫¶Êõ¥È´òÔºåÂ≠¶Âà∞‰∫ÜÂæàÂ§ö‚Äù‰ªÖÂú®ËÆ≠ÁªÉÈõÜ‰∏äÊàêÁ´ãÁöÑËßÑÂæã‚ÄúÔºåÂØºËá¥Êç¢‰∏Ä‰∏™Êï∞ÊçÆÈõÜÂΩìÂâçËßÑÂæãÂ∞±‰∏çÈÄÇÁî®‰∫Ü„ÄÇÂÖ∂ÂÆûÂè™Ë¶ÅÂÖÅËÆ∏‰∏ÄÊ£µÊ†ëÁöÑÂè∂Â≠êËäÇÁÇπË∂≥Â§üÂ§öÔºåËÆ≠ÁªÉÈõÜÊÄªÊòØËÉΩËÆ≠ÁªÉÂà∞100%ÂáÜÁ°ÆÁéáÁöÑÔºàÂ§ß‰∏ç‰∫ÜÊúÄÂêé‰∏Ä‰∏™Âè∂Â≠ê‰∏äÂè™Êúâ‰∏Ä‰∏™instance)„ÄÇÂú®ËÆ≠ÁªÉÁ≤æÂ∫¶ÂíåÂÆûÈôÖÁ≤æÂ∫¶ÔºàÊàñÊµãËØïÁ≤æÂ∫¶Ôºâ‰πãÈó¥ÔºåÂêéËÄÖÊâçÊòØÊàë‰ª¨ÊÉ≥Ë¶ÅÁúüÊ≠£ÂæóÂà∞ÁöÑ„ÄÇ Êàë‰ª¨ÂèëÁé∞Âõæ1‰∏∫‰∫ÜËææÂà∞100%Á≤æÂ∫¶‰ΩøÁî®‰∫Ü3‰∏™featureÔºà‰∏äÁΩëÊó∂Èïø„ÄÅÊó∂ÊÆµ„ÄÅÁΩëË¥≠ÈáëÈ¢ùÔºâÔºåÂÖ∂‰∏≠ÂàÜÊûù‚Äú‰∏äÁΩëÊó∂Èïø&gt;1.1h‚Äù ÂæàÊòæÁÑ∂Â∑≤ÁªèËøáÊãüÂêà‰∫ÜÔºåËøô‰∏™Êï∞ÊçÆÈõÜ‰∏äA,B‰πüËÆ∏ÊÅ∞Â•ΩAÊØèÂ§©‰∏äÁΩë1.09h, B‰∏äÁΩë1.05Â∞èÊó∂Ôºå‰ΩÜÁî®‰∏äÁΩëÊó∂Èó¥ÊòØ‰∏çÊòØ&gt;1.1Â∞èÊó∂Êù•Âà§Êñ≠ÊâÄÊúâ‰∫∫ÁöÑÂπ¥ÈæÑÂæàÊòæÁÑ∂ÊòØÊúâÊÇñÂ∏∏ËØÜÁöÑÔºõ Áõ∏ÂØπÊù•ËØ¥Âõæ2ÁöÑboostingËôΩÁÑ∂Áî®‰∫Ü‰∏§Ê£µÊ†ë Ôºå‰ΩÜÂÖ∂ÂÆûÂè™Áî®‰∫Ü2‰∏™featureÂ∞±ÊêûÂÆö‰∫ÜÔºåÂêé‰∏Ä‰∏™featureÊòØÈóÆÁ≠îÊØî‰æãÔºåÊòæÁÑ∂Âõæ2ÁöÑ‰æùÊçÆÊõ¥Èù†Ë∞±„ÄÇÔºàÂΩìÁÑ∂ÔºåËøôÈáåÊòØLZÊïÖÊÑèÂÅöÁöÑÊï∞ÊçÆÔºåÊâÄ‰ª•ÊâçËÉΩÈù†Ë∞±ÂæóÂ¶ÇÊ≠§ÁãóË°Ä„ÄÇÂÆûÈôÖ‰∏≠Èù†Ë∞±‰∏çÈù†Ë∞±ÊÄªÊòØÁõ∏ÂØπÁöÑÔºâ BoostingÁöÑÊúÄÂ§ßÂ•ΩÂ§ÑÂú®‰∫éÔºåÊØè‰∏ÄÊ≠•ÁöÑÊÆãÂ∑ÆËÆ°ÁÆóÂÖ∂ÂÆûÂèòÁõ∏Âú∞Â¢ûÂ§ß‰∫ÜÂàÜÈîôinstanceÁöÑÊùÉÈáçÔºåËÄåÂ∑≤ÁªèÂàÜÂØπÁöÑinstanceÂàôÈÉΩË∂ãÂêë‰∫é0„ÄÇËøôÊ†∑ÂêéÈù¢ÁöÑÊ†ëÂ∞±ËÉΩË∂äÊù•Ë∂ä‰∏ìÊ≥®ÈÇ£‰∫õÂâçÈù¢Ë¢´ÂàÜÈîôÁöÑinstance„ÄÇÂ∞±ÂÉèÊàë‰ª¨ÂÅö‰∫íËÅîÁΩëÔºåÊÄªÊòØÂÖàËß£ÂÜ≥60%Áî®Êà∑ÁöÑÈúÄÊ±ÇÂáëÂêàÁùÄÔºåÂÜçËß£ÂÜ≥35%Áî®Êà∑ÁöÑÈúÄÊ±ÇÔºåÊúÄÂêéÊâçÂÖ≥Ê≥®ÈÇ£5%‰∫∫ÁöÑÈúÄÊ±ÇÔºåËøôÊ†∑Â∞±ËÉΩÈÄêÊ∏êÊää‰∫ßÂìÅÂÅöÂ•ΩÔºåÂõ†‰∏∫‰∏çÂêåÁ±ªÂûãÁî®Êà∑ÈúÄÊ±ÇÂèØËÉΩÂÆåÂÖ®‰∏çÂêåÔºåÈúÄË¶ÅÂàÜÂà´Áã¨Á´ãÂàÜÊûê„ÄÇÂ¶ÇÊûúÂèçËøáÊù•ÂÅöÔºåÊàñËÄÖÂàö‰∏äÊù•Â∞±‰∏ÄÂÆöË¶ÅÂÅöÂà∞Â∞ΩÂñÑÂ∞ΩÁæéÔºåÂæÄÂæÄÊúÄÁªà‰ºöÁ´πÁØÆÊâìÊ∞¥‰∏ÄÂú∫Á©∫„ÄÇ 2ÔºâGradientÂë¢Ôºü‰∏çÊòØ‚ÄúG‚ÄùBDT‰πàÔºü Âà∞ÁõÆÂâç‰∏∫Ê≠¢ÔºåÊàë‰ª¨ÁöÑÁ°ÆÊ≤°ÊúâÁî®Âà∞Ê±ÇÂØºÁöÑGradient„ÄÇÂú®ÂΩìÂâçÁâàÊú¨GBDTÊèèËø∞‰∏≠ÔºåÁöÑÁ°ÆÊ≤°ÊúâÁî®Âà∞GradientÔºåËØ•ÁâàÊú¨Áî®ÊÆãÂ∑Æ‰Ωú‰∏∫ÂÖ®Â±ÄÊúÄ‰ºòÁöÑÁªùÂØπÊñπÂêëÔºåÂπ∂‰∏çÈúÄË¶ÅGradientÊ±ÇËß£. 3ÔºâËøô‰∏çÊòØboostingÂêßÔºüAdaboostÂèØ‰∏çÊòØËøô‰πàÂÆö‰πâÁöÑ„ÄÇ ËøôÊòØboostingÔºå‰ΩÜ‰∏çÊòØAdaboost„ÄÇGBDT‰∏çÊòØAdaboost Decistion Tree„ÄÇÂ∞±ÂÉèÊèêÂà∞ÂÜ≥Á≠ñÊ†ëÂ§ßÂÆ∂‰ºöÊÉ≥Ëµ∑C4.5ÔºåÊèêÂà∞boostÂ§öÊï∞‰∫∫‰πü‰ºöÊÉ≥Âà∞Adaboost„ÄÇAdaboostÊòØÂè¶‰∏ÄÁßçboostÊñπÊ≥ïÔºåÂÆÉÊåâÂàÜÁ±ªÂØπÈîôÔºåÂàÜÈÖç‰∏çÂêåÁöÑweightÔºåËÆ°ÁÆócost functionÊó∂‰ΩøÁî®Ëøô‰∫õweightÔºå‰ªéËÄåËÆ©‚ÄúÈîôÂàÜÁöÑÊ†∑Êú¨ÊùÉÈáçË∂äÊù•Ë∂äÂ§ßÔºå‰ΩøÂÆÉ‰ª¨Êõ¥Ë¢´ÈáçËßÜ‚Äù„ÄÇBootstrap‰πüÊúâÁ±ª‰ººÊÄùÊÉ≥ÔºåÂÆÉÂú®ÊØè‰∏ÄÊ≠•Ëø≠‰ª£Êó∂‰∏çÊîπÂèòÊ®°ÂûãÊú¨Ë∫´Ôºå‰πü‰∏çËÆ°ÁÆóÊÆãÂ∑ÆÔºåËÄåÊòØ‰ªéN‰∏™instanceËÆ≠ÁªÉÈõÜ‰∏≠Êåâ‰∏ÄÂÆöÊ¶ÇÁéáÈáçÊñ∞ÊäΩÂèñN‰∏™instanceÂá∫Êù•ÔºàÂçï‰∏™instanceÂèØ‰ª•Ë¢´ÈáçÂ§çsampleÔºâÔºåÂØπÁùÄËøôN‰∏™Êñ∞ÁöÑinstanceÂÜçËÆ≠ÁªÉ‰∏ÄËΩÆ„ÄÇÁî±‰∫éÊï∞ÊçÆÈõÜÂèò‰∫ÜËø≠‰ª£Ê®°ÂûãËÆ≠ÁªÉÁªìÊûú‰πü‰∏ç‰∏ÄÊ†∑ÔºåËÄå‰∏Ä‰∏™instanceË¢´ÂâçÈù¢ÂàÜÈîôÁöÑË∂äÂéâÂÆ≥ÔºåÂÆÉÁöÑÊ¶ÇÁéáÂ∞±Ë¢´ËÆæÁöÑË∂äÈ´òÔºåËøôÊ†∑Â∞±ËÉΩÂêåÊ†∑ËææÂà∞ÈÄêÊ≠•ÂÖ≥Ê≥®Ë¢´ÂàÜÈîôÁöÑinstanceÔºåÈÄêÊ≠•ÂÆåÂñÑÁöÑÊïàÊûú„ÄÇAdaboostÁöÑÊñπÊ≥ïË¢´ÂÆûË∑µËØÅÊòéÊòØ‰∏ÄÁßçÂæàÂ•ΩÁöÑÈò≤Ê≠¢ËøáÊãüÂêàÁöÑÊñπÊ≥ïÔºå‰ΩÜËá≥‰∫é‰∏∫‰ªÄ‰πàÂàôËá≥‰ªäÊ≤°‰ªéÁêÜËÆ∫‰∏äË¢´ËØÅÊòé„ÄÇGBDT‰πüÂèØ‰ª•Âú®‰ΩøÁî®ÊÆãÂ∑ÆÁöÑÂêåÊó∂ÂºïÂÖ•Bootstrap re-samplingÔºåGBDTÂ§öÊï∞ÂÆûÁé∞ÁâàÊú¨‰∏≠‰πüÂ¢ûÂä†ÁöÑËøô‰∏™ÈÄâÈ°πÔºå‰ΩÜÊòØÂê¶‰∏ÄÂÆö‰ΩøÁî®ÂàôÊúâ‰∏çÂêåÁúãÊ≥ï„ÄÇre-sampling‰∏Ä‰∏™Áº∫ÁÇπÊòØÂÆÉÁöÑÈöèÊú∫ÊÄßÔºåÂç≥ÂêåÊ†∑ÁöÑÊï∞ÊçÆÈõÜÂêàËÆ≠ÁªÉ‰∏§ÈÅçÁªìÊûúÊòØ‰∏ç‰∏ÄÊ†∑ÁöÑÔºå‰πüÂ∞±ÊòØÊ®°Âûã‰∏çÂèØÁ®≥ÂÆöÂ§çÁé∞ÔºåËøôÂØπËØÑ‰º∞ÊòØÂæàÂ§ßÊåëÊàòÔºåÊØîÂ¶ÇÂæàÈöæËØ¥‰∏Ä‰∏™Ê®°ÂûãÂèòÂ•ΩÊòØÂõ†‰∏∫‰Ω†ÈÄâÁî®‰∫ÜÊõ¥Â•ΩÁöÑfeatureÔºåËøòÊòØÁî±‰∫éËøôÊ¨°sampleÁöÑÈöèÊú∫Âõ†Á¥†„ÄÇ Âõõ„ÄÅShrinkage ShrinkageÔºàÁº©ÂáèÔºâÁöÑÊÄùÊÉ≥ËÆ§‰∏∫ÔºåÊØèÊ¨°Ëµ∞‰∏ÄÂ∞èÊ≠•ÈÄêÊ∏êÈÄºËøëÁªìÊûúÁöÑÊïàÊûúÔºåË¶ÅÊØîÊØèÊ¨°Ëøà‰∏ÄÂ§ßÊ≠•ÂæàÂø´ÈÄºËøëÁªìÊûúÁöÑÊñπÂºèÊõ¥ÂÆπÊòìÈÅøÂÖçËøáÊãüÂêà„ÄÇÂç≥ÂÆÉ‰∏çÂÆåÂÖ®‰ø°‰ªªÊØè‰∏Ä‰∏™Ê£µÊÆãÂ∑ÆÊ†ëÔºåÂÆÉËÆ§‰∏∫ÊØèÊ£µÊ†ëÂè™Â≠¶Âà∞‰∫ÜÁúüÁêÜÁöÑ‰∏ÄÂ∞èÈÉ®ÂàÜÔºåÁ¥ØÂä†ÁöÑÊó∂ÂÄôÂè™Á¥ØÂä†‰∏ÄÂ∞èÈÉ®ÂàÜÔºåÈÄöËøáÂ§öÂ≠¶Âá†Ê£µÊ†ëÂº•Ë°•‰∏çË∂≥„ÄÇÁî®ÊñπÁ®ãÊù•ÁúãÊõ¥Ê∏ÖÊô∞ÔºåÂç≥ Ê≤°Áî®ShrinkageÊó∂ÔºöÔºàyiË°®Á§∫Á¨¨iÊ£µÊ†ë‰∏äyÁöÑÈ¢ÑÊµãÂÄºÔºå y(1~i)Ë°®Á§∫ÂâçiÊ£µÊ†ëyÁöÑÁªºÂêàÈ¢ÑÊµãÂÄºÔºâ y(i+1) = ÊÆãÂ∑Æ(y1~yi)Ôºå ÂÖ∂‰∏≠Ôºö ÊÆãÂ∑Æ(y1~yi) = yÁúüÂÆûÂÄº ‚Äì y(1 ~ i) y(1 ~ i) = SUM(y1, ‚Ä¶, yi) Shrinkage‰∏çÊîπÂèòÁ¨¨‰∏Ä‰∏™ÊñπÁ®ãÔºåÂè™ÊääÁ¨¨‰∫å‰∏™ÊñπÁ®ãÊîπ‰∏∫Ôºö y(1 ~ i) = y(1 ~ i-1) + step * yi Âç≥Shrinkage‰ªçÁÑ∂‰ª•ÊÆãÂ∑Æ‰Ωú‰∏∫Â≠¶‰π†ÁõÆÊ†áÔºå‰ΩÜÂØπ‰∫éÊÆãÂ∑ÆÂ≠¶‰π†Âá∫Êù•ÁöÑÁªìÊûúÔºåÂè™Á¥ØÂä†‰∏ÄÂ∞èÈÉ®ÂàÜÔºàstep*ÊÆãÂ∑ÆÔºâÈÄêÊ≠•ÈÄºËøëÁõÆÊ†áÔºåstep‰∏ÄËà¨ÈÉΩÊØîËæÉÂ∞èÔºåÂ¶Ç0.01~0.001ÔºàÊ≥®ÊÑèËØ•stepÈùûgradientÁöÑstepÔºâÔºåÂØºËá¥ÂêÑ‰∏™Ê†ëÁöÑÊÆãÂ∑ÆÊòØÊ∏êÂèòÁöÑËÄå‰∏çÊòØÈô°ÂèòÁöÑ„ÄÇÁõ¥Ëßâ‰∏äËøô‰πüÂæàÂ•ΩÁêÜËß£Ôºå‰∏çÂÉèÁõ¥Êé•Áî®ÊÆãÂ∑Æ‰∏ÄÊ≠•‰øÆÂ§çËØØÂ∑ÆÔºåËÄåÊòØÂè™‰øÆÂ§ç‰∏ÄÁÇπÁÇπÔºåÂÖ∂ÂÆûÂ∞±ÊòØÊääÂ§ßÊ≠•ÂàáÊàê‰∫ÜÂæàÂ§öÂ∞èÊ≠•„ÄÇÊú¨Ë¥®‰∏äÔºåShrinkage‰∏∫ÊØèÊ£µÊ†ëËÆæÁΩÆ‰∫Ü‰∏Ä‰∏™weightÔºåÁ¥ØÂä†Êó∂Ë¶Å‰πò‰ª•Ëøô‰∏™weightÔºå‰ΩÜÂíåGradientÂπ∂Ê≤°ÊúâÂÖ≥Á≥ª„ÄÇËøô‰∏™weightÂ∞±ÊòØstep„ÄÇÂ∞±ÂÉèAdaboost‰∏ÄÊ†∑ÔºåShrinkageËÉΩÂáèÂ∞ëËøáÊãüÂêàÂèëÁîü‰πüÊòØÁªèÈ™åËØÅÊòéÁöÑÔºåÁõÆÂâçËøòÊ≤°ÊúâÁúãÂà∞‰ªéÁêÜËÆ∫ÁöÑËØÅÊòé„ÄÇ ‰∫î„ÄÅ GBDTÁöÑÈÄÇÁî®ËåÉÂõ¥ ËØ•ÁâàÊú¨GBDTÂá†‰πéÂèØÁî®‰∫éÊâÄÊúâÂõûÂΩíÈóÆÈ¢òÔºàÁ∫øÊÄß/ÈùûÁ∫øÊÄßÔºâÔºåÁõ∏ÂØπlogistic regression‰ªÖËÉΩÁî®‰∫éÁ∫øÊÄßÂõûÂΩíÔºåGBDTÁöÑÈÄÇÁî®Èù¢ÈùûÂ∏∏Âπø„ÄÇ‰∫¶ÂèØÁî®‰∫é‰∫åÂàÜÁ±ªÈóÆÈ¢òÔºàËÆæÂÆöÈòàÂÄºÔºåÂ§ß‰∫éÈòàÂÄº‰∏∫Ê≠£‰æãÔºåÂèç‰πã‰∏∫Ë¥ü‰æãÔºâ„ÄÇ ÂÖ≠„ÄÅ ÊêúÁ¥¢ÂºïÊìéÊéíÂ∫èÂ∫îÁî® RankNet ÊêúÁ¥¢ÊéíÂ∫èÂÖ≥Ê≥®ÂêÑ‰∏™docÁöÑÈ°∫Â∫èËÄå‰∏çÊòØÁªùÂØπÂÄºÔºåÊâÄ‰ª•ÈúÄË¶Å‰∏Ä‰∏™Êñ∞ÁöÑcost functionÔºåËÄåRankNetÂü∫Êú¨Â∞±ÊòØÂú®ÂÆö‰πâËøô‰∏™cost functionÔºåÂÆÉÂèØ‰ª•ÂÖºÂÆπ‰∏çÂêåÁöÑÁÆóÊ≥ïÔºàGBDT„ÄÅÁ•ûÁªèÁΩëÁªú‚Ä¶Ôºâ„ÄÇ ÂÆûÈôÖÁöÑÊêúÁ¥¢ÊéíÂ∫è‰ΩøÁî®ÁöÑÊòØLambdaMARTÁÆóÊ≥ïÔºåÂøÖÈ°ªÊåáÂá∫ÁöÑÊòØÁî±‰∫éËøôÈáåË¶Å‰ΩøÁî®ÊéíÂ∫èÈúÄË¶ÅÁöÑcost functionÔºåLambdaMARTËø≠‰ª£Áî®ÁöÑÂπ∂‰∏çÊòØÊÆãÂ∑Æ„ÄÇLambdaÂú®ËøôÈáåÂÖÖÂΩìÊõø‰ª£ÊÆãÂ∑ÆÁöÑËÆ°ÁÆóÊñπÊ≥ïÔºåÂÆÉ‰ΩøÁî®‰∫Ü‰∏ÄÁßçÁ±ª‰ººGradient*Ê≠•ÈïøÊ®°ÊãüÊÆãÂ∑ÆÁöÑÊñπÊ≥ï„ÄÇËøôÈáåÁöÑMARTÂú®Ê±ÇËß£ÊñπÊ≥ï‰∏äÂíå‰πãÂâçËØ¥ÁöÑÊÆãÂ∑ÆÁï•Êúâ‰∏çÂêåÔºåÂÖ∂Âå∫Âà´ÊèèËø∞ËßÅËøôÈáå„ÄÇ Â∞±ÂÉèÊâÄÊúâÁöÑÊú∫Âô®Â≠¶‰π†‰∏ÄÊ†∑ÔºåÊêúÁ¥¢ÊéíÂ∫èÁöÑÂ≠¶‰π†‰πüÈúÄË¶ÅËÆ≠ÁªÉÈõÜÔºåËøôÈáå‰∏ÄËà¨ÊòØÁî®‰∫∫Â∑•Ê†áÊ≥®ÂÆûÁé∞ÔºåÂç≥ÂØπÊØè‰∏Ä‰∏™(query,doc) pairÁªôÂÆö‰∏Ä‰∏™ÂàÜÂÄºÔºàÂ¶Ç1,2,3,4Ôºâ,ÂàÜÂÄºË∂äÈ´òË°®Á§∫Ë∂äÁõ∏ÂÖ≥ÔºåË∂äÂ∫îËØ•ÊéíÂà∞ÂâçÈù¢„ÄÇÁÑ∂ËÄåËøô‰∫õÁªùÂØπÁöÑÂàÜÂÄºÊú¨Ë∫´ÊÑè‰πâ‰∏çÂ§ßÔºå‰æãÂ¶Ç‰Ω†ÂæàÈöæËØ¥1ÂàÜÂíå2ÂàÜÊñáÊ°£ÁöÑÁõ∏ÂÖ≥Á®ãÂ∫¶Â∑ÆÂºÇÊòØ1ÂàÜÂíå3ÂàÜÊñáÊ°£Â∑ÆË∑ùÁöÑ‰∏ÄÂçä„ÄÇÁõ∏ÂÖ≥Â∫¶Êú¨Ë∫´Â∞±ÊòØ‰∏Ä‰∏™Âæà‰∏ªËßÇÁöÑËØÑÂà§ÔºåÊ†áÊ≥®‰∫∫ÂëòÊó†Ê≥ïÂÅöÂà∞ËøôÁßçÂÆöÈáèÊ†áÊ≥®ÔºåËøôÁßçÊ†áÂáÜ‰πüÊó†Ê≥ïÂà∂ÂÆö„ÄÇ‰ΩÜÊ†áÊ≥®‰∫∫ÂëòÂæàÂÆπÊòìÂÅöÂà∞ÁöÑÊòØ‚ÄùABÈÉΩ‰∏çÈîôÔºå‰ΩÜÊñáÊ°£AÊØîÊñáÊ°£BÊõ¥Áõ∏ÂÖ≥ÔºåÊâÄ‰ª•AÊòØ4ÂàÜÔºåBÊòØ3ÂàÜ‚Äú„ÄÇRankNetÂ∞±ÊòØÂü∫‰∫éÊ≠§Âà∂ÂÆö‰∫Ü‰∏Ä‰∏™Â≠¶‰π†ËØØÂ∑ÆË°°ÈáèÊñπÊ≥ïÔºåÂç≥cost function„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåRankNetÂØπ‰ªªÊÑè‰∏§‰∏™ÊñáÊ°£A,BÔºåÈÄöËøáÂÆÉ‰ª¨ÁöÑ‰∫∫Â∑•Ê†áÊ≥®ÂàÜÂ∑ÆÔºåÁî®sigmoidÂáΩÊï∞‰º∞ËÆ°‰∏§ËÄÖÈ°∫Â∫èÂíåÈÄÜÂ∫èÁöÑÊ¶ÇÁéáP1„ÄÇÁÑ∂ÂêéÂêåÁêÜÁî®Êú∫Âô®Â≠¶‰π†Âà∞ÁöÑÂàÜÂ∑ÆËÆ°ÁÆóÊ¶ÇÁéáP2ÔºàsigmoidÁöÑÂ•ΩÂ§ÑÂú®‰∫éÂÆÉÂÖÅËÆ∏Êú∫Âô®Â≠¶‰π†ÂæóÂà∞ÁöÑÂàÜÂÄºÊòØ‰ªªÊÑèÂÆûÊï∞ÂÄºÔºåÂè™Ë¶ÅÂÆÉ‰ª¨ÁöÑÂàÜÂ∑ÆÂíåÊ†áÂáÜÂàÜÁöÑÂàÜÂ∑Æ‰∏ÄËá¥ÔºåP2Â∞±Ë∂ãËøë‰∫éP1Ôºâ„ÄÇËøôÊó∂Âà©Áî®P1ÂíåP2Ê±ÇÁöÑ‰∏§ËÄÖÁöÑ‰∫§ÂèâÁÜµÔºåËØ•‰∫§ÂèâÁÜµÂ∞±ÊòØcost function„ÄÇÂÆÉË∂ä‰ΩéËØ¥ÊòéÊú∫Âô®Â≠¶ÂæóÁöÑÂΩìÂâçÊéíÂ∫èË∂äË∂ãËøë‰∫éÊ†áÊ≥®ÊéíÂ∫è„ÄÇ‰∏∫‰∫Ü‰ΩìÁé∞NDCGÁöÑ‰ΩúÁî®ÔºàNDCGÊòØÊêúÁ¥¢ÊéíÂ∫è‰∏öÁïåÊúÄÂ∏∏Áî®ÁöÑËØÑÂà§Ê†áÂáÜÔºâÔºåRankNetËøòÂú®cost function‰∏≠‰πò‰ª•‰∫ÜNDCG„ÄÇ Â•ΩÔºåÁé∞Âú®Êàë‰ª¨Êúâ‰∫Ücost functionÔºåËÄå‰∏îÂÆÉÊòØÂíåÂêÑ‰∏™ÊñáÊ°£ÁöÑÂΩìÂâçÂàÜÂÄºyiÁõ∏ÂÖ≥ÁöÑÔºåÈÇ£‰πàËôΩÁÑ∂Êàë‰ª¨‰∏çÁü•ÈÅìÂÆÉÁöÑÂÖ®Â±ÄÊúÄ‰ºòÊñπÂêëÔºå‰ΩÜÂèØ‰ª•Ê±ÇÂØºÊ±ÇGradientÔºåGradientÂç≥ÊØè‰∏™ÊñáÊ°£ÂæóÂàÜÁöÑ‰∏Ä‰∏™‰∏ãÈôçÊñπÂêëÁªÑÊàêÁöÑNÁª¥ÂêëÈáèÔºåN‰∏∫ÊñáÊ°£‰∏™Êï∞ÔºàÂ∫îËØ•ËØ¥ÊòØquery-doc pair‰∏™Êï∞Ôºâ„ÄÇËøôÈáå‰ªÖ‰ªÖÊòØÊää‚ÄùÊ±ÇÊÆãÂ∑Æ‚ÄúÁöÑÈÄªËæëÊõøÊç¢‰∏∫‚ÄùÊ±ÇÊ¢ØÂ∫¶‚ÄúÔºåÂèØ‰ª•ËøôÊ†∑ÊÉ≥ÔºöÊ¢ØÂ∫¶ÊñπÂêë‰∏∫ÊØè‰∏ÄÊ≠•ÊúÄ‰ºòÊñπÂêëÔºåÁ¥ØÂä†ÁöÑÊ≠•Êï∞Â§ö‰∫ÜÔºåÊÄªËÉΩËµ∞Âà∞Â±ÄÈÉ®ÊúÄ‰ºòÁÇπÔºåËã•ËØ•ÁÇπÊÅ∞Â•Ω‰∏∫ÂÖ®Â±ÄÊúÄ‰ºòÁÇπÔºåÈÇ£ÂíåÁî®ÊÆãÂ∑ÆÁöÑÊïàÊûúÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇËøôÊó∂Â•óÂà∞‰πãÂâçËÆ≤ÁöÑÈÄªËæëÔºåGDBTÂ∞±Â∑≤ÁªèÂèØ‰ª•‰∏ä‰∫Ü„ÄÇÈÇ£‰πàÊúÄÁªàÊéíÂ∫èÊÄé‰πà‰∫ßÁîüÂë¢ÔºüÂæàÁÆÄÂçïÔºåÊØè‰∏™Ê†∑Êú¨ÈÄöËøáShrinkageÁ¥ØÂä†ÈÉΩ‰ºöÂæóÂà∞‰∏Ä‰∏™ÊúÄÁªàÂæóÂàÜÔºåÁõ¥Êé•ÊåâÂàÜÊï∞‰ªéÂ§ßÂà∞Â∞èÊéíÂ∫èÂ∞±ÂèØ‰ª•‰∫ÜÔºàÂõ†‰∏∫Êú∫Âô®Â≠¶‰π†‰∫ßÁîüÁöÑÊòØÂÆûÊï∞ÂüüÁöÑÈ¢ÑÊµãÂàÜÔºåÊûÅÂ∞ë‰ºöÂá∫Áé∞Âú®‰∫∫Â∑•Ê†áÊ≥®‰∏≠Â∏∏ËßÅÁöÑ‰∏§ÊñáÊ°£ÂàÜÊï∞Áõ∏Á≠âÁöÑÊÉÖÂÜµÔºåÂá†‰πé‰∏çÂêåËÄÉËôëÂêåÂàÜÊñáÊ°£ÁöÑÊéíÂ∫èÊñπÂºèÔºâ Âè¶Â§ñÔºåÂ¶ÇÊûúfeature‰∏™Êï∞Â§™Â§öÔºåÊØè‰∏ÄÊ£µÂõûÂΩíÊ†ëÈÉΩË¶ÅËÄóË¥πÂ§ßÈáèÊó∂Èó¥ÔºåËøôÊó∂ÊØè‰∏™ÂàÜÊîØÊó∂ÂèØ‰ª•ÈöèÊú∫ÊäΩ‰∏ÄÈÉ®ÂàÜfeatureÊù•ÈÅçÂéÜÊ±ÇÊúÄ‰ºòÔºàELFÊ∫êÁ†ÅÂÆûÁé∞ÊñπÂºèÔºâ„ÄÇ]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[‰ªéÊçüÂ§±ÂáΩÊï∞Áúã-xgboost„ÄÅGBDT„ÄÅadaboost]]></title>
    <url>%2F2018%2F06%2F05%2F%E4%BB%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9C%8B-xgboost%E3%80%81GBDT%E3%80%81adaboost%2F</url>
    <content type="text"><![CDATA[adaboostÂíåGBDTÂíåxgboostÂú®ÊçüÂ§±ÂáΩÊï∞ÁöÑÊúÄ‰ºòÂåñÊñπÊ≥ïÊòØÊúâÂæàÂ§ö‰∏çÂêåÁöÑÔºå‰∏âËÄÖÁöÑ‰∏çÂêå‰πãÂ§ÑÂÖ∂ÂÆûÂ∞±Âú®‰∫éÊúÄ‰ºòÂåñÊñπÊ≥ïÁöÑ‰∏çÂêåÔºàËøôÊ†∑ËØ¥‰∏çÁü•ÈÅìÊòØÂê¶Â¶•ÂΩìÔºåËá≥Â∞ëÁ´ôÂú®Ëøô‰∏™ËßíÂ∫¶ÁöÑÊàëËÆ§‰∏∫ÊòØÊ≠£Á°ÆÁöÑÔºåÂ§öÂπ¥ÂêéÂèØËÉΩÂèëÁé∞Ëøô‰∏™ËßÇÁÇπ‰∏çÂ§™Â¶•ÂΩìÔºâ„ÄÇadaboostÂú®ÊùéËà™ÂçöÂ£´ÁöÑ„ÄäÁªüËÆ°Â≠¶‰π†Âü∫Á°Ä„ÄãÈáåÈù¢Áî®Âä†Ê≥ïÊ®°ÂûãÂíåÂêëÂâçÁÆóÊ≥ïËß£Èáä‰∫ÜÊùÉÂÄºÊõ¥Êñ∞Á≠ñÁï•„ÄÇÂú®Ëß£ÈáäÁöÑËøáÁ®ã‰∏≠ÔºåÊ†∑Êú¨ÊùÉÂÄºÊõ¥Êñ∞ÂíåÂº±ÂàÜÁ±ªÂô®ÊùÉÂÄºÁöÑÊ±ÇÂèñÊòØÁõ¥Êé•ÈÄöËøáÂÅèÂØºÊï∞Á≠â‰∫éÈõ∂Êù•ËÆ°ÁÆóÁöÑÔºåÂ¶ÇÊûúËÆ∞‰∏çÊ∏ÖÊ•öÁöÑÂèØ‰ª•ÂõûÂéª‰ªîÁªÜÁúã‰∏Ä‰∏ã„ÄÇ‰∏çÁÆ°ÊòØÂÅöÂõûÂΩíËøòÊòØÂÅöÂàÜÁ±ªÔºåadaboostÁöÑÁõÆÊ†áÈÉΩÊòØÊâæÂà∞‰∏Ä‰∏™ÂÄºÔºàÈÄöËøá‰ΩøÂæóÂÅèÂØºÊï∞Á≠âÈõ∂ÁöÑÊñπÊ≥ïÔºâÁõ¥Êé•‰ΩøÂæóÊçüÂ§±ÂáΩÊï∞Èôç‰ΩéÂà∞ÊúÄÂ∞è„ÄÇ ËÄåGBDT‰ΩøÁî®ÁöÑÊòØÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ï‰ΩøÂæóÊçüÂ§±ÂáΩÊï∞ÊúâÊâÄÈôç‰Ωé„ÄÇÈ¶ñÂÖàËØ¥‰∏Ä‰∏ã‰∏∫‰ªÄ‰πà‰ºöÊèêÂá∫ËøôÁßçÊñπÊ≥ïÔºåÁÑ∂ÂêéËØ¥‰∏Ä‰∏ã‰∏∫‰ªÄ‰πàGBDTÊòØ‰ΩøÁî®Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÂ∞ÜÊçüÂ§±ÂáΩÊï∞ÊúâÊâÄÈôç‰Ωé„ÄÇËøôÁßçÊñπÊ≥ïÁöÑÂá∫Áé∞ËÇØÂÆöÊòØÂõ†‰∏∫ÂØπ‰∫éÊüê‰∫õÊçüÂ§±ÂáΩÊï∞ÔºåÂÅèÂØºÊï∞Á≠â‰∫éÈõ∂ÊòØÂèØËß£ÁöÑÔºåËÄåÂØπ‰∫é‰∏ÄËà¨ÁöÑÊçüÂ§±ÂáΩÊï∞ÂÅèÂØºÊï∞Á≠â‰∫éÈõ∂ÊòØÊó†Ê≥ïÊ±ÇËß£ÁöÑÔºå‰∏çÁÑ∂Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ï‰πü‰∏ç‰ºöÂ≠òÂú®‰∫Ü„ÄÇËÄåËá≥‰∫é‰∏∫‰ªÄ‰πàGBDT‰ΩøÁî®ÁöÑÊòØÊ¢ØÂ∫¶‰∏ãÈôçÊ≥ïÂ∞ÜÊçüÂ§±ÂáΩÊï∞Èôç‰ΩéÁöÑ„ÄÇÈ¶ñÂÖàÂ∞ÜÊçüÂ§±ÂáΩÊï∞Âú®ft-1ÔºàxÔºâÂ§ÑÂÅö‰∏ÄÈò∂Ê≥∞ÂãíÂ±ïÂºÄÔºàËøôÈáåÁöÑft-1ÔºàxÔºâÂ∞±ÊòØÂâçt-1‰∏™Âº±ÂàÜÁ±ªÂô®ÁöÑÊï¥ÂêàÔºâÔºåÂ±ïÂºÄ‰πãÂêéÊòØ‰∏Ä‰∏™Â∏∏Êï∞C Âíå Ê¢ØÂ∫¶gÂíåÊñ∞ÁöÑÂº±ÂàÜÁ±ªÂô®fÁöÑ‰πòÁßØÔºàÂ∞±ÊòØL = C + gfÔºâÔºåËøôÊó∂ÂÄôÂ¶ÇÊûúË¶ÅËÆ©ÊçüÂ§±ÂáΩÊï∞LÂèòÂæóÊõ¥Â∞èÔºåÂ∞±Ë¶ÅËÆ©Ê¢ØÂ∫¶gÂíåÂº±ÂàÜÁ±ªÂô®fÁöÑ‰πòÁßØÂèòÂæóÊõ¥Â∞èÔºåÊâÄ‰ª•Â∞±ÊòØÁî®Ë¥üÊ¢ØÂ∫¶Êù•ÊãüÂêàÊñ∞ÁöÑÂº±ÂàÜÁ±ªÂô®„ÄÇ Â¶ÇÊûúÊääÂº±ÂàÜÁ±ªÂô®ÂΩìÂÅöÊ≠•ÈïøÔºåÈÇ£‰πàÊçüÂ§±ÂáΩÊï∞Â∞±‰ª£Ë°®Ê≤øÁùÄÊ¢ØÂ∫¶Ëµ∞‰∫Ü‰∏ÄÊ≠•„ÄÇÂ¶ÇÊûúÊñ∞ÁöÑÂº±ÂàÜÁ±ªÂô®Ë∑üË¥üÊ¢ØÂ∫¶ÂÆåÁæéÊãüÂêàÔºåÈÇ£‰πàÊçüÂ§±ÂáΩÊï∞Â∞±ÂèòÊàê‰∫ÜL = C + gÔºà-fÔºâ= C-ggÔºåÊâÄ‰ª•ËøôÊó∂ÂÄôÊçüÂ§±ÂáΩÊï∞ÊòØ‰∏ãÈôç‰∫ÜÁöÑ„ÄÇÂ¶ÇÊûúÊòØËøôÊ†∑ÁöÑËØùÔºåÂè™Ë¶Å‰øùËØÅÊñ∞ÁöÑÂº±ÂàÜÁ±ªÂô®fÁöÑÂêëÈáèÊñπÂêëÂíåÊ¢ØÂ∫¶gÁõ∏ÂèçÔºåËÄåÂêëÈáèÊ®°ÈïøÂ∞ΩÂèØËÉΩÂ§ß‰∏çÂ∞±Ë°å‰∫ÜÂêóÔºüÁ≠îÊ°àÂΩìÁÑ∂ÊòØ‰∏çË°åÁöÑÔºåÂõ†‰∏∫Êàë‰ª¨ÂÅöÁöÑÊòØÊçüÂ§±ÂáΩÊï∞Âú®ft-1ÔºàxÔºâÁöÑÊ≥∞ÂãíÂ±ïÂºÄÂïäÔºåÂ¶ÇÊûúÊ≠•ÈïøÂ§™ÈïøÁöÑËØùÊ≤øÁùÄÊ¢ØÂ∫¶Áõ∏ÂèçÁöÑÊñπÂêëËµ∞‰∏ÄÊ≠•ÔºåËØØÂ∑ÆÂ∞±Â§ß‰∫Ü„ÄÇÊâÄ‰ª•Ê≠•Èïø‰πüË¶ÅÂ∞è‰∏ÄÁÇπ„ÄÇÂú®Ëøô‰∏çÁü•ÈÅì‰∏∫‰ªÄ‰πàÂÆö‰πâÊ≠•ÈïøfÁöÑÊ®°ÂíåÊ¢ØÂ∫¶ÁöÑÊ®°Áõ∏Á≠âÔºåÂèØËÉΩFreidmanÊúâÊâÄËÄÉËôëÁöÑÂêß,Â¶ÇÊûúÂêéÁª≠ËÆ©ÊàëÊù•ÊîπËøõÔºåÊàëÂ∞±‰ªéËØ•Âú∞ÊñπÂÖ•ÊâãÔºåÈÄâÊã©Â§öÈïøÁöÑÊ≠•Èïø‰πüÂ∞±ÊòØ-a*gÊù•ÊãüÂêàÊñ∞ÁöÑÂº±ÂàÜÁ±ªÂô®fÔºåÂÖ∂‰∏≠a&gt;0„ÄÇ xgboostÊçüÂ§±ÂáΩÊï∞ÂêåÊ†∑ÊòØÂú®ft-1ÔºàxÔºâÂ§ÑÂÅöÊ≥∞ÂãíÂ±ïÂºÄÔºå‰∏çÂêåÁöÑÊòØÂÅöÁöÑ‰∫åÈò∂Ê≥∞ÂãíÂ±ïÂºÄÔºåËÄå‰∏îËøòÈôÑÂ∏¶‰∫ÜÂØπÊ†ëÂè∂Â≠êËäÇÁÇπÁ∫¶Êùü„ÄÅÊùÉÂÄºÊ®°ÈïøÔºàÂÖ∂ÂÆûÂèØ‰ª•ËÄÉËôë‰∏∫Ê≠•ÈïøÔºâÁ∫¶ÊùüÁöÑÊ≠£ÂàôÈ°π„ÄÇÂΩìÁÑ∂Âú®GBDTÈáåÈù¢‰∏ÄÊ†∑ÂèØ‰ª•ÂÅöËøô‰∫õÊ≠£ÂàôÈ°πÁ∫¶ÊùüÔºåËøôÈáåÈù¢Âπ∂‰∏çÊòØxgboostÁâπÊúâÁöÑÔºå‰ΩÜÊòØ‰∏çÂêåÁöÑÂú∞ÊñπÂ∞±ÊòØxgboostÊçüÂ§±ÂáΩÊï∞Âú®ft-1ÔºàxÔºâÂ§ÑÂÅö‰∫åÈò∂Ê≥∞ÂãíÂ±ïÂºÄ„ÄÇÂØπÂ∞±ÊòØ‰ªé‰∫åÈò∂Ê≥∞ÂãíÂ±ïÂºÄÂÖ•ÊâãÂéªÊúÄ‰ºòÂåñÊçüÂ§±ÂáΩÊï∞ÁöÑ„ÄÇ]]></content>
      <categories>
        <category>Êú∫Âô®Â≠¶‰π†</category>
      </categories>
      <tags>
        <tag>Êú∫Âô®Â≠¶‰π†</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo ÂçöÂÆ¢ÈÖçÁΩÆ]]></title>
    <url>%2F2018%2F06%2F04%2Fhexo%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Êú¨ÂçöÂÆ¢ÈÄâÁî®‰∫Ü hexo ‰∏≠ Next‰∏ªÈ¢ò„ÄÇÂú® Next ‰∏ªÈ¢ò‰∏≠ÔºåÈááÁî®‰∫Üfont awesome Â≠ó‰ΩìÂ∫ì„ÄÇ Áõ∏ÂÖ≥ÁöÑÂØπÁÖßË°®ÔºåÂ¶Çfont awesomeÊâÄÁ§∫„ÄÇ Âú®ÂçöÂÆ¢‰∏≠ËÆæÁΩÆÂàÜÁ±ªÂíåÊ†áÁ≠æÊòØÈùûÂ∏∏ÈáçË¶ÅÁöÑÊìç‰Ωú„ÄÇ È¶ñÂÖàÊñ∞Âª∫‰∏Ä‰∏™ tags È°µÈù¢ 12$ cd your-hexo-site$ hexo new page tags ËøõÂÖ•ÂàöÂàöÁîüÊàêÁöÑÈ°µÈù¢Ôºå‰∏ÄËà¨‰∏∫ index Êñá‰ª∂Ôºå‰øÆÊîπ type Á±ªÂûã‰∏∫tags Âç≥ÂèØ„ÄÇÂ¶Ç‰∏ãÔºö 1234title: Ê†áÁ≠ædate: 2014-12-22 12:39:04type: "tags"--- ÂØπ‰∫éÂàÜÁ±ª categories ÂêåÁêÜÔºåÂè™ÈúÄË¶ÅÂ∞Ü type Á±ªÂûãÊîπ‰∏∫ categories Âç≥ÂèØ„ÄÇ‰ΩÜÊòØÈúÄË¶ÅÊ≥®ÊÑè‰∏ÄÁÇπÔºåÂØπ‰∫é tags Ê≤°ÊúâÂ±ÇÁ∫ßÂå∫ÂàÜ„ÄÇËÄå categories ÂàôÊúâÁùÄ‰∏•Ê†ºÁöÑÂÖàÂêéÁ∫ßÂÖ≥Á≥ª„ÄÇ ÂèØ‰ª•‰øÆÊîπ /scaffolds/post.md Ê®°ÊùøÔºåÂä†ÂÖ• tags„ÄÅ categories ÁöÑÊ†áÁ≠æ„ÄÇÂ¶Ç‰∏ãÔºö 123456---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags:categories:--- Âú® hexo ‰∏≠ËÆæÁΩÆÈ¶ñÈ°µÂçïÈ°µÊñáÁ´†Êï∞ÁõÆÔºö 12345678910Âú®Á´ôÁÇπÈÖçÁΩÆÊñá‰ª∂‰∏≠Ôºå‰øÆÊîπ per_page ÁöÑÊï∞ÁõÆ# Home page setting# path: Root path for your blogs index page. (default = '')# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: '' per_page: 5 order_by: -date Âú® hexo ‰∏≠ÂºÄÂêØÊêúÁ¥¢ÈÄâÈ°πÊñπÊ≥ï 1ÔºâÈ¶ñÂÖàÂÆâË£ÖÊèí‰ª∂ 12345Âú®Á´ôÁÇπÁöÑÊ†πÁõÆÂΩï‰∏ãÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºönpm install hexo-generator-search --savenpm install hexo-generator-searchdb --save 2ÔºâÂêØÁî®ÊêúÁ¥¢ ‰øÆÊîπÁ´ôÁÇπÈÖçÁΩÆÊñá‰ª∂ 1234567ÁºñËæë Á´ôÁÇπÈÖçÁΩÆÊñá‰ª∂ÔºåÊñ∞Â¢û‰ª•‰∏ãÂÜÖÂÆπÂà∞‰ªªÊÑè‰ΩçÁΩÆÔºösearch: path: search.xml field: post format: html limit: 10000 ‰øÆÊîπ‰∏ªÈ¢òÈÖçÁΩÆÊñá‰ª∂ 1234ÊàëÁöÑË∑ØÂæÑÔºö/blog/themes/next‰∏ãÁöÑ_config.ymlÊñá‰ª∂ÔºåËøõË°åÁºñËæë„ÄÇlocal_search: enable: true]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu12.04 Hadoop+Spark ÂàÜÂ∏ÉÂºèÈÉ®ÁΩ≤Ê≠•È™§]]></title>
    <url>%2F2018%2F06%2F04%2Fubuntu12-04-Hadoop-Spark-%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[1„ÄÅÂáÜÂ§áÂ∑•ÂÖ∑Ôºö ËôöÊãüÊú∫Ôºövmware Êìç‰ΩúÁ≥ªÁªü Ôºöubuntu12.04 ÔºàÊ°åÈù¢ÁâàÊàñserverÁâàÔºâ(isoÊñá‰ª∂) javaÔºöjdk-7u75-linux-x64.tar.gz oracleÂÆòÁΩëÂèØ‰∏ãËΩΩ hadoopÔºöhaopop2.5.2 spark: spark1.1.1 ÁôªÈôÜÁªàÁ´Ø secure crt Êñá‰ª∂‰º†ËæìÔºöfile zilla 2„ÄÅÂÖ∑‰ΩìÊ≠•È™§ Ôºà1Ôºâ„ÄÅÂÆâË£Övmware ‚ÄîËøáÁ®ãÁï• Ôºà2Ôºâ„ÄÅÊñ∞Âª∫ËôöÊãüÊú∫Ôºà3‰∏™ÔºâÂÆâË£ÖubuntuÁ≥ªÁªü ‚ÄîËøáÁ®ãÁï• Ê≥®Ôºö‰∏ãÈù¢ÁöÑÊ≠•È™§Êó†ÁâπÊÆäËØ¥ÊòéÂàôÈúÄË¶ÅÂú®‰∏â‰∏™ËôöÊãüÊú∫‰∏äÂêåÊó∂ÊâßË°å Ôºà3Ôºâ„ÄÅapt-get Êõ¥Êñ∞Ôºö apt-get update Ôºà4Ôºâ„ÄÅÂÆâË£Övim opensshÂÆ¢Êà∑Á´ØÔºåÊúçÂä°Á´Ø apt-get instsall vim openssh-client openssh-server Âà§Êñ≠sshÊòØÂê¶Ê≠£Â∏∏ÂêØÂä® ÂëΩ‰ª§Ôºöps -e |grep ssh Â¶ÇÊûúÊó¢Êúâsshd ÂèàÊúâssh-agent ÂàôsshÂêØÂä®ÊàêÂäü Ôºà5Ôºâ„ÄÅÂÆâË£ÖjavaÁéØÂ¢É ÂÅáÂÆöÂáÜÂ§áÂ∞ÜjavaÂÆâË£ÖÂú®/usr/lib/java‰∏≠ ÂàôÂª∫Á´ãËØ•ÁõÆÂΩïÔºömkdir /usr/lib/java ÈÄöËøáfile zilla Â∞ÜÂéãÁº©Êñá‰ª∂‰º†ÈÄÅÂà∞ËôöÊãüÊú∫‰∏ä Ëß£Âéãjdk tar -xzvf jdkxxxxxx ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáèÔºö ÁºñËæë /etc/profileÊñá‰ª∂ÔºåÂú®ÂÖ∂‰∏≠Âä†ÂÖ•‰ª•‰∏ãÂÜÖÂÆπ export JAVA_HOME=/usr/lib/java/jdk1.7.0_75 export JRE_HOME={$JAVA_HOME}/jre ‰øùÂ≠òÈÄÄÂá∫ ‰ª§ÂÖ∂ÁîüÊïà source /etc/profile Ê≠§Êó∂Êü•ÁúãjavaÁâàÊú¨ java -version Â¶ÇÊûúÁ≥ªÁªü‰∏≠ÊúâÂ§ö‰∏™javaÁéØÂ¢ÉÂàôÈúÄË¶ÅÊâãÂä®ËÆæÁΩÆÈªòËÆ§ÁöÑjdk ÂëΩ‰ª§Â¶Ç‰∏ãÔºö sudo update-alternatives ‚Äìinstall /usr/bin/javac javac /usr/lib/java/jdk1.7.0_75/bin/javac 300 sudo update-alternatives ‚Äìinstall /usr/bin/java java /usr/lib/java/jdk1.7.0_75/bin/java 300 sudo update-alternatives ‚Äìinstall /usr/bin/jps jps /usr/lib/java/jdk1.7.0_75/bin/jps 300 Ê≠§Êó∂Âú®ËæìÂÖ•java -versionÂ∞±ÂèØ‰ª•ÁúãÂà∞‰∏äÂõæÁöÑÁªìÊûú‰∫Ü Ôºà6Ôºâ„ÄÅ‰øÆÊîπhostname+ÈÖçÁΩÆhosts ‰øÆÊîπhostname:ÁºñËæë/etc/hostnameÊñá‰ª∂ÂàÜÂà´Â∞Ü‰∏âÂè∞Êú∫Âô®ÁöÑhostnameÊîπÊàêmaster slave1 slave2 ‰πãÂêéÈáçÂêØÊúçÂä°Âô® ÈÖçÁΩÆhostsÔºöÁºñËæë/etc/hostsÔºåÊ∑ªÂä† ip ÂíåÂØπÂ∫îÁöÑhostname„ÄÇ ‰øùÂ≠òÈÄÄÂá∫‰πãÂêéÈ™åËØÅ‰∏âÂè∞Êú∫Âô®Áõ¥Êé•ping ÂêÑËá™ÁöÑhostnameÊòØÂê¶‰∫íÈÄö Ôºà7Ôºâ„ÄÅÈÖçÁΩÆssh hadoopÊòØÈÄöËøásshÊù•ËøõË°åËäÇÁÇπÈó¥ÁöÑÈÄö‰ø°ÔºåÊâÄ‰ª•Ë¶ÅÂéªÊéâËäÇÁÇπÈó¥sshÁöÑÂØÜÁ†ÅÔºåÊâÄ‰ª•ÈúÄË¶ÅÂØπsshËøõË°åÈÖçÁΩÆ aÔºâ„ÄÅÁîüÊàêÂÖ¨Èí•ÔºöÂëΩ‰ª§Ôºössh-keygen -t rsa -P ‚Äú‚Äù bÔºâ„ÄÅÂ∞ÜmasterÂÖ¨Èí•‰øùÂ≠òÂú®authorized_keys‰∏ã cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys ÂØπ‰∫éÊú¨Êú∫ÂèØÁî®ssh localhostÊù•È™åËØÅÊòØÂê¶ËÆæÁΩÆÊàêÂäü cÔºâ„ÄÅÂ∞ÜÂè¶Â§ñ‰∏§Âè∞Êú∫Âô®ÁöÑÂÖ¨Èí•ÁîüÊàêÂêéÈÄöËøáscpÂëΩ‰ª§Ê±áËÅöÂà∞master‰∏ãÂëΩ‰ª§Â¶Ç‰∏ãÔºöÂú®ÁõÆÂΩï/root/.ssh‰∏≠ scp id_rsa.pub root@master:/root/.ssh/id_rsa.pub.slave1(2) dÔºâ„ÄÅÂ∞ÜÊâÄÊúâÂÖ¨Èí•ÂÖ®ÈÉ®‰øùÂ≠òÂú®authorized_keys‰∏ãÂëΩ‰ª§Â¶Çb eÔºâ„ÄÅÂ∞ÜËÆæÁΩÆÂ•ΩÁöÑauthorized_keysÊñá‰ª∂ÂèëÈÄÅÁªô‰∏§‰∏™slaveËäÇÁÇπÔºå‰πãÂêéÈ™åËØÅ‰∏âÂè∞Êú∫Âô®ÊòØÂê¶ÂèØ‰ª•sshÊó†ÂØÜÁ†ÅÁôªÈôÜ Ôºà8Ôºâ„ÄÅÂÆâË£Öhadoop Ëß£Âéã‰πãÂâçÁöÑÂéãÁº©Êñá‰ª∂Ôºàtar -zxvfÔºâ ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáèÔºöÂú®/etc/profile‰∏≠Ê∑ªÂä†Áõ∏Â∫îÂèòÈáè export HADOOP_COMMON_HOME=/usr/local/hadoopspark/hadoop-2.5.2 export HADOOP_CONF_DIR=/usr/local/hadoopspark/hadoop-2.5.2/etc/hadoop export YARN_CONF_DIR=/usr/local/hadoopspark/hadoop-2.5.2/etc/hadoop export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_COMMON_HOME/lib/native export HADOOP_OPTS=‚Äù-Djava.library.path=$HADOOP_COMMON_HOME/lib‚Äù export CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar export PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin:${HADOOP_COMMON_HOME}/bin:${HADOOP_COMMON_HOME}/sbin:$PATH ‰øùÂ≠òÂπ∂‰ΩøÂÖ∂ÁîüÊïà„ÄÇ ËÆæÁΩÆhadoop-env.shÔºàÊñá‰ª∂‰Ωç‰∫éhadoopÂΩíÊ°£ÂêéÁöÑÁõÆÂΩïÁöÑetc/hadoop/‰∏≠ÔºâÔºö Âä†ÂÖ•javaÁéØÂ¢ÉÂèòÈáè export JAVA_HOME=/usr/lib/java/jdk1.7.0_75 Ê≠§Êó∂ÂèØÈ™åËØÅhadoopÊòØÂê¶ÂÆâË£ÖÊàêÂäüÔºöhadoop version Ëã•ËÉΩÂá∫Áé∞hadoop‰ø°ÊÅØËØ¥ÊòéÂÆâË£ÖÂü∫Êú¨ÊàêÂäü Ôºà9Ôºâ„ÄÅÈÖçÁΩÆhadoop‰ø°ÊÅØÔºö ‰∏ªË¶ÅÈÖçÁΩÆ‰∏Ä‰∏ãÂá†‰∏™Êñá‰ª∂ ÈÖçÁΩÆ‰πãÂâçÂÖàÂª∫Á´ãÂá†se‰∏™ÁõÆÂΩïÁî®Êù•‰øùÂ≠òhadoopÊâßË°åËøáÁ®ã‰∏≠ÁöÑÁõ∏ÂÖ≥‰ø°ÊÅØÔºömkdir tmp hdfs hdfs/name hdfs/data 1Ôºâ„ÄÅetc/hadoop/core-site.xml 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/hadoop-2.7.0/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2)„ÄÅetc/hadoop/mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3Ôºâ„ÄÅetc/hadoop/hdfs-site.xml 1234567891011121314151617181920&lt;value&gt;Master:50090&lt;/value&gt;ÊòØÁΩëÈ°µËÆøÈóÆÂú∞ÂùÄ/home/hadoop/tools/hadoop-2.7.0/&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;Master:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/hadoop-2.7.0/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/hadoop-2.7.0/hdfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4Ôºâ„ÄÅetc/hadoop/yarn-site.xml 12345678910111213141516171819202122232425262728&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;Master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;Master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;Master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;Master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;Master:8088&lt;/value&gt; &lt;/property&gt;&lt;!-- Site specific YARN configuration properties --&gt;&lt;/configuration&gt; Ôºà5Ôºâ„ÄÅetc/hadoop/slaves Ê≥®ÊÑèË¶ÅÂà†Èô§Ëá™Â∑±Êú¨Ë∫´ÁöÑËäÇÁÇπÔºåÊàñËÄÖÂú®ÈÖçÁΩÆ‰∫í‰ø°Êó∂ÔºåÂä†ÂÖ•Ëá™Â∑±ÁöÑÂØÜÈí•„ÄÇ ÈÖçÁΩÆÂÆåÊØïÂàôÂèØ‰ª•ÂáÜÂ§á0ÂêØÂä®hadoop ‰∏ãÈù¢ÁöÑËøáÁ®ãÂú®masterËäÇÁÇπ‰∏äÊâßË°åÁõ∏ÂÖ≥Êìç‰Ωú namenodeÁöÑÊ†ºÂºèÂåñÔºöhadoop namenode -format ‰πãÂêéÂà∞hadoopÊâÄÂú®ÁõÆÂΩïÂêØÂä®hadoopÔºösbin/start-all.sh ‰∏ÄÂ∞èÊÆµÁ≠âÂæÖÂêéhadoopÂêØÂä®ÔºåÁî®jpsÊü•Áúã 123456DataNodeJpsSecondaryNameNodeNameNodeNodeManagerResourceManager Ëã•Âá∫Áé∞‰∏ãÈù¢‰ø°ÊÅØÂàôÂêØÂä®ÊàêÂäüÔºåÂèØÈÄöËøáweb-browserÊü•ÁúãÁõ∏ÂÖ≥‰ø°ÊÅØurl:http://masterip:50070 ÂÖ≥Èó≠hadoopÊúçÂä°Ôºösbin/stop-all-sh Ëã•ÈúÄË¶ÅÈáçÊñ∞Ê†ºÂºèÂåñnamenodeÂàôÈúÄË¶ÅÂà†Èô§Êéâtmp Âíå/tmp/hadoop* Âíåhdfs/name hdfs/data‰∏≠ÁöÑÂÜÖÂÆπ Ëá≥Ê≠§hadoop ÂàÜÂ∏ÉÂºèÈÉ®ÁΩ≤ÊàêÂäüÔºå‰∏ãÈù¢ÂáÜÂ§áÂú®Ê≠§Âü∫Á°Ä‰∏äÂÆâË£ÖsparkÔºàÁî±‰∫ésparkÁöÑÂ∑•‰ΩúÈúÄË¶ÅÂü∫‰∫éhadoop‰∏≠ÁöÑhdfsÂàÜÂ∏ÉÂºèÊñá‰ª∂Á≥ªÁªüÔºâ Ôºà10Ôºâ„ÄÅÂÆâË£Öscala Ëß£Âéã‰∏ãËΩΩÂ•ΩÁöÑÂéãÁº©Êñá‰ª∂Ôºàtar -zxvfÔºâ ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáèÔºà/etc/profileÔºâ export SCALA_HOME=/usr/lib/scala/scala-2.11.4 È™åËØÅÔºöÈîÆÂÖ•scalaÔºöÂá∫Áé∞scalaÂëΩ‰ª§ÊéßÂà∂Ë°åËØ¥ÊòéÂÆâË£ÖÊàêÂäü Ôºà11Ôºâ„ÄÅÂÆâË£Öspark Ëß£ÂéãÊñá‰ª∂‚ÄìËøáÁ®ãÁï• ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáèÔºöÁºñËæëconf‰∏ãÁöÑspark-env.shÊñá‰ª∂Â∞Üjava scalaÁõ∏ÂÖ≥ÁéØÂ¢ÉÂèòÈáèÂä†ÂÖ•Âπ∂‰∏îÈÖçÁΩÆmasterip‰ª•ÂèäÂêÑ‰∏™workerËäÇÁÇπÁöÑÂÜÖÂ≠ò export JAVA_HOME=/usr/lib/java/jdk1.7.0_75 export SCALA_HOME=/usr/lib/scala/scala-2.11.4 export SPARK_MASTER_IP=192.168.194.129 export SPARK_WORKER_MEMORY=2g export SPARK_CONF_DIR=/usr/local/hadoopspark/spark-1.1.1-bin-hadoop2.4/conf ÈÖçÁΩÆÂêåÁõÆÂΩï‰∏ãÁöÑslaves Â∞ÜÊâÄÊúâËäÇÁÇπÁöÑhostnameÊ∑ªÂä†ËøõËØ•Êñá‰ª∂Ôºà‰∏âÂè∞Êú∫Âô®ÂùáÈÖçÁΩÆÔºâ ÈÖçÁΩÆÂÆåÊØïÂêØÂä®spark sbin/start-all.sh jpsÂëΩ‰ª§Êü•ÁúãËã•Êñ∞Â¢ûËøô‰∏§‰∏™ËøõÁ®ãÂç≥sparkÊàêÂäüÂêØÂä®ÔºàMaster WorkerÔºâ Ëá≥Ê≠§spark+hadoopÁöÑÂàÜÂ∏ÉÂºèÈÉ®ÁΩ≤ÂÆåÊàê]]></content>
      <categories>
        <category>Â§ßÊï∞ÊçÆ-ÂàÜÂ∏ÉÂºè</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo Êñá‰ª∂ÁªìÊûÑ]]></title>
    <url>%2F2018%2F06%2F04%2Fhexo-%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[hexo Êñá‰ª∂ÁªìÊûÑ Êé•ÁùÄ‰∏äÁØáÂ¶Ç‰ΩïÂèëÂ∏É‰∏ÄÁØáÂçöÂÆ¢ÔºåÊú¨ÁØáÂ∞ÜÂàÜÊûê‰ª•‰∏ãhexoÊ°ÜÊû∂‰∏≠ÁöÑÂü∫Êú¨Êñá‰ª∂ÁªìÊûÑ„ÄÇ Âú®hexoÊ°ÜÊû∂‰∏≠‰∏ªË¶ÅÁõÆÂΩïÁªìÊûÑÂ¶Ç‰∏ãÔºö 12345678|-- _config.yml|-- package.json|-- scaffolds|-- source |-- _posts|-- themes|-- .gitignore|-- package.json _config.ymlÔºöÂÖ®Â±ÄÈÖçÁΩÆÊñá‰ª∂ÔºåÁΩëÁ´ôÁöÑÂæàÂ§ö‰ø°ÊÅØÈÉΩÂú®ËøôÈáåÈÖçÁΩÆÔºåËØ∏Â¶ÇÁΩëÁ´ôÂêçÁß∞ÔºåÂâØÊ†áÈ¢òÔºåÊèèËø∞Ôºå‰ΩúËÄÖÔºåËØ≠Ë®ÄÔºå‰∏ªÈ¢òÔºåÈÉ®ÁΩ≤Á≠âÁ≠âÂèÇÊï∞„ÄÇ package.jsonÔºöhexoÊ°ÜÊû∂ÁöÑÂèÇÊï∞ÂíåÊâÄ‰æùËµñÊèí‰ª∂„ÄÇ scaffoldsÔºöscaffoldsÊòØ‚ÄúËÑöÊâãÊû∂„ÄÅÈ™®Êû∂‚ÄùÁöÑÊÑèÊÄùÔºåÂΩì‰Ω†Êñ∞Âª∫‰∏ÄÁØáÊñáÁ´†Ôºàhexo new 'title'ÔºâÁöÑÊó∂ÂÄôÔºåhexoÊòØÊ†πÊçÆËøô‰∏™ÁõÆÂΩï‰∏ãÁöÑÊñá‰ª∂ËøõË°åÊûÑÂª∫ÁöÑ„ÄÇÂü∫Êú¨‰∏çÁî®ÂÖ≥ÂøÉ„ÄÇ sourceÔºö Ëøô‰∏™ÁõÆÂΩïÂæàÈáçË¶ÅÔºåÊñ∞Âª∫ÁöÑÊñáÁ´†ÈÉΩÊòØÂú®‰øùÂ≠òÂú®Ëøô‰∏™ÁõÆÂΩï‰∏ãÁöÑ._posts „ÄÇÈúÄË¶ÅÊñ∞Âª∫ÁöÑÂçöÊñáÈÉΩÊîæÂú® _posts ÁõÆÂΩï‰∏ã„ÄÇ _posts ÁõÆÂΩï‰∏ãÊòØ‰∏Ä‰∏™‰∏™ markdown Êñá‰ª∂„ÄÇ‰Ω†Â∫îËØ•ÂèØ‰ª•ÁúãÂà∞‰∏Ä‰∏™ hello-world.md ÁöÑÊñá‰ª∂ÔºåÊñáÁ´†Â∞±Âú®Ëøô‰∏™Êñá‰ª∂‰∏≠ÁºñËæë„ÄÇ _posts ÁõÆÂΩï‰∏ãÁöÑmdÊñá‰ª∂Ôºå‰ºöË¢´ÁºñËØëÊàêhtmlÊñá‰ª∂ÔºåÊîæÂà∞ public ÔºàÊ≠§Êñá‰ª∂Áé∞Âú®Â∫îËØ•Ê≤°ÊúâÔºåÂõ†‰∏∫‰Ω†ËøòÊ≤°ÊúâÁºñËØëËøáÔºâÊñá‰ª∂Â§π‰∏ã„ÄÇ themesÔºöÁΩëÁ´ô‰∏ªÈ¢òÁõÆÂΩïÔºåhexoÊúâÈùûÂ∏∏Â•ΩÁöÑ‰∏ªÈ¢òÊãìÂ±ïÔºåÊîØÊåÅÁöÑ‰∏ªÈ¢ò‰πüÂæà‰∏∞ÂØå„ÄÇËØ•ÁõÆÂΩï‰∏ãÔºåÊØè‰∏Ä‰∏™Â≠êÁõÆÂΩïÂ∞±ÊòØ‰∏Ä‰∏™‰∏ªÈ¢ò„ÄÇ _config.yml Êñá‰ª∂ÁöÑÂÖ∑‰ΩìËØ¥Êòé 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: Hexo #ÁΩëÁ´ôÊ†áÈ¢òsubtitle: #ÁΩëÁ´ôÂâØÊ†áÈ¢òdescription: #ÁΩëÁ´ôÊèèËø∞author: John Doe #‰ΩúËÄÖlanguage: #ËØ≠Ë®Ätimezone: #ÁΩëÁ´ôÊó∂Âå∫„ÄÇHexo ÈªòËÆ§‰ΩøÁî®ÊÇ®ÁîµËÑëÁöÑÊó∂Âå∫„ÄÇÊó∂Âå∫ÂàóË°®„ÄÇÊØîÂ¶ÇËØ¥ÔºöAmerica/New_York, Japan, Âíå UTC „ÄÇ# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://yoursite.com #‰Ω†ÁöÑÁ´ôÁÇπUrlroot: / #Á´ôÁÇπÁöÑÊ†πÁõÆÂΩïpermalink: :year/:month/:day/:title/ #ÊñáÁ´†ÁöÑ Ê∞∏‰πÖÈìæÊé• Ê†ºÂºè permalink_defaults: #Ê∞∏‰πÖÈìæÊé•‰∏≠ÂêÑÈÉ®ÂàÜÁöÑÈªòËÆ§ÂÄº# Directory source_dir: source #ËµÑÊ∫êÊñá‰ª∂Â§πÔºåËøô‰∏™Êñá‰ª∂Â§πÁî®Êù•Â≠òÊîæÂÜÖÂÆπpublic_dir: public #ÂÖ¨ÂÖ±Êñá‰ª∂Â§πÔºåËøô‰∏™Êñá‰ª∂Â§πÁî®‰∫éÂ≠òÊîæÁîüÊàêÁöÑÁ´ôÁÇπÊñá‰ª∂„ÄÇtag_dir: tags # Ê†áÁ≠æÊñá‰ª∂Â§π archive_dir: archives #ÂΩíÊ°£Êñá‰ª∂Â§πcategory_dir: categories #ÂàÜÁ±ªÊñá‰ª∂Â§πcode_dir: downloads/code #Include code Êñá‰ª∂Â§πi18n_dir: :lang #ÂõΩÈôÖÂåñÔºài18nÔºâÊñá‰ª∂Â§πskip_render: #Ë∑≥ËøáÊåáÂÆöÊñá‰ª∂ÁöÑÊ∏≤ÊüìÔºåÊÇ®ÂèØ‰ΩøÁî® glob Ë°®ËææÂºèÊù•ÂåπÈÖçË∑ØÂæÑ„ÄÇ # Writingnew_post_name: :title.md # Êñ∞ÊñáÁ´†ÁöÑÊñá‰ª∂ÂêçÁß∞default_layout: post #È¢ÑËÆæÂ∏ÉÂ±Ätitlecase: false # ÊääÊ†áÈ¢òËΩ¨Êç¢‰∏∫ title caseexternal_link: true # Âú®Êñ∞Ê†áÁ≠æ‰∏≠ÊâìÂºÄÈìæÊé•filename_case: 0 #ÊääÊñá‰ª∂ÂêçÁß∞ËΩ¨Êç¢‰∏∫ (1) Â∞èÂÜôÊàñ (2) Â§ßÂÜôrender_drafts: false #ÊòØÂê¶ÊòæÁ§∫ËçâÁ®øpost_asset_folder: false #ÊòØÂê¶ÂêØÂä® Asset Êñá‰ª∂Â§πrelative_link: false #ÊääÈìæÊé•Êîπ‰∏∫‰∏éÊ†πÁõÆÂΩïÁöÑÁõ∏ÂØπ‰ΩçÂùÄ future: true #ÊòæÁ§∫Êú™Êù•ÁöÑÊñáÁ´†highlight: #ÂÜÖÂÆπ‰∏≠‰ª£Á†ÅÂùóÁöÑËÆæÁΩÆ enable: true line_number: true auto_detect: false tab_replace:# Category &amp; Tagdefault_category: uncategorizedcategory_map: #ÂàÜÁ±ªÂà´Âêçtag_map: #Ê†áÁ≠æÂà´Âêç# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DD #Êó•ÊúüÊ†ºÂºètime_format: HH:mm:ss #Êó∂Èó¥Ê†ºÂºè # Pagination## Set per_page to 0 to disable paginationper_page: 10 #ÂàÜÈ°µÊï∞Èáèpagination_dir: page # Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: landscape #‰∏ªÈ¢òÂêçÁß∞# Deployment## Docs: https://hexo.io/docs/deployment.html# ÈÉ®ÁΩ≤ÈÉ®ÂàÜÁöÑËÆæÁΩÆdeploy:]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Â¶Ç‰ΩïÂèëÂ∏É‰∏ÄÁØáÂçöÂÆ¢]]></title>
    <url>%2F2018%2F06%2F04%2F%E5%A6%82%E4%BD%95%E5%8F%91%E5%B8%83%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[Â¶Ç‰ΩïÂèëÂ∏É‰∏ÄÁØáÂçöÂÆ¢ Ëá™Â∑±ÊõæÁªèÊê≠Âª∫ËøáÊó†Êï∞‰∏™ÂçöÂÆ¢„ÄÇ‰ªéÂºÄÂßãËá™Â∑±‰ΩøÁî®yaf+phpËá™Â∑±ÂÜôÁºñÂÜôÔºåÂà∞ÈááÁî®GitHub+jeklly„ÄÇÂêéÊù•‰πüÁî®ËøáËÖæËÆØ‰∫ëÊúçÂä°Âô®+WordPressÁöÑÊ®°Âºè„ÄÇÊúÄËøëÁü•ÈÅì‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÂçöÂÆ¢Êê≠Âª∫ÊñπÊ≥ïÔºåGitHub+hexoÁöÑÊ®°Âºè„ÄÇhexoÊ°ÜÊû∂ÊúâÁùÄ‰∏∞ÂØåÁöÑ‰∏ªÈ¢òÔºåÂπ∂‰∏î‰ΩøÁî®Êñπ‰æø„ÄÇÈ©¨‰∏äË¶ÅÂ∑•‰Ωú‰∫ÜÔºåËøôÊ¨°Ëä±Ë¥π‰∫ÜÂá†Â§©Êê≠Âª∫‰∫Ü‰∏Ä‰∏ãÊñ∞ÁöÑÂçöÂÆ¢„ÄÇÊú™Êù•‰πü‰ºöÊÖ¢ÊÖ¢ÊääËá™Â∑±ÂéüÂÖàÁöÑÂçöÂÆ¢ÔºåÊÖ¢ÊÖ¢ËøÅÁßªÂà∞ËøôÈáå„ÄÇ hexo+GitHubÊê≠Âª∫ÊØîËæÉÁÆÄÂçïËßÅÂ§ßÁ•ûÊïôÁ®ã Êú¨Êñá‰∏ªË¶ÅÂàÜÊûê‰∫Ü‰∏Ä‰∏ãÔºåÂçöÂÆ¢ÂèëÂ∏ÉÁöÑËøáÁ®ã„ÄÇ 1ÊâßË°å‰ª£Á†ÅÔºöhexo new [layout] &lt;title&gt; ËøôÈáå [layout] ÊòØÊåáÂ∏ÉÂ±ÄÊ†ºÂºèÔºåÈªòËÆ§ÁöÑÂ∏ÉÂ±ÄÊ†ºÂºè‰∏∫post„ÄÇÂú®hexo ‰∏≠‰∏ÄÂÖ±Êúâ3‰∏≠‰∏çÂêåÁöÑÈªòËÆ§Ê†ºÂºèÔºö Â∏ÉÂ±ÄÊ†ºÂºè Ë∑ØÂæÑ post source/_posts Page source Draft source/_drafts ÈªòËÆ§Â∏ÉÂ±ÄÊ†ºÂºè‰∏∫ post„ÄÇÂÖà‰ª•Â∏ÉÂ±ÄÊ†ºÂºè‰∏∫ post ÁöÑÊÉÖÂÜµ‰∏∫ËØ¥Êòé„ÄÇ È¶ñÂÖà‰ºöÂú®Âú®Ê†πÁõÆÂΩïÁöÑ‰∏ãÁöÑ source Êñá‰ª∂Â§π‰∏≠ÂàõÂª∫‰∫Ü‰∏Ä‰∏™ _post Êñá‰ª∂Â§πÔºåÂπ∂Âú®ÈáåÈù¢ÁîüÊàê‰∏Ä‰∏™ÂØπÂ∫îÁöÑÊñá‰ª∂„ÄÇ‰æãÂ¶ÇmyBlog.md„ÄÇËøôÊó∂ÂÄôÊñá‰ª∂‰ºöÊ†πÊçÆÊ®°ÊùøÔºåÂ°´ÂÖ•Áõ∏Â∫î‰ø°ÊÅØ„ÄÇÊ®°ÊùøÂ≠òÂÇ®Âú® scaffolds ‰∏≠ÔºåÂèØ‰ª•Ê†πÊçÆÈúÄË¶ÅËá™Ë°å‰øÆÊîπ„ÄÇ Ê†πÊçÆ‰∏çÂêåÁöÑÊ®°ÊùøÔºåÂÜ≥ÂÆö‰∫Ü Front-matter ÁöÑÂÜÖÂÆπ„ÄÇ Front-matter ÊòØÊñá‰ª∂ÊúÄ‰∏äÊñπ‰ª• --- ÂàÜÈöîÁöÑÂå∫ÂüüÔºåÁî®‰∫éÊåáÂÆö‰∏™Âà´Êñá‰ª∂ÁöÑÂèòÈáè„ÄÇ‰æãÂ¶ÇÔºö 123456---title: Â¶Ç‰ΩïÂèëÂ∏É‰∏ÄÁØáÂçöÂÆ¢date: 2018-06-04 01:35:26tags: hexocategories: blog--- Âú® Front-matter ‰∏≠Êúâ‰ª•‰∏ãÈ¢ÑÂÆöÂèÇÊï∞: ÂèÇÊï∞ ÊèèËø∞ ÈªòËÆ§ÂÄº layout Â∏ÉÂ±Ä title Ê†áÈ¢ò&lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space:pre&quot;&gt;&lt;/span&gt; date Âª∫Á´ãÊó•Êúü &lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space:pre&quot;&gt;&lt;/span&gt;Êñá‰ª∂Âª∫Á´ãÊó•Êúü updated Êõ¥Êñ∞Êó•Êúü &lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space:pre&quot;&gt;&lt;/span&gt;Êñá‰ª∂Êõ¥Êñ∞Êó•Êúü comments ÂºÄÂêØÊñáÁ´†ÁöÑËØÑËÆ∫ÂäüËÉΩ true tags Ê†áÁ≠æÔºà‰∏çÈÄÇÁî®‰∫éÂàÜÈ°µÔºâ categories ÂàÜÁ±ªÔºà‰∏çÈÄÇÁî®‰∫éÂàÜÈ°µÔºâ permalink Ë¶ÜÁõñÊñáÁ´†ÁΩëÂùÄ ÂÖ∂‰∏≠ tags ÈÉΩÊòØÂπ∂ÂàóÁöÑ„ÄÇcategories ÂàôÊúâ‰∏•Ê†ºÁöÑÈ°∫Â∫èÊÄßÂíåÂ±ÇÊ¨°ÊÄß„ÄÇ‰æãÂ¶Ç 12345categories:- blogtags:- ÊäÄÊúØ- Êê≠Âª∫]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
