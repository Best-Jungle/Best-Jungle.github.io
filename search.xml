<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[500英里的 BUG]]></title>
    <url>%2F2018%2F06%2F05%2F500%E8%8B%B1%E9%87%8C%E7%9A%84-BUG%2F</url>
    <content type="text"><![CDATA[500英里的BUG BUG是一个程序员必然遇到的问题，我想说任何BUG都有着背后奇怪的逻辑！搞清楚这些对自己真的是一种。。享受 --王栎汉2018年2月2日于南山 发生于麻省理工的一个有意思的bug：只能发500英里的邮件。相当Nerd的bug，有兴趣的人看看吧。 大意是，当年麻省的一名系统管理员，忽然收到统计系主任打来的求助电话“咱们的邮件发不了500英里以外的地方，其实，是520英里更准确点”。 系统管理员心里￥！&amp;……*&amp;。 不过在他开始用自己的邮件测试后，发现邮件的确只能发往520英里以内，其余的收件地点一律失败。 于是在他一片纠结中他渐渐开始发现问题，邮件服务器被人更新过操作系统（当年还是SunOS），但是由于操作系统的发行版往往配备了旧版软件，于是在更新操作系统的时候邮件软件反而被降级了（Sendmail 8 -&gt; Sendmail 5）。 于是进一步调查发现，在更新操作系统时，管理员自己编写的Sendmail配置文件（sendmail.cf）被保留了下来。这样就出现了这种状况：Sendmail 5尝试解析Sendmail 8的配置文件。但是为什么会是500miles呢？为什么是500miles咧？ 原因是这样的，Sendmail 5面对陌生的配置文件，凡是不理解的部分都会忽略，凡是没设置过的配置项自动设置成0。这样其中有一个被设置成0，这一项就是 （连接远端SMTP服务器的超时时间）timeout to connect to the remote SMTP server。后来经过实验，发现0秒的timeout会导致Sendmail在3毫秒后中断连接。 所以，为啥是500miles？ 在当年，MIT的校园网是没有那么多router的，也就没那么多网络延迟，所以连接一个远端主机的时间大概就是光所需的时间。于是3毫秒, 就意味着： 0.003 * 3 * 10 ^ 8 * 0.001 * 0.621 = 558.9000000000001 558英里。也就是558英里以外的服务器，都无法连接到，而558英里以内的服务器，都可以正常通信。当当当，这就是500英里的bug啦。]]></content>
      <categories>
        <category>杂记</category>
      </categories>
      <tags>
        <tag>杂记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Novel Anti-Obfuscation Model for Detecting Malicious Code]]></title>
    <url>%2F2018%2F06%2F05%2FA-Novel-Anti-Obfuscation-Model-for-Detecting-Malicious-Code%2F</url>
    <content type="text"><![CDATA[A Novel Anti-Obfuscation Model for Detecting Malicious Code Yuehan Wang Tong Li YongQuan Cai Zhenghu Ning Fei Xue Abstract In this article, the authors present a new malicious code detection model. The detection model improves typical n-gram feature extraction algorithms that are easy to be obfuscated. Specifically, the proposed model can dynamically determine obfuscation features and then adjust the selection of meaningful features to improve corresponding machine learning analysis. The experimental results show that the feature database, which is built based on the proposed feature selection and cleaning method, contains a stable number of features and can automatically get rid of obfuscation features. Overall, the proposed detection model has features of long timeliness, high applicability and high accuracy of identification Keyword： Malicious、Machine Learning、Feature Extraction、Obfuscation 1 Introduction The malicious code is a kind of software that is intended to damage or disable computers and computer systems, including computer Trojans, blackmail software, spyware, and so on . According to Symantec (2015), more than 44.5 million new pieces of malware created in May 2015. One of the main reasons for this high volume of malware samples is the extensive use of obfuscation and metamorphic techniques by malware developers . So the most of new malicious code can be divided into several families by the original code . The malicious code detection technologies are usually based on features, which represent the original software code. Thus, same malware familys should have the same features (e.g., Wołkowicz &amp; Kešelj (2013) and Preda &amp; Giacobazzi (2005)). By extracting the family features in each malware family, the defense systems can constructs a feature database for detecting variants. However, the obfuscation techniques can help variants to escape the detection by interfering the feature extraction. For example, in the malicious defense system (Lu, Wang, Zhao, Wang, &amp; Su, 2013) which extracting the key string as a feature. Variants escape the detection by equivalently replacing the key string or adding the invalid string. Many scholars (Shafiq, Tabish, Mirza, &amp; Farooq, 2009; Sung, Xu, Chavez, &amp; Mukkamala, 2004; Gaudesi, Marcelli, Sanchez, Squillero, &amp; Tonda, 2016; Tabish, Shafiq, &amp; Farooq,2009) have proposed various feature extraction methods to defend against this kind of obfuscation technology. But such extraction methods can also be broken by emerging obfuscation technology. On the other hand, more effectively extraction methods will also lead to excessive computing resources, systems real-time poor and so on. Machine learning model (Tahan, Rokach, &amp; Shahar, 2012; Narouei, Ahmadi, Giacinto, Takabi, &amp; Sami, 2015; O.W.D.C., 1992) are used to deal with detection malicious code, which have achieved good results. Through the feature database and labels, the model will train a set of classifiers to identify the variants. However, the accuracy of machine learning model depend on the quality of feature database, so that the feature extraction method will determine the accuracy of model . When the extraction method is broken, the obfuscation technologys (Nataraj, Karthikeyan, Jacob, &amp; Manjunath, 2011; Fredrikson, Jha, Christodorescu, Sailer, &amp; Yan, 2010; Svetnik et al., 2003) will make feature database contains a lot of obfuscation features and the accuracy will be be seriously influenced .For the machine learning model used in detection malicious code, ensuring the effectiveness of feature database is an essential research task . In particular, due to the rapid growth of malicious code, the timeliness of feature extraction method becomes more and more short. In addition, it becomes increasingly difficult to maintain the security of the system by using the replacement feature extraction method. In this article, we propose a method to ensure the effectiveness of feature database which cleans the feature database rather than changing the extraction method.The method was guided by the obfuscation features cleaning and feature selection. The final database will be used in the random forests algorithm.The main contributions of this paper are summarized as follows: 1. An algorithm based on multi-sample analysis is proposed to identity obfuscation features dynamically. This method get through analyzing some numbers of sample data in detail and builds a linear regression algorithm. This linear algorithm is used to compute the thresholds of the obfuscation features dynamically for each sample. 2. A feature selection algorithm is proposed to select family feature. The method first normalizes the eigenvector and identify the family feature according to the number of input data set. 3. Achieving the malicious code detection model. The model use random forest algorithm to reduce the effect of obfuscation technologys furtherly and improves the data utilization.The detection of result by the classifier voted. 2. METHODOLOGY In this paper, the main research content is to clean the feature database based on the machine learning malicious code detection system. The n-gram algorithm is one of the earliest feature extraction algorithms for malware code and the feature has a stronger readable and interpretable that extract by the extraction method. It is impossible to guarantee the feature extraction algorithm never be broken. So this paper choose n-gram feature extraction method to build the feature database. The modern obfuscation technologys make n-gram algorithm is invalid and the feature database is full of obfuscation and noisy feature. So the final feature database was guide by the obfuscation features cleaning method and feature selection method. The two clean method make the feature database has a good anti-interference, and replace the bad features automatically with the training samples increased. As shown in Figure 1. Firstly, this paper construct the linear regression algorithm to identity the obfuscation features and clean dynamically.Secondly, the normalizing method make the eigenvalues range be uniform so that the feature selection method will not be affected by the range of eigenvalues. The feature selection method will guide the train database to select the family features. Finally, this paper choose random forest algorithm to construct the classifier cluster. For the test set samples, the final result is voted on by the cluster. The overall flow chart of the model is shown in Figure 1: 2.1. Initial Feature Database Construction The n-gram algorithm is one of the most primitive malicious code feature extraction methods, which requires less malicious code acquisition and lower computational resources. According to this kind of extraction method, the features can be described from the perspective of the real semantics. This model actually characterizes the importance of each feature by counting the number of occurrences of each feature in a single sample.The obfuscation technology confuse the sequence of operations code and produced a lot of obfuscation features, greatly changing the distribution of the features of the sample. The value of obfuscation features are very large, resulting in the problem that these features have an important impact on the model. For a “.asm” malicious code disassembler, mainly by the paragraph start identifier, memory address, bytecode, opcode and parameters formed. Disassembly program fragment as shown in Figure 2, segment that corresponds to the current instruction belongs to the paragraph, address means memory address, bytes corresponding to the hexadecimal code, opcode means opcode and operands are passed parameters. For a disassembled file, we can extract the corresponding opcode by locating “.text” and obtain the malicious code n-gram features. 2.2. Obfuscation Features Cleaning Method The feature database is full of obfuscation and noise features.If the features database is used for training, the model would be likely to appear a serious over-fitting phenomenon, that the mode cannot detect new variants in the future. Therefore, it is necessary to clean the obfuscation features in the database. Due to the complexity of the samples in the training set, the obfuscation technologs used in each sample is different and the feature distribution is also different. Therefore, for each sample, it is necessary to calculation the value of obfuscation features dynamically. It is more reasonable to find the minimum value and clear others obfuscation features which larger than the value. The thresholds of the obfuscation value (ξ) are dynamically changed in each sample. In order to measure and characterize the size of the value effectively, we define the following two indicators: the expected value (Feature_averages) and the feature standard (Feature_median). These two indicators are obtained by solving a single sample dynamically, which are used to describe the distribution of features in the sample. α and β represent the proportion of the characteristic expected value and the characteristic standard value in the feature obfuscation value respectively. The feature obfuscation threshold is calculated as shown in Equation (1), which reflects the relationship between the threshold and the expected standard values: 1ξ = α * Feature_averages + β * Feature_median For a malicious code sample, the features expected value ( Featureaverages represents the ideal value of the feature in the most primitive case of the sample. However, the n-gram algorithm will extract a lot of noisy features naturally which only appeared small amounts. If all feature are directly manipulated, the expected value will be seriously underestimated. The noisy features will be removed through the feature selection method. In this section, the method is used to remove the larger feature value. Therefore, when calculating of the expected value ( Feature_averages , the first step is to delete the same feature value, and then do the average operation. The expected value of the feature is calculated as shown in Equation (2), “m” is the number of residual features after removing the repetition, and the featurei represents the value of the each features. 1Feature_averages = ∑feature(i)/m The feature expected value can only be used to describe the current sample feature distribution. In order to describe the actual semantics of the sample this paper points out the characteristic standard value ( Featuremedian , which is obtained by calculating the median of all the features in the sample. It can reflect the ideal value of the feature when the sample is undisturbed. The distribution of features in a malicious code sample tends to be a Gaussian distribution, the obscure feature has a very small proportion in its feature distribution. Therefore, the range of the ideal feature can be obtained by solving the median value in the distribution.The characteristic standard value is calculated by the following formula 3, m is the number of residual features after removing the repetition, and the featurei represents the feature of the feature(i). The mid function is the median of the solution sequence. 1Feature(median) = mid(feature(1),feature(2),...,feature(m)) By defining two indicators above, we can describe the obfuscation threshold for each malicious sample dynamically. However, the sample set is always very large. It is a very difficult task to analyze all the malicious code samples manually and find the appropriate weight parameter (α, β) to describe the obfuscation threshold. Linear regression is a statistical analysis method that uses the regression analysis in mathematical statistics to determine the quantitative relationship between two or more variables. Therefore, this paper chooses linear regression to learn the value of the parameter (α, β) and calculate the obfuscation threshold in each sample. However, each sample in the set has only the family label and it is useless to identify the obfuscation features. Thus,we should study a small number of samples and make the label of obfuscation features firstly. Then train the linear regression equation based on the labels. The finial equation can be used to predict the obfuscation threshold in unknown samples. The main steps are as follows: Randomly select multiple malicious code sample files. Artificial analysis of sample files to extract the obfuscation threshold. Extracting the feature values and characteristic values of each sample. Selecting 70% of the samples as the training samples of the model, and 30% of the samples as the test samples of the model. Selecting the characteristic prediction value and the characteristic standard value as the model input data and the obfuscation threshold as the prediction data of the model. Inputting Linear Regression Prediction Model Training. The final linear regression equation is obtained to dynamically describe the confounding threshold of the sample. 2.3. Feature Selection Method The n-gram algorithm will extract many noisy features. they will result in some problems, including unclear data features, overheaded model calculation and other issues.Although most of smaller feature values are made up of the noisy data, some of them are important family features in the malicious code. If all the smaller features are cleared, the accuracy of the model will be interfered inevitably. This paper points out a way to quantify the importance of characterization based on the size of the input training data set. If a small feature feature belongs to the noisy data in the overall training set, the feature will only appear in a few samples. And, if it belongs to family features, it will be repeated in the same samples. Therefore, we can select the family features by summing all the sample features. Before constructing the feature selection scheme, this paper refers an operation of feature normalization. Due to the diversity of malicious code samples, the range of eigenvectors in each sample is also different. For the same value of the feature, the importance is different. In order to eliminate the impact of the final measurement of the features which caused by different ranges, this paper presents a standardized operation based on occupancy. For a single sample, the importance of each feature in the sample is measured by calculating the ratio and sum. The characteristic criterion algorithm is as follows: Equation (4), m is the number of all the features in the current sample and feature' represents the new value of the feature after the standardization. 1feature(i) = feature(i)/∑feature(i) (4) From the formula we can see that for the normalized training feature database, the sum of all features of a single sample is 1. Therefore, for the total number of samples S, the sum of all features is S. This paper proposes a feature selection method based on the number of input sample S and the number of malicious code family n in the training set. Since the family features will be repeated in the same family, they will increase the size of the final feature after they have been accumulated. As shown in Equation (5), the value of the Featurei or a feature is the sum of the values of the feature in all sample files. Featurei is the value of the final i-th feature, S is the number of training set samples and Featurei represents the value of the current feature in each sample. 1Feature(i) = ∑feature(i) (5) By studying a small number of samples, we find that for a single sample, the number of features that can usually be extracted is much larger than 100 and floats around 1000. Thus, for a valid feature, the proportion in the sample feature sequence is higher than 0.001. Taking into account that each feature is finally summed by the corresponding features in each sample. So the final selection for the training of the features should meet the following formula 6. Featureselect is the final selection feature for the training signature, S is the total number of training samples and n is the number of malicious code family categories in the sample. 1Feature(select) &gt; S*0.001/n (6) 2.4. Classifier Cluster Construction Due to the complexity of malicious code variants, the final feature database is also very difficult to remove all invalid features completely. Therefore, it is necessary to sample the set and put back. it will improve the generalization and diversity of data sets. The final detection system will be better in anti-interference, robustness. The stochastic forest tree model is an improved model based on the decision tree. The model constructs multiple decision trees by selecting the sample combination feature vector randomly. The malicious database are taken by random sampling with replacement.The different sample set maximizes the utilization rate of the database, which improves the ability of the model for detection future variants of malicious code. The random forest model is shown in Figure 3: When the random forest in training, it will choose different features to build database randomly. In the training feature database that used by the model, some bad features cannot be cleaned up which means there are still some obfuscation features. For the single random node in the model, since the input feature may contain obfuscation features, the final output classifier maybe have some poor bad classifier. However, after the previous obfuscation process, the vast majority of the obfuscation features in the training database has been cleared. What is more, the remaining bad features have only a small proportion in the feature database. By extracting the feature randomly, the effect of the bad features on the final classification can be further relieved. 3. eXPeRIMeNTAL DATA AND RESULTS ANALYSIS 3.1. experimental environment and Test Data The malicious code anti-obfuscation model based on large data is used by the database provided by Microsoft. The set contains a set of known malware files representing a mix of 9 different families. Each malware file has an Id, a 20 character hash value uniquely identifying the file, and a Class, an integer representing one of 9 family names to which the malware may belong:1. Ramnit, 2. Lollipop, 3. Kelihos_ver3, 4. Vundo, 5. Simda, 6. Tracur, 7. Kelihos_ver1, 8. Obfuscator.ACY, 9. Gatak. For each file, the raw data contains the hexadecimal representation of the file’s binary content, without the PE header (to ensure sterility). The total of number of malicious code samples is 10868. The malicious code sample test set is described below in Figure 4: 3.2. experimental Design This experiment mainly verifies the validity of the two feature processing methods mentioned above. According to the previous papers (Svetnik et al, 2003; Latinne, Debeir, &amp; Decaestecker, 2001), The value of the sliding window n in the n-gram algorithm will be taken as 3 and the number of trees in the random forest algorithm is chosen as 100. Control variables in this experiment include the size of input set, the method of cleaning the obfuscated features, the method of feature selection and the method of standardized processing. The method of cleaning the obfuscated features will effect the selection method.Therefore, the first part of experiment will be carried out without feature obfuscation value cleaning process, testing the different feature selection method on the impact of the model. In this experiment, the sample set size 1000, 2000 is to test the effect of set size and schemes on accuracy when the number of samples is lacking. The 5000 is to verify the upper limit of accuracy on schemes when the number of samples is abundance. This experiment also verifies the validity of the normalizing by the A and B series schemes. The selection criteria are adjusted manually according to the analysis of data set. The specific feature selection scheme, and the corresponding parameter selection are shown in Table 1: The A1 and A2 schemes are used to verify the effectiveness of manual analysis and to compare with the other schemes. The B1 and B2 schemes are used to verify the effectiveness of normalizing. And the C1 schemes is the feature selection method of the paper. In order to evaluate the effect of obfuscation feature cleaning method effectively. This paper select the C1 feature selection method, and tests the effect of different cleaning schemes on the accuracy. Considering the A1 and A2 scheme, select the features that the sum of values larger than 300 or 500. Therefore, in the E1 and E2 scheme, feature values which larger than 300 or 500 are selected for cleaning. The specific obfuscation features cleaning scheme is shown in Table 2: E1 and E2 schemes are mainly used to compare the impact of different cleaning conditions on the final program. F1 and F2 schemes are the same in condition, the value of ξ is the threshold of obfuscation features. In addition to compare the effects of different condition with E1, E2 programs, the effects of different cleaning methods on the final results were also tested. 3.3. experimental Results and Analysis Firstly, this paper tests the accuracy of each scheme in different data sets to verify the effectiveness of the random forest algorithm. Considering the random forest algorithm constructs the training subset by extracting the method randomly, the accuracy of each model is different. As shown in Figure 5, the relationships between times of tests and accuracy. The model is based on the A1 test scheme in different input data sets and its tested accuracy. The abscissa indicates the number of model constructions and the ordinate indicates the accuracy of the model. From Figure 5 we can figure out that with the growth of the input data set, the accuracy will increase at the same time and the degree of volatility will reduce gradually. When the input data set reaches a certain threshold, it would be hard to improve the accuracy by increasing the data set continually. The reason is that when the data set is lacking, the feature database is also not comprehensive enough. The classifiers could be affected by the bad features, it will lead to lower and unstable on accuracy. When the data set is adequate, the accuracy of a single classifier will be improved and the predictions between the classifiers tend to be consistent. 3.3.1. Comparison of Feature Selection Schemes In the contrast experiment of the feature selection method, the A1, A2, B1, B2 and C1 schemes verify the validity of the feature selection method. The effect of different schemes on the accuracy is described below in Figure 6: As we can see from Figure 6. For all scheme, the accuracy will increase as the input data set increases. And the A2 scheme is able to achieve the best accuracy of 0.976. A1 scheme is the only test scheme in which the model accuracy is reduced as the input sample set grows. The reason for the decrease in accuracy will be analyzed later. For A2,B1,B2C1 scheme, it is difficult to measure the differences because their accuracy is closer. this paper compare the relationship between the accuracy and the number of features in these schemes. As shown in Figure 7 feature selection test program accuracy and characteristic relationship: From the Figure 7 of A1, we can see that when the input sample reaches 5000, the number of features in the model training feature database is more than 8,000. Excessive quantity of features will lead to obscure model features, increased interference of obfuscation features and so on. The parameters used in the B1 and B2 test schemes are different in different input data sets and the number of features is also change greatly. For example, when the number of input samples reaches 1000, the numbers of features are much lower than the other test programs and the accuracy is also weak. While, when the number of input samples reaches 2000, you can get the best detection results. Compared with the results of the B1, B2 test methods, C1 test method has a stronger adaptability, the number of features reach to 1000 stably. In order to verify the feature selection method proposed in this paper and ensure the stability of the final extracted feature number, this paper tests the relationship between the accuracy and the number of features in all test schemes (E1, E2, F1, F2, C1))) by using the feature selection method. At the same time, in order to compare the unused feature selection methods, this article also joined the A1, A2 test program as a comparison. As shown in Figure 8 feature selection method validation: Figure 8 shows that the use of C1 feature selection method of the test program, the final selection of the number of features is more stable. The final accuracy of the model will increase steadily with the number of the input sample set when the variation of the characteristic number is small. Due to the dynamic selection of features, will be based on the number of the input sample set to adjust the selected features. When the input sample set is increased, the feature selection method will also improve the selection of the features of the features to achieve the features of the elimination of poor features. 3.3.2. Obfuscation Value Cleaning Scheme Comparison Although we can guarantee the stability of the number of features used in the model by adjusting the selection method dynamically. However, the method of cleaning the obfuscated features will influence the effect of the accuracy greatly. From Figure 6 can be learned, the accuracy in C1 scheme is lowest, the final detection accuracy has a greater space for improvement. In order to evaluate the impact of the clean obfuscation features method on the model and the differences between different methods. This paper compares the C1, E1, E2, F1, F2, 5 different schemes.In particularly, the C1 scheme has no clean the obfuscation features and the others schemes has different clean method. The model accuracy of each scheme and the sample set relations as shown in Figure 9. Accuracy and input sample set relationship: As shown in Figure 9, the E1, E2, F1, and F2 schemes higher in the final model accuracy than the C1 scheme. The obfuscation feature cleaning method improved accuracy. It is worth mentioning that, for F1, F2 scheme is the same way to find out the obfuscation threshold. But the F1 scheme reduce the obfuscation features value to the obfuscation threshold and the F2 scheme choice reduce to zero. In the F1 scheme the accuracy reducing with the increase in the number of samples. The reason is that F1 scheme changes the feature distribution of the sample by changing the value of the obfuscated features in the sample. Since the value of obfuscated features is still greater than zero after the change and this kind of the feature distribution can be learned by the model. When the input sample size is small, this way will improve the accuracy of the model. However, when the input sample continues to increase, this changing have a certain conflict with the actual feature distribution and the accuracy will be reduced . The F2 scheme clean the obfuscation features from the sample distribution. And this change has not been studied by the model, so there is no human impact the accuracy will be more real. Since the E1, E2 and F2 schemes has adopted the C1 feature selection method, the model detection accuracy and the number of features are closer to each other . The random forest features will select features randomly.Thus,the more obfuscation features in the database, the more unstable the final result. When repeated experiments, the detection accuracy of large fluctuations means the obfuscation features clean schemes is worse. If the number of experimented is small, the results are difficult to observe.In this paper, we choose experimented 100 times for the every test schemes E1, E2 and F2. It is enough to observe the result. The cleaning effect of each test scheme is measured by comparing the degree of volatility of the final model accuracy of each test scheme. The accuracy and frequency of each obfuscation cleaning scheme are shown in Figures 10, 11, 12: In order to measure the detection accuracy of the fluctuations of the test scenarios in Figure 10- 12 in different data sets, the standard deviation is used to measure the volatility of the final model accuracy of each test scheme in this paper. The standard deviation is calculated from the square root of the arithmetic mean of the squared difference squared, reflecting the degree of discretization of a data set. The average number of the same two sets of data, the standard deviation may not be the same. This paper counts the standard deviations of the cleaning schemes in different input samples, as shown in Table 3. 4. RELATED WORK At present, the research of malicious detection technology is mainly focus on the feature extraction. In terms of the features extraction in malicious code (Rieck, Trinius, &amp; Willems, 2011; Li, Santorelli, Laforest, &amp; Coates, 2015; Yong &amp; Zuo, 2007), they have been studied by previous researchers fully. In order to defent against obfuscation technologys, a various of methods of malicious code feature extraction have been proposed by researchers. These methods from the security attributes, dependencies, real semantics and so on. The modern extraction method focuses on the the actual semantics of malicious code. This kind of semantic-based method can express the actual behavior of malicious code. It has strong interpretability, easy to maintain the actual analysis of personnel and the development of the detection strategy. The related work in this paper will be introduced from the malicious code detection technology and feature extraction methods these two aspects. 4.1. Detection Technology In general, malicious code detection techniques can be divided into two types of technology (Shahzad &amp; Lavesson, 2013), including the detection method based on heuristic and the detection method based on signature. Early detection methods are mostly based on heuristic, the requirements of this detection method about the researchers’ experience and judgment are very high. For example, the Rootkit Revealer (Willems, Holz, &amp; Freiling, 2007) detection system identifies hidden processes, files and associated registry information by comparing system upper-level information and file system status from the kernel. The detection effect of this type of detection system depends on the degree of research on the system, it does not have generalization and versatility. Therefore, most of the detection systems is based on feature database. Machine learning model can predict variants well based on database, the modern detection systems used machine learning to train classifiers .The detection technique could be divided into three steps (Yin, Song, Egele, Kruegel, &amp; Kirda, 2007; Narudin, Feizollah, Anuar, &amp; Gani, 2014), the first step is the extraction of the features of malicious code. Second, removing obfuscation features, fusion and building low-dimensional features database. Finally, the machine learning algorithms use features database to train the classifier and identity the classification of malicious code. The malicious code detection system which based on signature (Abawajy, Kelarev, &amp; Chowdhury, 2014) constructs the feature database of a kind of malicious code by extracting the morphological features of executable binary file. After the same feature has been extracted from the malicious code to be detected, it is detected by pattern matching with the feature database. However, this method could only match a single malicious code. With the increase number of the sample, the data volume of the feature set is often very large, which is not conducive to the automatic detection of malicious code in the future. With the progress of dis-assembly technology, hidden sensitive vocabulary, special methods, resource calls could be found through the dis-assembly technology on the malicious code dynamic analysis in the malicious code samples. Therefore, researchers have proposed the detection method that based on these behavioral features. Behavior-based detection methods pay more attention to the actual behavior of malicious code and have a better ability to defence obfuscation operation. 4.2. Malicious Code Feature extraction The use of a single feature extraction method does not have good anti-jamming and stability. When the extraction method is compromised, the monitoring system will break generally. In this case, some scholars (Kirda, Kruegel, Banks, Vigna, &amp; Kemmerer, 2006) proposed for the malicious code for multi-feature extraction and the extraction of the features of the fusion. So that it could get more robust anti-interference malicious code family core features. This multi-feature extraction and fusion method enhances the anti-jamming of the features. The malicious code can’t fully affect the fusion features. So it is difficult to influence the final result. Kirda and others (Rui, Feng, Yi, &amp; Pu-Rui, 2012) use spy code to obtain user-sensitive data and then leak the behavior of the data features of the test. But this method is limited to the detection of spy malicious code, other data does not cause the disclosure of malicious code could not be detected. From the perspective of the actual semantic code of malicious code, Rui et al. (2012) and others use feature map to calculate malicious code by constructing a behavioral feature map based on malicious code semantics. It has achieved very good detection results. However, this method is based on the detection method of the program itself merely. It cannot recognize some special variants of malicious code without taking into account the program for the resource call problem. Mao et al. (2017) and others (Zhang, Ren, Jiang, &amp; Zhang, 2015) proposed an active learning method that solves the problem of malicious code detection when the labeled samples are less marked. The feature extraction method mainly constructs the system data flow dependency graph. The graph according to the resource scheduling relation of the sample from the angle of the malicious code resource call. The method is to use a large number of normal software behavior data and a small amount of tagged malicious code. The normal software resources would be distinguished between differentiated malicious software through the active learning way to update the malicious code classifier. In a small amount of samples of the case, the timely identification of new malicious code. This kind of method mainly describes the features from the aspects of security attribute and statistical feature. However, the expression form of this feature is not comprehensive for the future analysis of malicious code. Nataraj and Karthikeyan (2011) first proposed the concept of malicious images. Malicious images from the malicious code executable binary file and the binary file mapping to generate grayscale malicious images. The method uses the same family of malicious code programs that will use some family of historical resource files, resulting in malicious images that are similar. The method requires a small computational cost and a strong anti - jamming capability. However, due to the increasing complexity of malicious code, for malicious code image extraction detection methods, you can also call the obfuscation of resources, etc. to bypass or interfere with the detection of features. Han, Qu, Yao, Guo, and Zhou (2014) and Han, Yao, Wu, and Guo (2014) combined with image analysis technology to extract the fingerprint feature of the image, and use the gray-level co-occurrence matrix algorithm to extract the malicious code texture fingerprint. 5. SUMMARY Due to the feature extraction method was broken, the feature database full of obfuscation features and the malware variants escape detection. An new type of malicious code detection model is proposed in this paper. Our feature database is built on n-gram feature extraction method. At the same time, obfuscation features cleaning and feature selection method will eliminate the effects of obfuscation techniques on database. In particular, our method can dynamically determine obfuscation features and adjust the selection of family features. In such a way, the method can ensuring the effectiveness of feature database and improve the accuracy of detect variants. When the value of obfuscation features is close to the value of family features, the cleaning method can not remove this kind of obfuscation features effectively. Thus, most of obfuscation features have been cleared by this cleaning method. Moreover, the random forest algorithm will further reduce the influence of remaining obfuscation features. In order to select the features more rationally, this paper proposes a dynamic selection method based on sample set. This method can stabilize the number of features in the database, the features will also change when the number of input samples increases. Although the new malicious code detection model proposed in this paper can eliminate most of the obfuscation features, there are still some shortcomings. For example, the speed of operation is too low, the dependency of a large number of mark data is quite strong and so on. On the other hand, the database is very large, it is difficult for model to learn and detect new malware families that do not have a lot of tagged detal. Therefore, how to deal with fewer malicious code families will be launched in the follow-up work. The spark platform has the ability to parallelize the process of data. It is necessary to combine the test model and spark platform together. Considering that our method takes more time for a single sample process, it is need to separate the model detection and the sample feature extraction separately. At the same time, the high real-time requirements of online detection and the need to simultaneously handle multiple sample files.Therefore, the future model combined with the spark platform for the sample set to calculate and feature extraction, and return the eigenvector. Then, the feature vector is calculated by the classifier to improve the response speed of the model. We will also test the performance of the model in more malware sets.]]></content>
      <categories>
        <category>个人成果</category>
      </categories>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解 xgboost]]></title>
    <url>%2F2018%2F06%2F05%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-xgboost%2F</url>
    <content type="text"><![CDATA[深入理解 xgboost 前言 1）Boosted Tree的若干同义词 可能有人会问，为什么我没有听过这个名字。这是因为Boosted Tree有各种马甲，比如GBDT, GBRT (gradient boosted regression tree)，LambdaMART也是一种boosted tree的变种。网上有很多介绍Boosted tree的资料，不过大部分都是基于Friedman的最早一篇文章Greedy Function Approximation: A Gradient Boosting Machine的翻译。个人觉得这不是最好最一般地介绍boosted tree的方式。而网上除了这个角度之外的介绍并不多。 2）有监督学习算法的逻辑组成 要讲boosted tree，要先从有监督学习讲起。在有监督学习里面有几个逻辑上的重要组成部件33，初略地分可以分为：模型，参数 和 目标函数。 i. 模型和参数 模型指给定输入xixi如何去预测 输出 yiyi。我们比较常见的模型如线性模型（包括线性回归和logistic regression）采用了线性叠加的方式进行预测ŷ i=∑jwjxijy^i=∑jwjxij 。其实这里的预测yy可以有不同的解释，比如我们可以用它来作为回归目标的输出，或者进行sigmoid 变换得到概率，或者作为排序的指标等。而一个线性模型根据yy的解释不同（以及设计对应的目标函数）用到回归，分类或排序等场景。参数指我们需要学习的东西，在线性模型中，参数指我们的线性系数ww。 ii. 目标函数：损失 + 正则 模型和参数本身指定了给定输入我们如何做预测，但是没有告诉我们如何去寻找一个比较好的参数，这个时候就需要目标函数登场了。一般的目标函数包含下面两项： 1Obj(ø) = L(ø) + Ω(ø) 其中 L(ø) 用来指导模型参数调整的方向。Ω(ø) 为正则项，用来防止过拟合现象。 常见的误差函数比如平方误差、logistic误差函数等。而对于线性模型常见的正则化项有L2正则和L1正则。这样目标函数的设计来自于统计学习里面的一个重要概念叫做Bias-variance tradeoff44。比较感性的理解，Bias可以理解为假设我们有无限多数据的时候，可以训练出最好的模型所拿到的误差。而Variance是因为我们只有有限数据，其中随机性带来的误差。目标中误差函数鼓励我们的模型尽量去拟合训练数据，这样相对来说最后的模型会有比较少的 bias。而正则化项则鼓励更加简单的模型。因为当模型简单之后，有限数据拟合出来结果的随机性比较小，不容易过拟合，使得最后模型的预测更加稳定。 iii. 优化算法 讲了这么多有监督学习的基本概念，为什么要讲这些呢？ 是因为这几部分包含了机器学习的主要成分，也是机器学习工具设计中划分模块比较有效的办法。其实这几部分之外，还有一个优化算法，就是给定目标函数之后怎么学的问题。之所以我没有讲优化算法，是因为这是大家往往比较熟悉的“机器学习的部分”。而有时候我们往往只知道“优化算法”，而没有仔细考虑目标函数的设计的问题，比较常见的例子如决策树的学习，大家知道的算法是每一步去优化gini entropy，然后剪枝，但是没有考虑到后面的目标是什么。 Boosted Tree 话题回到boosted tree，我们也是从这几个方面开始讲，首先讲模型。Boosted tree 最基本的组成部分叫做回归树(regression tree)，也叫做CART5。 上面就是一个CART的例子。CART会把输入根据输入的属性分配到各个叶子节点，而每个叶子节点上面都会对应一个实数分数。上面的例子是一个预测一个人是否会喜欢电脑游戏的 CART，你可以把叶子的分数理解为有多可能这个人喜欢电脑游戏。有人可能会问它和decision tree的关系，其实我们可以简单地把它理解为decision tree的一个扩展。从简单的类标到分数之后，我们可以做很多事情，如概率预测，排序。 一个CART往往过于简单无法有效地预测，因此一个更加强力的模型叫做tree ensemble。在上面的例子中，我们用两棵树来进行预测。我们对于每个样本的预测结果就是每棵树预测分数的和。到这里，我们的模型就介绍完毕了。现在问题来了，我们常见的随机森林和boosted tree和tree ensemble有什么关系呢？如果你仔细的思考，你会发现RF和boosted tree的模型都是tree ensemble，只是构造（学习）模型参数的方法不同。第二个问题：在这个模型中的“参数”是什么。在tree ensemble中，参数对应了树的结构，以及每个叶子节点上面的预测分数。 最后一个问题当然是如何学习这些参数。在这一部分，答案可能千奇百怪，但是最标准的答案始终是一个：定义合理的目标函数，然后去尝试优化这个目标函数。在这里我要多说一句，因为决策树学习往往充满了heuristic。 如先优化吉尼系数，然后再剪枝啦，限制最大深度，等等。其实这些heuristic的背后往往隐含了一个目标函数，而理解目标函数本身也有利于我们设计学习算法，这个会在后面具体展开。 对于tree ensemble，我们可以比较严格的把我们的模型写成是： 其中每个f是一个在函数空间6(F)里面的函数，而F对应了所有regression tree的集合。我们设计的目标函数也需要遵循前面的主要原则，包含两部分 iii. 模型学习：additive training 其中第一部分是训练误差，也就是大家相对比较熟悉的如平方误差, logistic loss等。而第二部分是每棵树的复杂度的和。这个在后面会继续讲到。因为现在我们的参数可以认为是在一个函数空间里面，我们不能采用传统的如SGD之类的算法来学习我们的模型，因此我们会采用一种叫做additive training的方式（另外，在我个人的理解里面7，boosting就是指additive training的意思）。每一次保留原来的模型不变，加入一个新的函数$f$到我们的模型中。 现在还剩下一个问题，我们如何选择每一轮加入什么f呢？答案是非常直接的，选取一个f来使得我们的目标函数尽量最大地降低。 这个公式可能有些过于抽象，我们可以考虑当l是平方误差的情况。这个时候我们的目标可以被写成下面这样的二次函数： 更加一般的，对于不是平方误差的情况，我们会采用如下的泰勒展开近似来定义一个近似的目标函数，方便我们进行这一步的计算。 当我们把常数项移除之后，我们会发现如下一个比较统一的目标函数。这一个目标函数有一个非常明显的特点，它只依赖于每个数据点的在误差函数上的一阶导数和二阶导数。有人可能会问，这个材料似乎比我们之前学过的决策树学习难懂。为什么要花这么多力气来做推导呢？ 因为这样做使得我们可以很清楚地理解整个目标是什么，并且一步一步推导出如何进行树的学习。这一个抽象的形式对于实现机器学习工具也是非常有帮助的。传统的GBDT可能大家可以理解如优化平法a残差，但是这样一个形式包含可所有可以求导的目标函数。也就是说有了这个形式，我们写出来的代码可以用来求解包括回归，分类和排序的各种问题，正式的推导可以使得机器学习的工具更加一般。 iv. 树的复杂度 到目前为止我们讨论了目标函数中训练误差的部分。接下来我们讨论如何定义树的复杂度。我们先对于f的定义做一下细化，把树拆分成结构部分q和叶子权重部分w。下图是一个具体的例子。结构函数q把输入映射到叶子的索引号上面去，而w给定了每个索引号对应的叶子分数是什么。 当我们给定了如上定义之后，我们可以定义一棵树的复杂度如下。这个复杂度包含了一棵树里面节点的个数，以及每个树叶子节点上面输出分数的$L2$模平方。当然这不是唯一的一种定义方式，不过这一定义方式学习出的树效果一般都比较不错。下图还给出了复杂度计算的一个例子。 v. 关键步骤 接下来是最关键的一步11，在这种新的定义下，我们可以把目标函数进行如下改写，其中I被定义为每个叶子上面样本集合 Ij = { i|q(xi)=j} 这一个目标包含了T个相互独立的单变量二次函数。我们可以定义 那么这个目标函数可以进一步改写成如下的形式，假设我们已经知道树的结构q，我们可以通过这个目标函数来求解出最好的w，以及最好的w对应的目标函数最大的增益 这两个的结果对应如下，左边是最好的w，右边是这个w对应的目标函数的值。到这里大家可能会觉得这个推导略复杂。其实这里只涉及到了如何求一个一维二次函数的最小值的问题12。如果觉得没有理解不妨再仔细琢磨一下 vi. 打分函数计算举例 Obj代表了当我们指定一个树的结构的时候，我们在目标上面最多减少多少。我们可以把它叫做结构分数(structure score)。你可以认为这个就是类似吉尼系数一样更加一般的对于树结构进行打分的函数。下面是一个具体的打分函数计算的例子 vii. 枚举所有不同树结构的贪心法 所以我们的算法也很简单，我们不断地枚举不同树的结构，利用这个打分函数来寻找出一个最优结构的树，加入到我们的模型中，再重复这样的操作。不过枚举所有树结构这个操作不太可行，所以常用的方法是贪心法，每一次尝试去对已有的叶子加入一个分割。对于一个具体的分割方案，我们可以获得的增益可以由如下公式计算 对于每次扩展，我们还是要枚举所有可能的分割方案，如何高效地枚举所有的分割呢？我假设我们要枚举所有 x&lt;a 这样的条件，对于某个特定的分割a我们要计算a左边和右边的导数和。 我们可以发现对于所有的a，我们只要做一遍从左到右的扫描就可以枚举出所有分割的梯度和GL和GR。然后用上面的公式计算每个分割方案的分数就可以了。 观察这个目标函数，大家会发现第二个值得注意的事情就是引入分割不一定会使得情况变好，因为我们有一个引入新叶子的惩罚项。优化这个目标对应了树的剪枝， 当引入的分割带来的增益小于一个阀值的时候，我们可以剪掉这个分割。大家可以发现，当我们正式地推导目标的时候，像计算分数和剪枝这样的策略都会自然地出现，而不再是一种因为heuristic而进行的操作了。 讲到这里文章进入了尾声，虽然有些长，希望对大家有所帮助，这篇文章介绍了如何通过目标函数优化的方法比较严格地推导出boosted tree的学习。因为有这样一般的推导，得到的算法可以直接应用到回归，分类排序等各个应用场景中去。 5 尾声：xgboost 这篇文章讲的所有推导和技术都指导了xgboost 的设计。xgboost是大规模并行boosted tree的工具，它是目前最快最好的开源boosted tree工具包，比常见的工具包快10倍以上。在数据科学方面，有大量kaggle选手选用它进行数据挖掘比赛，其中包括两个以上kaggle比赛的夺冠方案。在工业界规模方面，xgboost的分布式版本有广泛的可移植性，支持在YARN, MPI, Sungrid Engine等各个平台上面运行，并且保留了单机并行版本的各种优化，使得它可以很好地解决于工业界规模的问题。有兴趣的同学可以尝试使用一下，也欢迎贡献代码。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解GBDT]]></title>
    <url>%2F2018%2F06%2F05%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3GBDT%2F</url>
    <content type="text"><![CDATA[深入理解GBDT GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。 GBDT主要由三个概念组成：Regression Decistion Tree（即DT)，Gradient Boosting（即GB)，Shrinkage (算法的一个重要演进分枝，目前大部分源码都按该版本实现）。搞定这三个概念后就能明白GBDT是如何工作的，要继续理解它如何用于搜索排序则需要额外理解RankNet概念，之后便功德圆满。下文将逐个碎片介绍，最终把整张图拼出来。 一、 DT：回归树 Regression Decision Tree 提起决策树（DT, Decision Tree) 绝大部分人首先想到的就是C4.5分类决策树。但如果一开始就把GBDT中的树想成分类树，那就是一条歪路走到黑，一路各种坑，最终摔得都要咯血了还是一头雾水说的就是LZ自己啊有木有。咳嗯，所以说千万不要以为GBDT是很多棵分类树。决策树分为两大类，回归树和分类树。前者用于预测实数值，如明天的温度、用户的年龄、网页的相关程度；后者用于分类标签值，如晴天/阴天/雾/雨、用户性别、网页是否是垃圾页面。这里要强调的是，前者的结果加减是有意义的，如10岁+5岁-3岁=12岁，后者则无意义，如男+男+女=到底是男是女？ GBDT的核心在于累加所有树的结果作为最终结果，就像前面对年龄的累加（-3是加负3），而分类树的结果显然是没办法累加的，所以GBDT中的树都是回归树，不是分类树，这点对理解GBDT相当重要（尽管GBDT调整后也可用于分类但不代表GBDT的树是分类树）。那么回归树是如何工作的呢？ 下面我们以对人的性别判别/年龄预测为例来说明，每个instance都是一个我们已知性别/年龄的人，而feature则包括这个人上网的时长、上网的时段、网购所花的金额等。 作为对比，先说分类树，我们知道C4.5分类树在每次分枝时，是穷举每一个feature的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的feature和阈值（熵最大的概念可理解成尽可能每个分枝的男女比例都远离1:1），按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。若还不明白可以Google “Regression Tree”，或阅读本文的第一篇论文中Regression Tree部分。 二、 GB：梯度迭代 Gradient Boosting 好吧，我起了一个很大的标题，但事实上我并不想多讲Gradient Boosting的原理，因为不明白原理并无碍于理解GBDT中的Gradient Boosting。 Boosting，迭代，即通过迭代多棵树来共同决策。这怎么实现呢？难道是每棵树独立训练一遍，比如A这个人，第一棵树认为是10岁，第二棵树认为是0岁，第三棵树认为是20岁，我们就取平均值10岁做最终结论？–当然不是！且不说这是投票方法并不是GBDT，只要训练集不变，独立训练三次的三棵树必定完全相同，这样做完全没有意义。之前说过，GBDT是把所有树的结论累加起来做最终结论的，所以可以想到每棵树的结论并不是年龄本身，而是年龄的一个累加量。GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。这就是Gradient Boosting在GBDT中的意义，简单吧。 三、 GBDT工作过程实例。 还是年龄预测，简单起见训练集只有4个人，A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下图1所示结果： 现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点做多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图2所示结果： 在第一棵树分枝和图1一样，由于A,B年龄较为相近，C,D年龄较为相近，他们被分为两拨，每拨用平均年龄作为预测值。此时计算残差（残差的意思就是： A的预测值 + A的残差 = A的实际值），所以A的残差就是16-15=1（注意，A的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是15，如果还有树则需要都累加起来作为A的预测值）。进而得到A,B,C,D的残差分别为-1,1，-1,1。然后我们拿残差替代A,B,C,D的原值，到第二棵树去学习，如果我们的预测值和它们的残差相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了。这里的数据显然是我可以做的，第二棵树只有两个值1和-1，直接分成两个节点。此时所有人的残差都是0，即每个人都得到了真实的预测值。 换句话说，现在A,B,C,D的预测值都和真实年龄一致了。Perfect!： A: 14岁高一学生，购物较少，经常问学长问题；预测年龄A = 15 – 1 = 14 B: 16岁高三学生；购物较少，经常被学弟问问题；预测年龄B = 15 + 1 = 16 C: 24岁应届毕业生；购物较多，经常问师兄问题；预测年龄C = 25 – 1 = 24 D: 26岁工作两年员工；购物较多，经常被师弟问问题；预测年龄D = 25 + 1 = 26 那么哪里体现了Gradient呢？其实回到第一棵树结束时想一想，无论此时的cost function是什么，是均方差还是均差，只要它以误差作为衡量标准，残差向量(-1, 1, -1, 1)都是它的全局最优方向，这就是Gradient。 讲到这里我们已经把GBDT最核心的概念、运算过程讲完了！没错就是这么简单。不过讲到这里很容易发现三个问题： 1）既然图1和图2 最终效果相同，为何还需要GBDT呢？ 答案是过拟合。过拟合是指为了让训练集精度更高，学到了很多”仅在训练集上成立的规律“，导致换一个数据集当前规律就不适用了。其实只要允许一棵树的叶子节点足够多，训练集总是能训练到100%准确率的（大不了最后一个叶子上只有一个instance)。在训练精度和实际精度（或测试精度）之间，后者才是我们想要真正得到的。 我们发现图1为了达到100%精度使用了3个feature（上网时长、时段、网购金额），其中分枝“上网时长&gt;1.1h” 很显然已经过拟合了，这个数据集上A,B也许恰好A每天上网1.09h, B上网1.05小时，但用上网时间是不是&gt;1.1小时来判断所有人的年龄很显然是有悖常识的； 相对来说图2的boosting虽然用了两棵树 ，但其实只用了2个feature就搞定了，后一个feature是问答比例，显然图2的依据更靠谱。（当然，这里是LZ故意做的数据，所以才能靠谱得如此狗血。实际中靠谱不靠谱总是相对的） Boosting的最大好处在于，每一步的残差计算其实变相地增大了分错instance的权重，而已经分对的instance则都趋向于0。这样后面的树就能越来越专注那些前面被分错的instance。就像我们做互联网，总是先解决60%用户的需求凑合着，再解决35%用户的需求，最后才关注那5%人的需求，这样就能逐渐把产品做好，因为不同类型用户需求可能完全不同，需要分别独立分析。如果反过来做，或者刚上来就一定要做到尽善尽美，往往最终会竹篮打水一场空。 2）Gradient呢？不是“G”BDT么？ 到目前为止，我们的确没有用到求导的Gradient。在当前版本GBDT描述中，的确没有用到Gradient，该版本用残差作为全局最优的绝对方向，并不需要Gradient求解. 3）这不是boosting吧？Adaboost可不是这么定义的。 这是boosting，但不是Adaboost。GBDT不是Adaboost Decistion Tree。就像提到决策树大家会想起C4.5，提到boost多数人也会想到Adaboost。Adaboost是另一种boost方法，它按分类对错，分配不同的weight，计算cost function时使用这些weight，从而让“错分的样本权重越来越大，使它们更被重视”。Bootstrap也有类似思想，它在每一步迭代时不改变模型本身，也不计算残差，而是从N个instance训练集中按一定概率重新抽取N个instance出来（单个instance可以被重复sample），对着这N个新的instance再训练一轮。由于数据集变了迭代模型训练结果也不一样，而一个instance被前面分错的越厉害，它的概率就被设的越高，这样就能同样达到逐步关注被分错的instance，逐步完善的效果。Adaboost的方法被实践证明是一种很好的防止过拟合的方法，但至于为什么则至今没从理论上被证明。GBDT也可以在使用残差的同时引入Bootstrap re-sampling，GBDT多数实现版本中也增加的这个选项，但是否一定使用则有不同看法。re-sampling一个缺点是它的随机性，即同样的数据集合训练两遍结果是不一样的，也就是模型不可稳定复现，这对评估是很大挑战，比如很难说一个模型变好是因为你选用了更好的feature，还是由于这次sample的随机因素。 四、Shrinkage Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。用方程来看更清晰，即 没用Shrinkage时：（yi表示第i棵树上y的预测值， y(1~i)表示前i棵树y的综合预测值） y(i+1) = 残差(y1~yi)， 其中： 残差(y1~yi) = y真实值 – y(1 ~ i) y(1 ~ i) = SUM(y1, …, yi) Shrinkage不改变第一个方程，只把第二个方程改为： y(1 ~ i) = y(1 ~ i-1) + step * yi 即Shrinkage仍然以残差作为学习目标，但对于残差学习出来的结果，只累加一小部分（step*残差）逐步逼近目标，step一般都比较小，如0.01~0.001（注意该step非gradient的step），导致各个树的残差是渐变的而不是陡变的。直觉上这也很好理解，不像直接用残差一步修复误差，而是只修复一点点，其实就是把大步切成了很多小步。本质上，Shrinkage为每棵树设置了一个weight，累加时要乘以这个weight，但和Gradient并没有关系。这个weight就是step。就像Adaboost一样，Shrinkage能减少过拟合发生也是经验证明的，目前还没有看到从理论的证明。 五、 GBDT的适用范围 该版本GBDT几乎可用于所有回归问题（线性/非线性），相对logistic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。 六、 搜索引擎排序应用 RankNet 搜索排序关注各个doc的顺序而不是绝对值，所以需要一个新的cost function，而RankNet基本就是在定义这个cost function，它可以兼容不同的算法（GBDT、神经网络…）。 实际的搜索排序使用的是LambdaMART算法，必须指出的是由于这里要使用排序需要的cost function，LambdaMART迭代用的并不是残差。Lambda在这里充当替代残差的计算方法，它使用了一种类似Gradient*步长模拟残差的方法。这里的MART在求解方法上和之前说的残差略有不同，其区别描述见这里。 就像所有的机器学习一样，搜索排序的学习也需要训练集，这里一般是用人工标注实现，即对每一个(query,doc) pair给定一个分值（如1,2,3,4）,分值越高表示越相关，越应该排到前面。然而这些绝对的分值本身意义不大，例如你很难说1分和2分文档的相关程度差异是1分和3分文档差距的一半。相关度本身就是一个很主观的评判，标注人员无法做到这种定量标注，这种标准也无法制定。但标注人员很容易做到的是”AB都不错，但文档A比文档B更相关，所以A是4分，B是3分“。RankNet就是基于此制定了一个学习误差衡量方法，即cost function。具体而言，RankNet对任意两个文档A,B，通过它们的人工标注分差，用sigmoid函数估计两者顺序和逆序的概率P1。然后同理用机器学习到的分差计算概率P2（sigmoid的好处在于它允许机器学习得到的分值是任意实数值，只要它们的分差和标准分的分差一致，P2就趋近于P1）。这时利用P1和P2求的两者的交叉熵，该交叉熵就是cost function。它越低说明机器学得的当前排序越趋近于标注排序。为了体现NDCG的作用（NDCG是搜索排序业界最常用的评判标准），RankNet还在cost function中乘以了NDCG。 好，现在我们有了cost function，而且它是和各个文档的当前分值yi相关的，那么虽然我们不知道它的全局最优方向，但可以求导求Gradient，Gradient即每个文档得分的一个下降方向组成的N维向量，N为文档个数（应该说是query-doc pair个数）。这里仅仅是把”求残差“的逻辑替换为”求梯度“，可以这样想：梯度方向为每一步最优方向，累加的步数多了，总能走到局部最优点，若该点恰好为全局最优点，那和用残差的效果是一样的。这时套到之前讲的逻辑，GDBT就已经可以上了。那么最终排序怎么产生呢？很简单，每个样本通过Shrinkage累加都会得到一个最终得分，直接按分数从大到小排序就可以了（因为机器学习产生的是实数域的预测分，极少会出现在人工标注中常见的两文档分数相等的情况，几乎不同考虑同分文档的排序方式） 另外，如果feature个数太多，每一棵回归树都要耗费大量时间，这时每个分支时可以随机抽一部分feature来遍历求最优（ELF源码实现方式）。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从损失函数看-xgboost、GBDT、adaboost]]></title>
    <url>%2F2018%2F06%2F05%2F%E4%BB%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9C%8B-xgboost%E3%80%81GBDT%E3%80%81adaboost%2F</url>
    <content type="text"><![CDATA[adaboost和GBDT和xgboost在损失函数的最优化方法是有很多不同的，三者的不同之处其实就在于最优化方法的不同（这样说不知道是否妥当，至少站在这个角度的我认为是正确的，多年后可能发现这个观点不太妥当）。adaboost在李航博士的《统计学习基础》里面用加法模型和向前算法解释了权值更新策略。在解释的过程中，样本权值更新和弱分类器权值的求取是直接通过偏导数等于零来计算的，如果记不清楚的可以回去仔细看一下。不管是做回归还是做分类，adaboost的目标都是找到一个值（通过使得偏导数等零的方法）直接使得损失函数降低到最小。 而GBDT使用的是梯度下降法使得损失函数有所降低。首先说一下为什么会提出这种方法，然后说一下为什么GBDT是使用梯度下降法将损失函数有所降低。这种方法的出现肯定是因为对于某些损失函数，偏导数等于零是可解的，而对于一般的损失函数偏导数等于零是无法求解的，不然梯度下降法也不会存在了。而至于为什么GBDT使用的是梯度下降法将损失函数降低的。首先将损失函数在ft-1（x）处做一阶泰勒展开（这里的ft-1（x）就是前t-1个弱分类器的整合），展开之后是一个常数C 和 梯度g和新的弱分类器f的乘积（就是L = C + gf），这时候如果要让损失函数L变得更小，就要让梯度g和弱分类器f的乘积变得更小，所以就是用负梯度来拟合新的弱分类器。 如果把弱分类器当做步长，那么损失函数就代表沿着梯度走了一步。如果新的弱分类器跟负梯度完美拟合，那么损失函数就变成了L = C + g（-f）= C-gg，所以这时候损失函数是下降了的。如果是这样的话，只要保证新的弱分类器f的向量方向和梯度g相反，而向量模长尽可能大不就行了吗？答案当然是不行的，因为我们做的是损失函数在ft-1（x）的泰勒展开啊，如果步长太长的话沿着梯度相反的方向走一步，误差就大了。所以步长也要小一点。在这不知道为什么定义步长f的模和梯度的模相等，可能Freidman有所考虑的吧,如果后续让我来改进，我就从该地方入手，选择多长的步长也就是-a*g来拟合新的弱分类器f，其中a&gt;0。 xgboost损失函数同样是在ft-1（x）处做泰勒展开，不同的是做的二阶泰勒展开，而且还附带了对树叶子节点约束、权值模长（其实可以考虑为步长）约束的正则项。当然在GBDT里面一样可以做这些正则项约束，这里面并不是xgboost特有的，但是不同的地方就是xgboost损失函数在ft-1（x）处做二阶泰勒展开。对就是从二阶泰勒展开入手去最优化损失函数的。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 博客配置]]></title>
    <url>%2F2018%2F06%2F04%2Fhexo%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[本博客选用了 hexo 中 Next主题。在 Next 主题中，采用了font awesome 字体库。 相关的对照表，如font awesome所示。 在博客中设置分类和标签是非常重要的操作。 首先新建一个 tags 页面 12$ cd your-hexo-site$ hexo new page tags 进入刚刚生成的页面，一般为 index 文件，修改 type 类型为tags 即可。如下： 1234title: 标签date: 2014-12-22 12:39:04type: "tags"--- 对于分类 categories 同理，只需要将 type 类型改为 categories 即可。但是需要注意一点，对于 tags 没有层级区分。而 categories 则有着严格的先后级关系。 可以修改 /scaffolds/post.md 模板，加入 tags、 categories 的标签。如下： 123456---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags:categories:--- 在 hexo 中开启搜索选项，]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu12.04 Hadoop+Spark 分布式部署步骤]]></title>
    <url>%2F2018%2F06%2F04%2Fubuntu12-04-Hadoop-Spark-%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[1、准备工具： 虚拟机：vmware 操作系统 ：ubuntu12.04 （桌面版或server版）(iso文件) java：jdk-7u75-linux-x64.tar.gz oracle官网可下载 hadoop：haopop2.5.2 spark: spark1.1.1 登陆终端 secure crt 文件传输：file zilla 2、具体步骤 （1）、安装vmware —过程略 （2）、新建虚拟机（3个）安装ubuntu系统 —过程略 注：下面的步骤无特殊说明则需要在三个虚拟机上同时执行 （3）、apt-get 更新： apt-get update （4）、安装vim openssh客户端，服务端 apt-get instsall vim openssh-client openssh-server 判断ssh是否正常启动 命令：ps -e |grep ssh 如果既有sshd 又有ssh-agent 则ssh启动成功 （5）、安装java环境 假定准备将java安装在/usr/lib/java中 则建立该目录：mkdir /usr/lib/java 通过file zilla 将压缩文件传送到虚拟机上 解压jdk tar -xzvf jdkxxxxxx 设置环境变量： 编辑 /etc/profile文件，在其中加入以下内容 export JAVA_HOME=/usr/lib/java/jdk1.7.0_75 export JRE_HOME={$JAVA_HOME}/jre 保存退出 令其生效 source /etc/profile 此时查看java版本 java -version 如果系统中有多个java环境则需要手动设置默认的jdk 命令如下： sudo update-alternatives –install /usr/bin/javac javac /usr/lib/java/jdk1.7.0_75/bin/javac 300 sudo update-alternatives –install /usr/bin/java java /usr/lib/java/jdk1.7.0_75/bin/java 300 sudo update-alternatives –install /usr/bin/jps jps /usr/lib/java/jdk1.7.0_75/bin/jps 300 此时在输入java -version就可以看到上图的结果了 （6）、修改hostname+配置hosts 修改hostname:编辑/etc/hostname文件分别将三台机器的hostname改成master slave1 slave2 之后重启服务器 配置hosts：编辑/etc/hosts，添加 ip 和对应的hostname。 保存退出之后验证三台机器直接ping 各自的hostname是否互通 （7）、配置ssh hadoop是通过ssh来进行节点间的通信，所以要去掉节点间ssh的密码，所以需要对ssh进行配置 a）、生成公钥：命令：ssh-keygen -t rsa -P “” b）、将master公钥保存在authorized_keys下 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys 对于本机可用ssh localhost来验证是否设置成功 c）、将另外两台机器的公钥生成后通过scp命令汇聚到master下命令如下：在目录/root/.ssh中 scp id_rsa.pub root@master:/root/.ssh/id_rsa.pub.slave1(2) d）、将所有公钥全部保存在authorized_keys下命令如b e）、将设置好的authorized_keys文件发送给两个slave节点，之后验证三台机器是否可以ssh无密码登陆 （8）、安装hadoop 解压之前的压缩文件（tar -zxvf） 设置环境变量：在/etc/profile中添加相应变量 export HADOOP_COMMON_HOME=/usr/local/hadoopspark/hadoop-2.5.2 export HADOOP_CONF_DIR=/usr/local/hadoopspark/hadoop-2.5.2/etc/hadoop export YARN_CONF_DIR=/usr/local/hadoopspark/hadoop-2.5.2/etc/hadoop export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_COMMON_HOME/lib/native export HADOOP_OPTS=”-Djava.library.path=$HADOOP_COMMON_HOME/lib” export CLASSPATH=.:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar export PATH=${JAVA_HOME}/bin:${JRE_HOME}/bin:${HADOOP_COMMON_HOME}/bin:${HADOOP_COMMON_HOME}/sbin:$PATH 保存并使其生效。 设置hadoop-env.sh（文件位于hadoop归档后的目录的etc/hadoop/中）： 加入java环境变量 export JAVA_HOME=/usr/lib/java/jdk1.7.0_75 此时可验证hadoop是否安装成功：hadoop version 若能出现hadoop信息说明安装基本成功 （9）、配置hadoop信息： 主要配置一下几个文件 配置之前先建立几se个目录用来保存hadoop执行过程中的相关信息：mkdir tmp hdfs hdfs/name hdfs/data 1）、etc/hadoop/core-site.xml 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/hadoop-2.7.0/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2)、etc/hadoop/mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3）、etc/hadoop/hdfs-site.xml 1234567891011121314151617181920&lt;value&gt;Master:50090&lt;/value&gt;是网页访问地址/home/hadoop/tools/hadoop-2.7.0/&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;Master:50090&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/hadoop-2.7.0/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/hadoop-2.7.0/hdfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4）、etc/hadoop/yarn-site.xml 12345678910111213141516171819202122232425262728&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;Master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;Master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;Master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;Master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;Master:8088&lt;/value&gt; &lt;/property&gt;&lt;!-- Site specific YARN configuration properties --&gt;&lt;/configuration&gt; （5）、etc/hadoop/slaves 注意要删除自己本身的节点，或者在配置互信时，加入自己的密钥。 配置完毕则可以准备0启动hadoop 下面的过程在master节点上执行相关操作 namenode的格式化：hadoop namenode -format 之后到hadoop所在目录启动hadoop：sbin/start-all.sh 一小段等待后hadoop启动，用jps查看 123456DataNodeJpsSecondaryNameNodeNameNodeNodeManagerResourceManager 若出现下面信息则启动成功，可通过web-browser查看相关信息url:http://masterip:50070 关闭hadoop服务：sbin/stop-all-sh 若需要重新格式化namenode则需要删除掉tmp 和/tmp/hadoop* 和hdfs/name hdfs/data中的内容 至此hadoop 分布式部署成功，下面准备在此基础上安装spark（由于spark的工作需要基于hadoop中的hdfs分布式文件系统） （10）、安装scala 解压下载好的压缩文件（tar -zxvf） 配置环境变量（/etc/profile） export SCALA_HOME=/usr/lib/scala/scala-2.11.4 验证：键入scala：出现scala命令控制行说明安装成功 （11）、安装spark 解压文件–过程略 配置环境变量：编辑conf下的spark-env.sh文件将java scala相关环境变量加入并且配置masterip以及各个worker节点的内存 export JAVA_HOME=/usr/lib/java/jdk1.7.0_75 export SCALA_HOME=/usr/lib/scala/scala-2.11.4 export SPARK_MASTER_IP=192.168.194.129 export SPARK_WORKER_MEMORY=2g export SPARK_CONF_DIR=/usr/local/hadoopspark/spark-1.1.1-bin-hadoop2.4/conf 配置同目录下的slaves 将所有节点的hostname添加进该文件（三台机器均配置） 配置完毕启动spark sbin/start-all.sh jps命令查看若新增这两个进程即spark成功启动（Master Worker） 至此spark+hadoop的分布式部署完成]]></content>
      <categories>
        <category>大数据-分布式</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 文件结构]]></title>
    <url>%2F2018%2F06%2F04%2Fhexo-%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[hexo 文件结构 接着上篇如何发布一篇博客，本篇将分析以下hexo框架中的基本文件结构。 在hexo框架中主要目录结构如下： 12345678|-- _config.yml|-- package.json|-- scaffolds|-- source |-- _posts|-- themes|-- .gitignore|-- package.json _config.yml：全局配置文件，网站的很多信息都在这里配置，诸如网站名称，副标题，描述，作者，语言，主题，部署等等参数。 package.json：hexo框架的参数和所依赖插件。 scaffolds：scaffolds是“脚手架、骨架”的意思，当你新建一篇文章（hexo new 'title'）的时候，hexo是根据这个目录下的文件进行构建的。基本不用关心。 source： 这个目录很重要，新建的文章都是在保存在这个目录下的._posts 。需要新建的博文都放在 _posts 目录下。 _posts 目录下是一个个 markdown 文件。你应该可以看到一个 hello-world.md 的文件，文章就在这个文件中编辑。 _posts 目录下的md文件，会被编译成html文件，放到 public （此文件现在应该没有，因为你还没有编译过）文件夹下。 themes：网站主题目录，hexo有非常好的主题拓展，支持的主题也很丰富。该目录下，每一个子目录就是一个主题。 _config.yml 文件的具体说明 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: Hexo #网站标题subtitle: #网站副标题description: #网站描述author: John Doe #作者language: #语言timezone: #网站时区。Hexo 默认使用您电脑的时区。时区列表。比如说：America/New_York, Japan, 和 UTC 。# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://yoursite.com #你的站点Urlroot: / #站点的根目录permalink: :year/:month/:day/:title/ #文章的 永久链接 格式 permalink_defaults: #永久链接中各部分的默认值# Directory source_dir: source #资源文件夹，这个文件夹用来存放内容public_dir: public #公共文件夹，这个文件夹用于存放生成的站点文件。tag_dir: tags # 标签文件夹 archive_dir: archives #归档文件夹category_dir: categories #分类文件夹code_dir: downloads/code #Include code 文件夹i18n_dir: :lang #国际化（i18n）文件夹skip_render: #跳过指定文件的渲染，您可使用 glob 表达式来匹配路径。 # Writingnew_post_name: :title.md # 新文章的文件名称default_layout: post #预设布局titlecase: false # 把标题转换为 title caseexternal_link: true # 在新标签中打开链接filename_case: 0 #把文件名称转换为 (1) 小写或 (2) 大写render_drafts: false #是否显示草稿post_asset_folder: false #是否启动 Asset 文件夹relative_link: false #把链接改为与根目录的相对位址 future: true #显示未来的文章highlight: #内容中代码块的设置 enable: true line_number: true auto_detect: false tab_replace:# Category &amp; Tagdefault_category: uncategorizedcategory_map: #分类别名tag_map: #标签别名# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DD #日期格式time_format: HH:mm:ss #时间格式 # Pagination## Set per_page to 0 to disable paginationper_page: 10 #分页数量pagination_dir: page # Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: landscape #主题名称# Deployment## Docs: https://hexo.io/docs/deployment.html# 部署部分的设置deploy:]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何发布一篇博客]]></title>
    <url>%2F2018%2F06%2F04%2F%E5%A6%82%E4%BD%95%E5%8F%91%E5%B8%83%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[如何发布一篇博客 自己曾经搭建过无数个博客。从开始自己使用yaf+php自己写编写，到采用GitHub+jeklly。后来也用过腾讯云服务器+WordPress的模式。最近知道了一种新的博客搭建方法，GitHub+hexo的模式。hexo框架有着丰富的主题，并且使用方便。马上要工作了，这次花费了几天搭建了一下新的博客。未来也会慢慢把自己原先的博客，慢慢迁移到这里。 hexo+GitHub搭建比较简单见大神教程 本文主要分析了一下，博客发布的过程。 1执行代码：hexo new [layout] &lt;title&gt; 这里 [layout] 是指布局格式，默认的布局格式为post。在hexo 中一共有3中不同的默认格式： 布局格式 路径 post source/_posts Page source Draft source/_drafts 默认布局格式为 post。先以布局格式为 post 的情况为说明。 首先会在在根目录的下的 source 文件夹中创建了一个 _post 文件夹，并在里面生成一个对应的文件。例如myBlog.md。这时候文件会根据模板，填入相应信息。模板存储在 scaffolds 中，可以根据需要自行修改。 根据不同的模板，决定了 Front-matter 的内容。 Front-matter 是文件最上方以 --- 分隔的区域，用于指定个别文件的变量。例如： 123456---title: 如何发布一篇博客date: 2018-06-04 01:35:26tags: hexocategories: blog--- 在 Front-matter 中有以下预定参数: 参数 描述 默认值 layout 布局 title 标题&lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space:pre&quot;&gt;&lt;/span&gt; date 建立日期 &lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space:pre&quot;&gt;&lt;/span&gt;文件建立日期 updated 更新日期 &lt;span class=&quot;Apple-tab-span&quot; style=&quot;white-space:pre&quot;&gt;&lt;/span&gt;文件更新日期 comments 开启文章的评论功能 true tags 标签（不适用于分页） categories 分类（不适用于分页） permalink 覆盖文章网址 其中 tags 都是并列的。categories 则有严格的顺序性和层次性。例如 12345categories:- blogtags:- 技术- 搭建]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
